[["index.html", "Time Series Analysis With R Chapter1 Time Series Analysis With R 1.1 Citation", " Time Series Analysis With R Nicola Righetti 2025-10-14 Chapter1 Time Series Analysis With R Welcome to this book on Time Series Analysis with R. This book provides a practical introduction to analyzing time series data using R. It guides the reader through: The characteristics and specificity of time series data Using the free statistical software R to conduct time series analysis Key univariate and multivariate techniques for analyzing time series By the end of the book, readers will understand the unique aspects of time series data and be able to perform simple analyses in R using the methods presented here. 1.1 Citation If you use this book in your work, please cite it as: Righetti, N. (2025). Time Series Analysis With R. Available at: https://nicolarighetti.github.io/Time-Series-Analysis-With-R/ "],["getting-started-with-r.html", "Chapter2 Getting started with R 2.1 RStudio Interface and Data 2.2 Basic R", " Chapter2 Getting started with R 2.1 RStudio Interface and Data 2.1.1 Download and Install RStudio This course is based on the statistical software R. R is easier to use within RStudio, which works on Windows, macOS, and other operating systems. It is possible to download a free version of RStudio Desktop from the official websites. You might also use a free online version of RStudio by registering to the RStudio Cloud free plan. However, the free plan gives you just 15 hours per months. Now let’s see how to get started with RStudio Desktop. First, download and install a free version of RStudio Desktop and open the software. 2.1.2 Create a RStudio Project and Import data When starting a data analysis project with RStudio, we create a new dedicated environment where we will keep all the scripts (files containing the code to perform the analysis), data sets, and outputs of the analysis (such as plots and tables). This dedicated workspace is simply called a project. To create a new project with RStudio, follow these steps: click on File (on the top left); then, click on New Project; select New Directory, and New Project; Choose a folder for the project and give a name to your project. You can use the name Time-Series-Analysis-With-R. This will create a new folder for the project in the main folder specified in the previous step. In this folder, you will find a file .Rproj, which has the same name as the project you assigned. To work on this project, you just need to open the .Rproj file. 2.1.3 Create a Script Once the project is created, you can open a new script and save it. A script is a file containing code. We can create a first script named basic-r-syntax, where you will test the basic code we are going to see. The script will be saved with extension .r. You can open, change, and save the file every time you work on it. Saving your code is important; otherwise, you would have to write the same code every time you work on the project! Click here to watch “Create and Save Scripts” Click here to watch “Update Scripts and Run Code” 2.1.4 The RStudio User Interface The interface of RStudio is organized in four main quadrants: The top-left quadrant is the editor. Here you can create or open a script and compose the R commands. The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. There is also the Files tab, where you can navigate files and folders and find, for instance, the data sets you want to upload. On the bottom left is the R Console window, where the code gets executed and the output is produced. You can run the commands, sending the code from the editor to the console, by highlighting it and hitting the Run button, or the Ctrl-Enter key combination. It is also possible to type and run commands directly into the console window (in this case, nothing will be saved). The bottom-right quadrant is a window for graphics output. Here you can visualize your plots. There are also tabs for R packages and the R Help facility. 2.1.5 Load and Save Data To load data into R, you can click on the Files window in the top-right quadrant, navigate your files and folders, and once you have found your data set file, you can just click it and follow the semi-automated import procedure. Click here to watch “Import Data” Otherwise, you can upload a data set by using a function. For instance, to import a csv file, one of the most common formats for data sets, you can use the function read.csv. The main argument of this function is the path of the file you want to upload. To specify the file path, consider that you are working within a specific environment; that is, your working directory is the folder of the project (you can double-check the working directory you are in by running the command getwd()). Thus, to indicate the path of the data set you want to upload, you can write a dot followed by a slash ./, followed by the path of the data set inside the working directory. For instance, in the case below, the data set is saved in a folder named data inside the working directory. The name of the data set is tweets_vienna and its extension is .csv. Therefore, the code to upload the file is as follows: To save data, there are a few options. Generally, if you want to save a data set, you can opt for the .csv or the .rds format. The .rds format is only readable by R, while the .csv format is “universal” (you can read it with Excel, for instance). To save a file as .csv, you can use the function write.csv. The main arguments of this function are the name of the object that has to be saved, the path to the folder where the object will be saved, and the name you want to assign to the file. To save a .rds file, the procedure is similar, but the saveRDS function has to be employed. To read an .rds file, the appropriate function is readRDS. In the code above, you can notice a hash mark sign followed by some text. It is a comment. Comments are textual content used to describe the code in order to make it easier to understand and reuse. Comments are written after the hash mark sign (#) because the text written after the hash mark sign is ignored by R: you can read the comments, but R does not consider them as code. 2.1.6 Create new Folders It is good practice to create, in the main folder of the project, sub-folders dedicated to different types of files used in the project, such as a folder data for the data sets. To create a new folder, you can go to the Files window in the RStudio interface, click New Folder, and give it a name. 2.2 Basic R 2.2.1 Objects An object is an R entity composed of a name and a value. The arrow (&lt;-) sign is used to create objects and assign a value to an object (or to change or “update” its previous value). Example: create an object with the name object_consisting_of_a_number and value equal to 2: Enter the name of the object in the console and run the command: the value assigned to the object will be displayed. ## [1] 2 The object is equal to its value. Therefore, for instance, an object with a numerical value can be used to perform arithmetical operations. ## [1] 20 The value of an object can be transformed: ## [1] 20 An object can also represent a function. Example: create an object for the sum (addition) function: The function can now be applied to two numerical values: ## [1] 7 Actually, we don’t need this function, since mathematical functions are already implemented in R. ## [1] 7 ## [1] 12 ## [1] 6 ## [1] 9 ## [1] 3 The value of an object can be a number, a function, or a vector. Vectors are sequences of values. ## [1] 1 2 3 4 5 6 7 8 9 10 A vector of numbers can be the argument of mathematical operations. ## [1] 2 4 6 8 10 12 14 16 18 20 ## [1] 4 5 6 7 8 9 10 11 12 13 Other R objects are matrix, list, and data.frame. A matrix is a table composed of rows and columns containing only numerical values. ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 11 21 31 41 ## [2,] 2 12 22 32 42 ## [3,] 3 13 23 33 43 ## [4,] 4 14 24 34 44 ## [5,] 5 15 25 35 45 ## [6,] 6 16 26 36 46 ## [7,] 7 17 27 37 47 ## [8,] 8 18 28 38 48 ## [9,] 9 19 29 39 49 ## [10,] 10 20 30 40 50 A list is just a list of other objects. For instance, this list includes a numerical value, a vector of numbers, and a matrix. ## [[1]] ## [1] 20 ## ## [[2]] ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## [[3]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 11 21 31 41 ## [2,] 2 12 22 32 42 ## [3,] 3 13 23 33 43 ## [4,] 4 14 24 34 44 ## [5,] 5 15 25 35 45 ## [6,] 6 16 26 36 46 ## [7,] 7 17 27 37 47 ## [8,] 8 18 28 38 48 ## [9,] 9 19 29 39 49 ## [10,] 10 20 30 40 50 A data.frame is like a matrix that can contain numbers but also other types of data, such as characters (a textual type of data) or factors (unordered categorical variables, such as gender, or ordered categories, such as low, medium, high). Data sets are usually stored in data.frames. For instance, if you import a csv or an Excel file in R, the corresponding R object is a data.frame. ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Monday&quot; ## [4] &quot;Tuesday&quot; &quot;Monday&quot; &quot;Wednesday&quot; ## [7] &quot;Thursday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; ## [10] &quot;Saturday&quot; &quot;Sunday&quot; &quot;Friday&quot; ## [13] &quot;Saturday&quot; &quot;Sunday&quot; ## first_variable second_variable ## 1 1 Monday ## 2 2 Tuesday ## 3 3 Monday ## 4 4 Tuesday ## 5 5 Monday ## 6 6 Wednesday ## 7 7 Thursday ## 8 8 Wednesday ## 9 9 Thursday ## 10 10 Saturday ## 11 11 Sunday ## 12 12 Friday ## 13 13 Saturday ## 14 14 Sunday To access a specific column of a data.frame, you can use the name of the data.frame, the dollar symbol $, and the name of the column. ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## [1] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Monday&quot; ## [4] &quot;Tuesday&quot; &quot;Monday&quot; &quot;Wednesday&quot; ## [7] &quot;Thursday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; ## [10] &quot;Saturday&quot; &quot;Sunday&quot; &quot;Friday&quot; ## [13] &quot;Saturday&quot; &quot;Sunday&quot; It is possible to add columns to a data.frame by writing: the name of the data.frame the dollar sign a name for the new column the arrow sign &lt;- a vector of values to be stored in the new column (it has to have length equal to the other vectors composing the data.frame) ## first_variable second_variable a_new_variable ## 1 1 Monday 12 ## 2 2 Tuesday 261 ## 3 3 Monday 45 ## 4 4 Tuesday 29 ## 5 5 Monday 54 ## 6 6 Wednesday 234 ## 7 7 Thursday 45 ## 8 8 Wednesday 42 ## 9 9 Thursday 6 ## 10 10 Saturday 267 ## 11 11 Sunday 87 ## 12 12 Friday 3 ## 13 13 Saturday 12 ## 14 14 Sunday 9 It is possible to visualize the first few rows of a data.frame by using the function head. ## first_variable second_variable a_new_variable ## 1 1 Monday 12 ## 2 2 Tuesday 261 ## 3 3 Monday 45 ## 4 4 Tuesday 29 ## 5 5 Monday 54 ## 6 6 Wednesday 234 2.2.2 Functions A function is a coded operation that applies to an object (e.g.: a number, a textual feature etc.) to transform it based on specific rules. A function has a name (the name of the function) and some arguments. Among the arguments of a function there is always an object or a value, for instance a numerical value, which is the content the function is applied to, and other possible arguments (either mandatory or optional). Functions are operations applied to objects that give a certain output. E.g.: the arithmetical operation “addition” is a function that applies to two or more numbers to give, as its output, their sum. The arguments of the “sum” function are the numbers that are added together. The name of the function is written out of parentheses, and the arguments of the function inside the parentheses: ## [1] 8 Arguments of functions can be numbers but also textual features. For instance, the function paste creates a string composed of the strings that it takes as arguments. ## [1] &quot;the cat is at home&quot; In R you can sometimes find a “nested” syntax, which can be confusing. The best practice is to keep things as simple as possible. ## [1] &quot;the cat is at home and sleeps on the sofa&quot; To sum up, functions manipulate and transform objects. Data wrangling, data visualization, as well as data analysis, are performed through functions. 2.2.3 Data Types Variables can have different R formats, such as: double: numbers that include decimals (0.1, 5.676, 121.67). This format is appropriate for continuous variables; integer: such as 1, 2, 3, 10, 400. It is a format suitable to count data; factors: for categorical variables. Factors can be ordered (e.g.: level of agreement: “high”, “medium”, “low”), or not (e.g.: hair colors “blond”, “dark brown”, “brown”); characters: textual labels; logicals: the format of logical values (i.e.: TRUE and FALSE) dates: used to represent days; POSIX: a class of R format to represent dates and times. Figure 2.1: R data formats. Tables from Gaubatz, K. T. (2014). A Survivor’s Guide to R: An Introduction for the Uninitiated and the Unnerved. SAGE Publications. It is better to specify the appropriate type of data when importing a data set. In the example below, the data format are specified by using the import process of RStudio. Notice that the data of type “date” requires users to specify the additional information regarding the format of the dates. Indeed, dates can be written in many different ways, and to read dates in R it is necessary to specify the structure of the date. In the example, dates are in the format Year-Month-Day, which is represented in R as “%Y-%m-%d” (further details will be provided in another section of the book). Click here to watch “Import Data and Specify Data Types” 2.2.4 Excercise Upload the data set “election news small”, using the appropriate data format; Open the script “basic-r-script” and perform the following operations: Check the first few rows of the data set; Access the single columns; Save the data frame with the name “election_news_small_test” in the folder “data” by using the function “write.csv” (to review the procedure go to the section “Load and Save Data” on this book); Comment the code (the comments have to be written after the hash sign #); Save the script. "],["basic-data-wrangling-with-tidyverse.html", "Chapter3 Basic Data Wrangling with Tidyverse 3.1 The Pipe Operator %&gt;% 3.2 Mutate 3.3 Rename 3.4 Summarize and group_by 3.5 Arrange 3.6 Filter 3.7 Select", " Chapter3 Basic Data Wrangling with Tidyverse Data wrangling is the process of transforming and mapping data from one “raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data. Another definition is as follows: Data wrangling is the process of profiling and transforming datasets to ensure they are actionable for a set of analysis tasks. One central goal is to make data usable: to put data in a form that can be parsed and manipulated by analysis tools. Another goal is to ensure that data is responsive to the intended analyses: that the data contain the necessary information, at an acceptable level of description and correctness, to support successful modeling and decision-making. How to “manipulate” data sets in R: use basic R functions; employ specific libraries such as tidyverse. Tidyverse is an R library composed of functions that allow users to perform basic and advanced data science operations. https://www.tidyverse.org. In R, a library (or “package”) is a coherent collection of functions, usually created for specific purposes. To work with the tidyverse library, it is necessary to install it first, by using the following command: install.packages(“tidyverse”). After having installed tidyverse (or any other library), it is necessary to load it, so as we can work with its functions in the current R session: Besides using the function install.packages(NAME-OF-THE-LIBRARY) by using a line of code, it is also possible to use the RStudio interface. Click here to watch “Install and Load Libraries” 3.1 The Pipe Operator %&gt;% Tidyverse has a peculiar syntax that makes use of the so-called pipe operator %&gt;%, like in the following example: ## # A tibble: 7 × 2 ## second_variable mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Friday 3 ## 2 Monday 37 ## 3 Saturday 140. ## 4 Sunday 48 ## 5 Thursday 25.5 ## 6 Tuesday 145 ## 7 Wednesday 138 To manipulate data sets we can rely on the functions included in dplyr: a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges, such as mutate, rename, summarize. ## # A tibble: 6 × 4 ## created_at screen_name source ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iP… ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iP… ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iP… ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iP… ## # ℹ 1 more variable: retweet_count &lt;int&gt; Click here to watch “Import Data and Specify Data Types (Dates and Times)” 3.2 Mutate The function mutate adds new variables to a data.frame or overwrites existing variables. ## # A tibble: 6 × 5 ## created_at screen_name source ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iP… ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iP… ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iP… ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iP… ## # ℹ 2 more variables: retweet_count &lt;int&gt;, ## # log_retweet_count &lt;dbl&gt; 3.3 Rename rename is a function to change the name of columns (sometimes it can be useful). ## # A tibble: 6 × 5 ## created_at screen_name device ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iP… ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iP… ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iP… ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iP… ## # ℹ 2 more variables: retweet_count &lt;int&gt;, ## # log_retweet_count &lt;dbl&gt; The previous two steps can be performed at the same time, by concatenating the operations through the pipe %&gt;% operator. ## # A tibble: 6 × 5 ## created_at screen_name device ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iP… ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iP… ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iP… ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iP… ## # ℹ 2 more variables: retweet_count &lt;int&gt;, ## # log_retweet_count &lt;dbl&gt; To check the data format of the variables stored in the data.frame, you can use the command str(). ## tibble [100 × 5] (S3: tbl_df/tbl/data.frame) ## $ created_at : POSIXct[1:100], format: ... ## $ screen_name : chr [1:100] &quot;DoYourThingUK&quot; &quot;DoYourThingUK&quot; &quot;AlexS1595&quot; &quot;MakesworthAcc&quot; ... ## $ device : chr [1:100] &quot;Twitter for iPhone&quot; &quot;Twitter for iPhone&quot; &quot;Twitter for iPhone&quot; &quot;Twitter Web App&quot; ... ## $ retweet_count : int [1:100] 0 0 3 0 4 96 0 1 0 3 ... ## $ log_retweet_count: num [1:100] 0 0 1.39 0 1.61 ... Sometimes variables are stored in the data.frame in the wrong format (see the paragraph “data type”), so we may want to convert them to a new format. For this purpose, we can use the function mutate along with other functions such as as.integer, as.numeric, as.character, as.factor, as.logical, as.Date, or as.POSIXct() depending on the desired data format (it is possible and advisable to upload the data by paying attention to the type of data. If you upload the data in the correct format, you can skip this step). ## # A tibble: 6 × 5 ## created_at screen_name device ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iP… ## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iP… ## 3 2021-03-24 08:53:52 AlexS1595 Twitter for iP… ## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App ## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App ## 6 2021-03-24 08:53:51 GGrahambute Twitter for iP… ## # ℹ 2 more variables: retweet_count &lt;int&gt;, ## # log_retweet_count &lt;dbl&gt; 3.4 Summarize and group_by To aggregate data and calculate summary values (for instance, the average number of tweets by day), you can use the function group_by (to aggregate data, for instance by day) and summarize to calculate the summary values. ## # A tibble: 6 × 2 ## screen_name average_retweets ## &lt;chr&gt; &lt;dbl&gt; ## 1 2EXvoZ6nublpw1F 164 ## 2 AdilHaiderMD 80 ## 3 AlexS1595 3 ## 4 Andecave 20 ## 5 ApKido 150 ## 6 BBVA_Trader 0 It is also possible to create more than one summary variables at once. ## # A tibble: 6 × 3 ## screen_name average_retweets average_log_retweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2EXvoZ6nub… 164 5.11 ## 2 AdilHaider… 80 4.39 ## 3 AlexS1595 3 1.39 ## 4 Andecave 20 3.04 ## 5 ApKido 150 5.02 ## 6 BBVA_Trader 0 0 3.4.1 Count occurrences A useful operation to perform when summarizing data is to count the occurrences of a certain variable. For instance, to count the number of tweets sent by each user, you can use the function n() inside the summarize function. ## # A tibble: 6 × 4 ## screen_name average_retweets average_log_retweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2EXvoZ6nub… 164 5.11 ## 2 AdilHaider… 80 4.39 ## 3 AlexS1595 3 1.39 ## 4 Andecave 20 3.04 ## 5 ApKido 150 5.02 ## 6 BBVA_Trader 0 0 ## # ℹ 1 more variable: number_of_tweets &lt;int&gt; 3.5 Arrange To explore a data set, it can be useful to sort the data (e.g., from the lowest to the highest value of a variable). With tidyverse, we can order a data.frame by using the function arrange. To sort the data from the highest to the lowest value (descending order), the minus sign (or the desc function) has to be added. ## # A tibble: 6 × 4 ## screen_name average_retweets average_log_retweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 iprdhzb 0.667 0.462 ## 2 DoYourThin… 0 0 ## 3 Makesworth… 2 0.805 ## 4 benphillip… 3.5 1.45 ## 5 viralvideo… 3.5 1.45 ## 6 2EXvoZ6nub… 164 5.11 ## # ℹ 1 more variable: number_of_tweets &lt;int&gt; ## # A tibble: 6 × 4 ## screen_name average_retweets average_log_retweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Oliver_Mig… 1988 7.60 ## 2 Lil_3arbiii 1627 7.40 ## 3 Kittyhawk6… 1285 7.16 ## 4 JulesFox12 1091 7.00 ## 5 rosaesaa26 983 6.89 ## 6 lewisabzue… 822 6.71 ## # ℹ 1 more variable: number_of_tweets &lt;int&gt; Without the minus sign (or the “desc” command), data are sorted from the lowest to the highest value. ## # A tibble: 6 × 4 ## screen_name average_retweets average_log_retweets ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2EXvoZ6nub… 164 5.11 ## 2 AdilHaider… 80 4.39 ## 3 AlexS1595 3 1.39 ## 4 Andecave 20 3.04 ## 5 ApKido 150 5.02 ## 6 BBVA_Trader 0 0 ## # ℹ 1 more variable: number_of_tweets &lt;int&gt; 3.6 Filter The function filter keeps only the cases (the “rows”) we want to focus on. The arguments of this function are the conditions that must be fulfilled to filter the data: a) the name of the column that we want to filter, and b) the values to be kept. ## # A tibble: 10 × 5 ## created_at screen_name device ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:31 Oliver_Miguel1 Twitter for… ## 2 2021-03-24 08:53:48 Lil_3arbiii Twitter for… ## 3 2021-03-24 08:53:48 Kittyhawk681 Twitter Web… ## 4 2021-03-24 08:53:37 JulesFox12 Twitter for… ## 5 2021-03-24 08:53:42 rosaesaa26 Twitter for… ## 6 2021-03-24 08:53:42 lewisabzueta Twitter for… ## 7 2021-03-24 08:53:42 jonvthvn08 Twitter for… ## 8 2021-03-24 08:53:34 florent61647053 Twitter for… ## 9 2021-03-24 08:53:37 Ritu89903967 Twitter for… ## 10 2021-03-24 08:53:33 Hurica3 Twitter for… ## # ℹ 2 more variables: retweet_count &lt;int&gt;, ## # log_retweet_count &lt;dbl&gt; In the examples below, notice the use of a double equal sign ==, and also of the quotation marks to indicate the modalities of a categorical variable. ## # A tibble: 1 × 5 ## created_at screen_name device ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:31 Oliver_Miguel1 Twitter for A… ## # ℹ 2 more variables: retweet_count &lt;int&gt;, ## # log_retweet_count &lt;dbl&gt; ## # A tibble: 33 × 5 ## created_at screen_name device ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:49 marcin_lukawski Twitter for… ## 2 2021-03-24 08:53:49 LebodyRanya Twitter for… ## 3 2021-03-24 08:53:47 anshunandanpra4 Twitter for… ## 4 2021-03-24 08:53:44 insoumise007 Twitter for… ## 5 2021-03-24 08:53:43 Metamorfopsies Twitter for… ## 6 2021-03-24 08:53:43 keepsmiling_130 Twitter for… ## 7 2021-03-24 08:53:43 lovebresil01 Twitter for… ## 8 2021-03-24 08:53:42 LightHealing Twitter for… ## 9 2021-03-24 08:53:42 lewisabzueta Twitter for… ## 10 2021-03-24 08:53:42 rosaesaa26 Twitter for… ## # ℹ 23 more rows ## # ℹ 2 more variables: retweet_count &lt;int&gt;, ## # log_retweet_count &lt;dbl&gt; It is also possible to use several conditions at the same time. ## # A tibble: 8 × 5 ## created_at screen_name device ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2021-03-24 08:53:31 Oliver_Miguel1 Twitter for … ## 2 2021-03-24 08:53:37 JulesFox12 Twitter for … ## 3 2021-03-24 08:53:42 rosaesaa26 Twitter for … ## 4 2021-03-24 08:53:42 lewisabzueta Twitter for … ## 5 2021-03-24 08:53:34 florent61647053 Twitter for … ## 6 2021-03-24 08:53:37 Ritu89903967 Twitter for … ## 7 2021-03-24 08:53:27 aspeaker66 Twitter for … ## 8 2021-03-24 08:53:42 JamesAn26254230 Twitter for … ## # ℹ 2 more variables: retweet_count &lt;int&gt;, ## # log_retweet_count &lt;dbl&gt; 3.7 Select select is used to keep only some of the columns of the original data.frame. For instance, we can apply the function to keep just the columns device and retweet_count. ## # A tibble: 6 × 2 ## device retweet_count ## &lt;chr&gt; &lt;int&gt; ## 1 Twitter for iPhone 0 ## 2 Twitter for iPhone 0 ## 3 Twitter for iPhone 3 ## 4 Twitter Web App 0 ## 5 Twitter Web App 4 ## 6 Twitter for iPad 96 "],["basic-concepts.html", "Chapter4 Basic Concepts 4.1 Time Series 4.2 Time Series Analysis 4.3 Stochastic and Deterministic Processes", " Chapter4 Basic Concepts 4.1 Time Series A time series is a serially sequenced set of values representing a variable value at different points in time (VanLear, “Time Series Analysis”). It consists in measures collected through time, at regular time intervals, about an unit of observation, resulting in a set of ordered values. This regularity is the frequency of time series (which can be, for instance, hourly, weekly, monthly, quarterly, yearly etc.). Time series data are different from cross-sectional data, which are set of data observed on a sample of units taken at a given point in time, or where the time dimension is not relevant and can be ignored. Cross-sectional data are a snapshot of a population of interest at one particular point in time, while time series show the dynamical evolution of a variable over time. Panel data combine cross-sectional and time series data by observing the same units over time. Time is a fundamental variable in time series. It is often not relevant in other types of statistical analyses. Also from a sociological perspective (and psychological as well), we can see that past events influence future behaviors. Oftentimes, we can make reasonable prediction about future social behaviors just by observing past behaviors. Actually, social reproduction of behaviors over time and predictability of future social behaviors based on past experience and shared knowledge are essential to social order, and thus, a fundamental dimension of human society. From a statistical perspective, the impact of time resulting from repeated measurements over time on a single subject or unit, introduce a dependency among data points which prevents the use of some of the most common statistical techniques. In cross-sectional data, observations are assumed to be independent: values observed on one unit has no influence on values observed on other units. Time series observations have a different nature: a time series is not a collection of independent observations, or observations taken on independent units, but a collection of successive observations on the same unit. Observations are not taken across units at the same time (or without regards to time), but across time on the same unit. When dealing with time series data, time is an important factor to be taken into account. It introduces a new dimension to the data. For instance, we can calculate how a variable increases or decreases over time, if it peaks at a given moment in time, or at regular intervals. We consider not just if, and how much, a variable is correlated with another variable, but if there is a correlation over time among them, if the peaks in one variable precedes the peaks in the other one, or how much time it requires for a variable to have an impact on another one, and how much this impact changes over time. Importantly, when dealing with time series data, we have to to acknowledge that sampling adjacent points in time introduces a correlation in the data. This serial dependency creates correlated errors which violates the assumptions of many traditional statistical analyses and can bias the estimation of error for confidence intervals or significance tests. This characteristic of time series data, in general, precludes the use of common statistical approaches such as linear regression and correlation analysis, which assume the observations to be independent. The application of “standard” statistical techniques to time series data might lead to foolish, and totally unreliable results. For instance, the statisticians George Udny Yule wrote: «It is fairly familiar knowledge that we sometimes obtain between quantities varying with the time (time-variables) quite high correlations to which we cannot attach any physical significance whatever, although under the ordinary test the correlation would be held to be certainly “significant.” (…) the occurrence of such “nonsense-correlations” makes one mistrust the serious arguments that are sometimes put forward on the basis of correlations between time-series. […] When the successive x’s and y’s in a sample no longer form a random series, but a series in which successive terms are closely related to one another, the usual conceptions (of correlation, ed.) to which we are accustomed fail totally and entirely to apply» (Yule, G.U. (1926). Why do we sometimes get nonsense-correlations between Time-Series? A study in sampling and the nature of time-series. Journal of the royal statistical society, 89(1), 1-63.) A funny website reporting spurious time series correlation is tylervigen.com. Despite it can be funny to see these improbable correlations, we have to keep in mind that adopting the right approach to analyze data is a serious issue when doing research. In a paper in the American Journal of Political Science, we can read, for instance: The results of the analysis below strongly suggest that the way event counts have been analyzed in hundreds of important political science studies have produced statistically and substantively unreliable results. Misspecification, inefficiency, bias, inconsistency, insufficiency, and other problems result from the unknowing application of two common methods that are without theoretical justification or empirical utility in this type of data. Due to the peculiarity of time series data, time series analysis has been developed as a specific statistical methodology appropriate for the analysis of time-dependent data. Time series analysis aims at providing an understanding of the underlying processes and patterns of change over time of a unit of observation and the relations between variables observed over time, handling the time structure of the data in a proper way. 4.2 Time Series Analysis Time series analysis is an approach employed in many disciplines. Almost every field of study has data characterized by a time development, and every phenomenon with a temporal dimension can be conceived as a time series and analyzed through time series analysis methods. Time series analysis is an important part of data analysis in disciplines such as economics, to analyze, for instance, inflation trends; marketing, to analyze the number of clients of a store or the number of accesses to an e-commerce website; demography, to study the growth of national population over time or trends in population ageing; engineering, to analyze radio frequencies; and neurology, to analyze brain waves detected through electroencephalograms. Political science can be interested in studying patterns in the alternation of political parties in government, and digital communication can use time series analysis to study series of tweets using a hashtag, the news media coverage on a certain topic, or the trends in user searches on search engines, such as those provided by Google Trends. About the use of time series analysis in communication science, it can be observed that: “Many of the major theories and models in our field contain time as a central player: the two-step flow, cultivation, spiral-of-silence, agenda-setting, framing, and communication mediation models, to name a few (Nabi &amp; Oliver, 2009). Each articulates a set of processes that play out in time: Messages work their way through media systems and networks, citizens perceive the world around them and decide to communicate, or not, and they make choices about participation, presumably as a product of a process that includes communication exposure. Indeed, the words that animate our field—effect, flow, influence, dynamic, cycle—reveal our understanding of communication as a process, and processes have temporal dimensions (Box-Steffensmeier, Freeman, Hitt, &amp; Pevehouse, 2014). The perspective of time series analysis can help expand our notions of time’s role in these dynamics. We see several ways in which we can become more attentive to time in our field”. Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, A., &amp; Yang, J. (2019). The Temporal Turn in Communication Research: Time Series Analyses Using Computational Approaches. International Journal of Communication (19328036), 13. “One of the most common applications of time series analyses in mass communication is in agenda-setting research. The approach is to correlate the national news coverage on a topic over time with public opinion or public policy on that topic, often to estimate lagged effects or the decay of effects over time. Likewise, both trends and cycles of television programming, viewing, and advertising, have been explored through time series analyses. In the interpersonal literature, the most popular and one of the most important applications of time series analysis has been the investigation of mutual adaptation in the form of patterns of reciprocity or compensation between conversational partners over the course of an interaction.” (C. Arthur VanLear, “Time Series Analysis”, in Allen, M. (Ed.). (2017). The SAGE encyclopedia of communication research methods. Sage Publications). In general, we can distinguish at least the following objectives of a time series analysis study: DESCRIPTION: Description of a process characterized by an intrinsic temporal dimension. Simple examples of related questions are: is there an upward trend? Is there a peak at a certain point in time? Is there a regular pattern recurring every year, at a particular moment in time? Descriptive questions like these can be answered via descriptive time series analysis. EVALUATION: Evaluation of the impact of a certain event, occurring at a particular point in time, on a process. For instance: did a change in social media moderation policy, such as those that led to banning accounts linked to conspiracy theories, impact the quantity of fake news shared online by users? Specific time series techniques can be used to perform this kind of analysis. EXPLANATION: Explanation of a phenomenon characterized by a time series structure on the basis of related variables. For instance: does the quantity of news shared on Facebook help explain the polarization of the debate online? Does the volume of news media articles on a topic help explain the growth of the debate online on the same topic? Inferential statistical techniques, such as regression models developed for time series, are used to answer questions like these. FORECASTING: Prediction of the future values of a process. For instance: can we expect that news media coverage on a certain topic will keep growing in the near future? This is the subject of time series forecasting. We can also distinguish between univariate and multivariate time series analysis. Time series analysis can be used to explain the temporal dependencies within and between processes. By temporal dependency within a social process, we mean that the current value of a variable is, in part, a function of previous values of that same variable. To analyze the univariate structure of time series, univariate techniques are used. Temporal dependency between social processes, conversely, indicates that the current value of a variable is in part a function of the previous values of other variables. Multivariate time series analysis is used to explain the relations between time series. 4.3 Stochastic and Deterministic Processes A general distinction can be made between time series, based on their deterministic or non-deterministic nature. A deterministic time series is one which can be explicitly expressed by an analytic expression. It has no random or probabilistic parts. It is always possible to exactly predict its future behavior, and state how it behaved in the past. Deterministic processes are pretty rare when dealing with individual and social behaviors! Predicting future behaviors of a crowd, of a person, of a social group, can be reasonably possible, sometimes, based on past behaviors and other contextual information, since human behavior is partly influenced by the past. However, it is not totally determined by the past. There is always a certain degree of uncertainty in the prediction; human behaviors are, generally speaking, not fully predictable. Social and individual behaviors, therefore, are non-deterministic. A non-deterministic time series cannot be fully described by an analytic expression. It has some random, or probabilistic component, that prevents its behavior from being explicitly described. It could be possible to say, in probabilistic terms, what its future behavior might be. However, there is always a residual, unpredictable, component. A time series may be considered non-deterministic also because all the information necessary to describe it explicitly is not available, although it might be in principle, or because the nature of the generating process, or part of it, is inherently random. We can say that the time series analyzed in social science have always, at least, a stochastic component that makes them not totally deterministic. Since non-deterministic time series have a random component, they follow probabilistic rather than deterministic laws. Random data are not defined by explicit mathematical relations, but rather in statistical terms, that is, by probability distributions and parameters such as mean and variance. Non-deterministic time series can be analyzed by assuming that they are manifestations of probabilistic or stochastic processes. "],["time-series-objects.html", "Chapter5 Time Series Objects 5.1 Time Series Objects", " Chapter5 Time Series Objects 5.1 Time Series Objects Every object we manipulate in R is characterized by a specific structure. Objects’ structures vary depending on the type of object: a list, a matrix, or a data.frame, are different objects with different structures. Every structure has its own manipulation methods. For instance, it can be accessed and analyzed by using different functions and strings of code. In R there are many different types of object. To get an overview you can refer to this handbook, to the R manual, or the chapter 5 of this free online book. In this course we are going to learn more about the data structures we have to deal with when conducting time series analysis in R, that is, the structure of time series objects and data sets. Time series data sets in R can be represented by different objects. Specific libraries (coherent collections of functions) can give different structures to time series data sets. There are many R libraries for handling and working with time series objects. Some of them are more general and other are useful to perform very specific analysis. On this page you can find a comprehensive list of the R libraries for time series analysis. For now, we just need to know that different libraries can create time series objects with different structures which can be manipulated through different functions. This means that not all the objects can be analyzed with all the functions, as well that there is not always compatibility between R libraries. Many functions have been developed with reference to specific libraries and objects, or require a particular object structure. As a consequence, creating a time series object with a certain structure or a certain library can imply that we can use some functions and perform only a certain type of analysis. In other words, specific type of objects could introduce specific constrains to data analysis (and visualization), so it could be wise to plan in advance the necessary analyses, so as to select the necessary libraries and data structure. We now introduce three types of objects that are commonly used to store and analyze time series data: The data.frame (in base R) The ts object (in base R) The xts object (created through the library xts). We analyze the structures of these objects and their strengths and limitations. In the next chapter, we’ll also learn the methods available to visualize them. 5.1.1 Time Series as Data Frames Data frames (data.frame) are the most common data set structure in R. A data.frame is simply a table cases by variables (each row is a case and each column is a variable). To see an example of data.frame containing time series data, we can upload a data set containing the number of news articles mentioning the keyword “elections” published by USA news media. I retrieved this data set from MediaCloud, a free and open source platform for studying media ecosystem that tracks millions of stories published online. You can download the data set at this link. We can upload the .csv file by using the function read.csv. The main argument of the read.csv function is the path of the file. By using the function class we can see that this is a data.frame. ## [1] &quot;data.frame&quot; We can check the first few rows of the data.frame by using the function head, which shows the first few rows of the data set, so as to get an idea of the structure of this simple data.frame. ## date count total_count ratio ## 1 2015-01-01 373 25611 0.01456405 ## 2 2015-01-02 387 31932 0.01211950 ## 3 2015-01-03 289 24646 0.01172604 ## 4 2015-01-04 322 25513 0.01262102 ## 5 2015-01-05 567 39982 0.01418138 ## 6 2015-01-06 626 42366 0.01477600 This data.frame contains time series data: the first column contains dates, and the other columns contain the values of the observations. We can also see that the data.frame seems to contain daily data, where each row corresponds to a specific day. The data frame also includes, in the column “count”, the number of news articles mentioning the keyword “elections”, in the column “total_count”, the total number of news articles on all the topics, and in the column “ratio” the proportion of news articles mentioning the keyword (count/total_count). The function head (and tail) can be impractical with data.frame including a lot of columns, so it could be better to use the function str to check the structure of the data.frame. ## &#39;data.frame&#39;: 2192 obs. of 4 variables: ## $ date : chr &quot;2015-01-01&quot; &quot;2015-01-02&quot; &quot;2015-01-03&quot; &quot;2015-01-04&quot; ... ## $ count : int 373 387 289 322 567 626 507 521 531 346 ... ## $ total_count: int 25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ... ## $ ratio : num 0.0146 0.0121 0.0117 0.0126 0.0142 ... As you can see in the output of the function str, the format of the column date is Factor. The format is, in this case, automatically attributed by R, but (as we have already said) it can be specified before importing the data. Factor is an appropriate format for categorical variables, but R includes a specific format for dates and times. In this case we have just a date, so we can convert it to a variable of type date. We can change the format of the variable by using the function as.Date. ## &#39;data.frame&#39;: 2192 obs. of 4 variables: ## $ date : Date, format: ... ## $ count : int 373 387 289 322 567 626 507 521 531 346 ... ## $ total_count: int 25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ... ## $ ratio : num 0.0146 0.0121 0.0117 0.0126 0.0142 ... We can also perform the same operation with tidyverse, by using the function mutate. A data.frame is the common format for data sets, including time series data sets. We can do many things with data stored in this format, such as creating plots and performing various types of analysis. However, to handle time series in R there are more specific data formats. 5.1.2 Time Series as TS objects The basic object created to handle time series in R is the object of class ts. The name stands for “Time Series”. An example of ts object is already present in R under the name of “AirPassengers”, a time series data set in ts format. We can load this data set with the function data. By applying the function class we can see that this is an object of class ts. ## [1] &quot;ts&quot; AirPassengers is a small data set so we can print all the data set to see its structure, which is an example of the standard structure of a ts object. ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 1949 112 118 132 129 121 135 148 148 136 119 104 ## 1950 115 126 141 135 125 149 170 170 158 133 114 ## 1951 145 150 178 163 172 178 199 199 184 162 146 ## 1952 171 180 193 181 183 218 230 242 209 191 172 ## 1953 196 196 236 235 229 243 264 272 237 211 180 ## 1954 204 188 235 227 234 264 302 293 259 229 203 ## 1955 242 233 267 269 270 315 364 347 312 274 237 ## 1956 284 277 317 313 318 374 413 405 355 306 271 ## 1957 315 301 356 348 355 422 465 467 404 347 305 ## 1958 340 318 362 348 363 435 491 505 404 359 310 ## 1959 360 342 406 396 420 472 548 559 463 407 362 ## 1960 417 391 419 461 472 535 622 606 508 461 390 ## Dec ## 1949 118 ## 1950 140 ## 1951 166 ## 1952 194 ## 1953 201 ## 1954 229 ## 1955 278 ## 1956 306 ## 1957 336 ## 1958 337 ## 1959 405 ## 1960 432 By calling the str function we get synthetic information on the object. ## Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ... The AirPassengers data set is a univariate time series representing monthly totals of international airline passengers from 1949 to 1960. As every time series, it has a start date and an end date. It also has a frequency, which is the frequency at which the observations were taken. All this characteristics differentiate a ts object from a data.frame. The structure of a data.frame lacks the start and the end date, and the frequency value. The functions start, end, and frequency, can be applied to a ts object to check their values. We started by saying that some functions work with some objects but not with other types of objects. This is an example. These functions, indeed, work with ts objects just because they are part of their structures, and are arguments usually specified when this kind of object is created. They do not work if applied to a data.frame object, since a data.frame structure does not include the start and end date, nor the frequency of observations. It can be seen that the ts structure is much more specific for time series data. ## [1] 1949 ## [1] 1960 ## [1] 12 Importantly, the frequency of a time series is assumed to be regular over time. This applies to time series in general, and not just to ts objects. In this case, the time series starts on January 1949 and ends on December 1969, and has monthly frequency. Monthly frequency is indicated in ts as “12”, meaning 12 months. Indeed, the reference unit of a ts object is a year. So, quarterly data, for instance, have frequency equal to 4. To create a ts object is necessary to follow specific steps and use specific functions. To exemplify the process of creation of a ts object we take the example of the data contained in the AirPassengers data set, and store them in a data.frame (you don’t need to learn how to do that, just copy and paste the code). A data.frame is the data set format you will probably start with, so it can be useful to see how to create a ts object starting from a data.frame. To create a “ts” time series object starting from a data.frame, we need: To specify which column contains the observations. In this case, the column name is “Passengers”. We then need to specify the start and end date, which in this case are in the format year/month, but can be just years in case of yearly data. The ts format for the start/end date is the following: c=(YEAR, MONTH). The c represents the concatenate function, and it concatenates the year and the month in a single vector. Finally, we indicate the frequency of the time series observations. The frequency is specified based on the time period of a year, so in this case we have a frequency equal to 12, because we have monthly observation, meaning that we have 12 observation per year. ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 1949 112 118 132 129 121 135 148 148 136 119 104 ## 1950 115 126 141 135 125 149 170 170 158 133 114 ## 1951 145 150 178 163 172 178 199 199 184 162 146 ## 1952 171 180 193 181 183 218 230 242 209 191 172 ## 1953 196 196 236 235 229 243 264 272 237 211 180 ## 1954 204 188 235 227 234 264 302 293 259 229 203 ## 1955 242 233 267 269 270 315 364 347 312 274 237 ## 1956 284 277 317 313 318 374 413 405 355 306 271 ## 1957 315 301 356 348 355 422 465 467 404 347 305 ## 1958 340 318 362 348 363 435 491 505 404 359 310 ## 1959 360 342 406 396 420 472 548 559 463 407 362 ## 1960 417 391 419 461 472 535 622 606 508 461 390 ## Dec ## 1949 118 ## 1950 140 ## 1951 166 ## 1952 194 ## 1953 201 ## 1954 229 ## 1955 278 ## 1956 306 ## 1957 336 ## 1958 337 ## 1959 405 ## 1960 432 ## [1] &quot;ts&quot; It’s important to notice that yearly, quarterly, and monthly data work fine with the ts structure, but more fine grained data create complications and are not totally suitable for a ts structure. This is due to the fact that time series objects require the frequency of observations to be regular and in ts the observations have to be regular with reference to a year. Unfortunately, a time series that spans over many years cannot be composed of a constant number of days, since the number of days will be sometimes 365 and other time 366, in case of leap years. This is a limitation of ts objects. However, when dealing with monthly data or data with frequency lower than one month (such as quarterly data), ts works great. 5.1.3 Time Series as XTS/ZOO objects Time series can be stored in object of class xts/zoo. This class of objects is created with the library xts, which is related and an extension of the package zoo (another package to deal with time series data). As other libraries, it requires to be installed and loaded. The xts object is more flexible than the ts one. We can create an xts time series by starting from the data.frame we have just created. Similarly to what is required by ts, we need to specify: the column of the data.frame (or the vector) containing the data; the column of the data.frame (or the vector) containing the dates/times (which has to be in a date/time format); the frequency of observations. We can use the data.frame already created with the AirPassengers data to create a new xts object. ## [1] &quot;xts&quot; &quot;zoo&quot; Also the structure of this object, like the ts one, includes the range of dates of the time series, with its starting and ending date. ## An xts object on 1949-01-01 / 1960-12-01 containing: ## Data: double [144, 1] ## Index: Date [144] (TZ: &quot;UTC&quot;) ## [,1] ## 1949-01-01 112 ## 1949-02-01 118 ## 1949-03-01 132 ## 1949-04-01 129 ## 1949-05-01 121 ## 1949-06-01 135 "],["plot-time-series.html", "Chapter6 Plot Time Series 6.1 Plot Time Series Objects 6.2 plot.ts 6.3 plot.xts 6.4 ggplot", " Chapter6 Plot Time Series 6.1 Plot Time Series Objects In this lecture we are going to learn how to plot time series data. We will take into account three main functions: ggplot from the tidyverse library, plot.ts from base R, and plot.xts from the xts library. Ggplot is probably the most versatile function from the perspective of the graphical results that can be obtained, but also the most complex, while for ordinary visualization, plot.ts is probably the easiest tool. Plotting time series is an important part of the analysis because it permits visualizing and exploring the data, both from a univariate perspective (focusing on the characteristics of a single time series) and from a multivariate perspective (focusing on the characteristics of many time series, and on the relations between them). To visualize and explore the relations between time series, we’ll learn to plot a single time series as well as many different time series at once. 6.2 plot.ts You can visualize a time series by using the function plot.ts() applied to a time series data in a ts format. An example of ts time series is provided by the AirPassengers dataset, already included in R (you can load the data by running data(“AirPassengers”)). data(&quot;AirPassengers&quot;) plot.ts(AirPassengers) You can add many details to your plot, such as a title, a label for the y axis and for the x axis, change the colors of the plot (use colors() to see the list of standard colors in R), and the size of the line. plot.ts(AirPassengers, main = &quot;PASSENGERS&quot;, xlab = &quot;1949-1960 (monthly data)&quot;, ylab = &quot;Passengers (1000&#39;s)&quot;, col = &quot;violetred3&quot;, lwd=5) You can use plot.ts to plot two (or more) time series together, a useful operation to take a look at their relations. The different time series must have the same structure (the same starting date, the same ending date, and the same frequency), and should be stored in the same ts object. To merge in one “ts” object two or more time series already in the ts format, you can employ the function ts.union(). You can plot both time series in the same plot, or create two different plots, by using the option “plot.type” and specifying single or multiple. With lwd you control the line width (or the line size): The line width, a positive number, defaulting to 1. The interpretation is device-specific, and some devices do not implement line widths less than one. With lty you control the line type: Line types can either be specified as an integer (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) or as one of the character strings “blank”, “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, or “twodash”, where “blank” uses ‘invisible lines’ (i.e., does not draw them). With the parameter nc you can control the number of columns used to display the data. To explore long time series it can be useful to focus on a limited time window. To do that, you can subset the data by using the function window. It is a function which extracts the subset of data observed between the specified start and end time. You can use the function window in the plot.ts function (alternatively, you can create a new object by applying the function window first, and then plot the new object). In the window function, you can also specify a frequency, and the series is then re-sampled at the new frequency. For instance, by re-sampling at quarterly frequency, the function keeps the observations made on January, April, July, and October, and by re-sampling at a six-month frequency, it keeps the observations made on January and July. You can use the window function also with more than one time series. To learn something more about the graphical options of plot.ts, you can open and read the help page by using ?plot.ts. The question mark followed by the name of a function opens the help page of that function. 6.3 plot.xts To plot a xts object we can similarly use the plot.xts function. You can create a xts object with the xts function (see the previous chapter), but if you already have a ts object, you can also convert it to a xts object by using the function as.xts By using multi.panel=TRUE, or multi.panel=FALSE you can plot all the time series in the same panel or using different panels. To subset the data, in order to visualize and focus on just one part of the series, instead of the function window, you have to write the dates into squared brackets as in the examples below. You can also change the frequency of the observations by using specific functions in the xts library. By using the function periodicity you can find the frequency of the time series. ## Monthly periodicity from Jan 1949 to Dec 1960 With the function to.period you can re-sample the data to “seconds”, “minutes”, “hours”, “days”, “weeks”, “months”, “quarters”, and “years”. You can only re-sample the data from a higher to a lower frequency, but not from a lower to a higher one. For instance, if you have monthly data, you can aggregate the data in quarterly or yearly data, but you cannot create a weekly or hourly time series. The result of the to.period function will contain the open (first) and close (last) value for the given period, as well as the maximum and minimum over the new period, reflected in the new high and low, respectively. ## AirPassengers_xts.Open ## Dec 1949 112 ## Dec 1950 115 ## Dec 1951 145 ## Dec 1952 171 ## Dec 1953 196 ## Dec 1954 204 ## Dec 1955 242 ## Dec 1956 284 ## Dec 1957 315 ## Dec 1958 340 ## Dec 1959 360 ## Dec 1960 417 ## AirPassengers_xts.High ## Dec 1949 148 ## Dec 1950 170 ## Dec 1951 199 ## Dec 1952 242 ## Dec 1953 272 ## Dec 1954 302 ## Dec 1955 364 ## Dec 1956 413 ## Dec 1957 467 ## Dec 1958 505 ## Dec 1959 559 ## Dec 1960 622 ## AirPassengers_xts.Low ## Dec 1949 104 ## Dec 1950 114 ## Dec 1951 145 ## Dec 1952 171 ## Dec 1953 180 ## Dec 1954 188 ## Dec 1955 233 ## Dec 1956 271 ## Dec 1957 301 ## Dec 1958 310 ## Dec 1959 342 ## Dec 1960 390 ## AirPassengers_xts.Close ## Dec 1949 118 ## Dec 1950 140 ## Dec 1951 166 ## Dec 1952 194 ## Dec 1953 201 ## Dec 1954 229 ## Dec 1955 278 ## Dec 1956 306 ## Dec 1957 336 ## Dec 1958 337 ## Dec 1959 405 ## Dec 1960 432 You can plot all the values, or select a value by using the square brackets with a comma, followed by the number of the column you want to plot (see the table above, with 4 columns). This notation is a way to access the columns (and the rows) of a data.frame or matrix. You write the name of the data.frame or the matrix, and the squared brackets indicate the index of the rows, in the first position before the comma, and the index of the columns, in the second position, after the comma. So, for instance, to access the value in the second column and the second row of the data.set “data”, you can write data[2,2], and to access the values in the third column and first row, you can write data[1,3]. If you leave a blank space in the column or row space, you get all the values in that column or rows. Therefore, by writing data[,2], or data[,3], you get all the values in the column 2 and 3, respectively. You can also re-sample and calculate the average (or another statistics) for the new period. For instance, in the example we re-sample the data by year and calculate the average. It is also possible to calculate other statistics such as, for instance, the median, just by writing “median” instead of “mean”. ## NOTE: `period.apply(..., FUN = mean)` operates by column, unlike other math ## functions (e.g. median, sum, var, sd). Please use `FUN = colMeans` instead, ## and use `FUN = function(x) mean(x)` to take the mean of all columns. Set ## `options(xts.message.period.apply.mean = FALSE)` to suppress this message. You can find additional details on xts and plot.xts by reading the help functions. At this link you can find a synthetic presentation of the functions of the xts library. 6.4 ggplot Ggplot2 is the tidyverse library for data visualization. We can use it to create time series plots and many other types of plot. We upload a dataset first, and we set the appropriate time format for the date. We place our dataset (“elections_news”) inside the ggplot function. Notice that the ggplot syntax is similar to the tidyverse one, but uses the plus sign instead of the pipe one (%&gt;%). To create a line plot with ggplot it is necessary to use the geom_line function. This function requires two parameters: the data for the x-axis and the data for the y-axis. These parameters have to be written inside the aes function. You can also specify the colors and the size of the line. By using additional functions, after the plus sign, you can also set the labels for the x- and y-axes, and the title, the subtitle, and the caption of the plot. You can also change the overall aspect of the plot by using one of the themes included in the library. You can also plot more than one series. For instance, you can create two plots, and then use the function grid.arrange, from the library gridExtra to combine the plots together. You can also plot two or more than two series in the same plot. To focus on a shorter time window, we can use the dplyr function filter. Besides filtering the data, we add a function “scale_x_datetime”, which control the labels on the x-axis, specifying we want to use monthly labels. You can also use ggplot to annotate the date. To create annotations in ggplot you can use the function “annotate”, to label the data point, and “geom_segment”, to trace lines for connecting data points to labels. You can learn more about annotation in ggplot here: https://ggplot2-book.org/annotations.html And about lines here: https://ggplot2.tidyverse.org/reference/geom_segment.html ## Warning in geom_segment(aes(x = as.Date(&quot;2018-11-01&quot;), xend = as.Date(&quot;2018-11-01&quot;), : All aesthetics have length 1, but the data has ## 2192 rows. ## ℹ Please consider using `annotate()` or provide ## this layer with data containing a single row. ## Warning in geom_segment(aes(x = as.Date(&quot;2019-05-01&quot;), xend = as.Date(&quot;2019-05-01&quot;), : All aesthetics have length 1, but the data has ## 2192 rows. ## ℹ Please consider using `annotate()` or provide ## this layer with data containing a single row. ## Warning in geom_segment(aes(x = as.Date(&quot;2016-11-01&quot;), xend = as.Date(&quot;2016-11-01&quot;), : All aesthetics have length 1, but the data has ## 2192 rows. ## ℹ Please consider using `annotate()` or provide ## this layer with data containing a single row. ## Warning in geom_segment(aes(x = as.Date(&quot;2020-11-01&quot;), xend = as.Date(&quot;2020-11-01&quot;), : All aesthetics have length 1, but the data has ## 2192 rows. ## ℹ Please consider using `annotate()` or provide ## this layer with data containing a single row. "],["structural-decomposition.html", "Chapter7 Structural Decomposition 7.1 Components of a time series 7.2 Structural decomposition 7.3 Adjust time series 7.4 White Noise and Stationarity", " Chapter7 Structural Decomposition 7.1 Components of a time series A time series can be considered composed of 4 main parts: trend, cycle, seasonality, and the irregular or remainder/residual part. 7.1.1 Trend and Cycle The Trend component is the longest-term behavior of a time series. The simplest model for a trend is a linear increase or decrease, but the trend does not have to be linear. In the AirPassengers time series there is a clear upward, linear trend. 7.1.2 Stochastic and Deterministic Trend There is a distinction between deterministic and stochastic trends. A deterministic trend is a fixed function of time. If a series has a deterministic trend, the increase (or decrease) in the value of the series is a function of time. For instance, it may appear to grow or decline steadily over time. A deterministic trend can be linear, as well as non linear. Deterministic trends have plausible explanations (for example, a deterministic increasing trend in the data may be related to an increasing population). A series with deterministic trend is also called trend stationary. A stochastic trend wanders up and down or shows change of direction at unpredictable times. Time series with a stochastic trend are also said to be difference stationary. An example of stochastic trend is provided by the so-called random walk process. Random Walk is a particular time series process in which the current values are combinations of the previous ones (\\(x_t = x_{t-1} + w_t\\), where \\(x_{t-1}\\) is the value immediately before \\(x\\), and \\(w_t\\) is a random component). The resulting time series is characterized by a discernible pattern over time which is not exactly predictable (stochastic trend). Starting from the same initial point, the same process can generate different time series. A paper on The effectiveness of social distancing in containing Covid-19 shows an example of stochastic trend and complex, deterministic nonlinear trends represented by polynomials. Figure 7.1: Figure 5 shows the actual number of Covid-19 cases recorded in the UK up to 17 June 2020. The stochastic trend estimated earlier is superimposed on the actual observations and so are two deterministic nonlinear trends represented by polynomials of degrees 5 and 6. We can see that the stochastic trend captures the slow growth at the beginning of the sample period whereas the two deterministic trends do not. The stochastic trend is also better at capturing the sharp increase represented by observation number 72. (original caption) The trend component of the series is often considered along with the cyclic one (trend-cycle). The cyclical component is represented by fluctuations (rises and falls) not occurring at a fixed frequency. The cycle component is therefore different from the seasonal variation (see below) in that it does not follow a fixed calendar frequency. 7.1.3 Seasonality The Seasonal component is a repeated pattern occurring at a fixed time period such as the time of the year or the day of the week (the frequency of seasonality, which is always a fixed and known frequency). There is a clear seasonal variation in the AirPassenger time series: bookings were highest during the summer months of June, July, and August and lowest during the autumn/winter months. It is possible to plot the distributions of data by months by using the function boxplot and cycle, to visualize the increasing number of passengers during the summer months. In this case, cycle is used to refer to the positions of each observation in the (yearly, in this case) cycle of observations (every year is considered to be a cycle of 12 observations). The library forecast, an R package that provides methods and tools for displaying and analysing time series forecasts, includes a function to create a “polar” seasonal plot. An example of weekly seasonality can be found in the COVID-19 statistics. Figure 7.2: Covid statistics (Google) Cyclic and seasonal variations can look similar. Both cyclic and seasonal variations have ‘peak-and-trough’ patterns. The main difference is that in seasonal patterns the period between successive peaks (or troughs) is constant, while in cyclical patterns the distance between successive peaks is irregular. 7.1.4 Residuals The irregular or remainder/residual component is the random-like part of the series. In general, when we fit mathematical models to time series data, the residual error series represents the discrepancies between the fitted values, calculated from the model, and the data. A good model encapsulates most of the deterministic features of the time series, and the residual error series should therefore appear to be a realization of independent random variables from some probability distribution. The analysis of residuals is thus important to judge the fit of a model. In this case, its residual error series appears to be a realization of independent random variables. Often the random variable is conceived as a Gaussian random variable. We’ll return to these topics in the last part of the chapter. AirPassengers_Random &lt;- decompose(AirPassengers, type=&quot;multiplicative&quot;)$random 7.2 Structural decomposition Along with the analysis of the peaks (see previous chapter), analyzing a time series based on these structural parts can be an important exploratory step. It helps understanding the likely causes of the series features and formulate an appropriate time series model. For instance, in the case of the AirPassengers series, we could hypothesize that the increasing trend is due to the rising prosperity in the aftermath of the Second World War, greater availability of aircraft, cheaper flights due to competition between airlines, and an increasing population. The seasonal variation, instead, seems to coincide with vacation periods. Decomposition methods try to identify and separate the above mentioned parts of a time series. Usually they consider together the trend and cycle (trend-cycle) - the longer-term changes in the series - and the seasonal factors - periodic fluctuations of constant length happening at a specific calendar frequency). There are two main ways through which these elements can be combined together: in the additive and the multiplicative form: The additive model (\\(x_{t} = m_{t} + s_{t} + z_{t}\\), where \\(x_{t}\\) is the observed time series, \\(m_{t}\\) is the trend-cycle component, \\(s_{t}\\) is the seasonal component and \\(z_{t}\\) is the residual) is useful when the seasonal variation is relatively constant over time The multiplicative model (\\(x_{t} = m_{t} * s_{t} * z_{t}\\)) is useful when the the seasonal effects tends to increase as the trend increases. There are different methods to decompose a time series. Here we consider the function decompose. This function is defined as Classical Seasonal Decomposition by Moving Averages. The function decompose uses a moving average (MA) approach to filter the data. Moving average is a classical approach to extract the trend from a time series by averaging out the seasonal effects. 7.2.1 Moving Average Moving average is a process that replaces each value \\(x_{t}\\) with an average of its current value \\(x_{t}\\) and its immediate neighbors in the past and future. For instance, it is possible to calculate a simple moving average by using the closest neighbors of a point, as follows: \\(x_{t} = \\frac{1}{3} (x_{t-1} + x_{t} + x_{t+1})\\). This is called Centered Moving Average. The number of neighbors in the past and future is determined by the analyst and is also called width of the window. The time window for the moving average is chosen by considering the frequency of the data and their seasonal effects. For instance, monthly data, which are supposed to show monthly seasonality (for instance, in the AirPassengers data there are more passengers during the summer months), can be averaged by using a period of 12 months (six months before and after each point. Since we have an even number of months, some other calculation are necessary. For instance, the moving average value for July, is calculated by averaging the average of January up to December, and the average of February up to January. R functions do this for you). The centered moving average is an example of a smoothing procedure that is applied retrospectively to a time series with the objective of identifying an underlying signal or trend. Smoothing procedures usually use points before and after the time at which the smoothed estimate is to be calculated. A consequence is that the smoothed series will have some points missing at the beginning and the end unless the smoothing algorithm is adapted for the end points. In the case of monthly data, for instance, the moving average filter determines the lost of the first and last six months of data. Smoothing procedures like moving average, allows the main underlying trend to emerge by filtering out seasonality and noise, so they are used to get an idea of the long-term underlying process of a time series. ## en en2 en4 ## 2015-01-01 0.01456405 0.01334178 NA ## 2015-01-02 0.01211950 0.01192277 0.01275765 ## 2015-01-03 0.01172604 0.01217353 0.01266199 ## 2015-01-04 0.01262102 0.01340120 0.01332611 ## 2015-01-05 0.01418138 0.01447869 0.01320110 ## 2015-01-06 0.01477600 0.01300100 0.01294493 ## 2015-01-07 0.01122600 0.01141117 0.01241382 ## 2015-01-08 0.01159633 0.01182664 0.01169304 ## 2015-01-09 0.01205695 0.01197492 0.01214178 ## 2015-01-10 0.01189290 0.01245691 0.01277492 ## en8 en16 en32 ## 2015-01-01 NA NA NA ## 2015-01-02 NA NA NA ## 2015-01-03 NA NA NA ## 2015-01-04 0.01285129 NA NA ## 2015-01-05 0.01253790 NA NA ## 2015-01-06 0.01250958 NA NA ## 2015-01-07 0.01267144 NA NA ## 2015-01-08 0.01285992 0.01257820 NA ## 2015-01-09 0.01262867 0.01239793 NA ## 2015-01-10 0.01226887 0.01234396 NA 7.2.2 Decompose To apply the function decompose, we need a ts object. Considering the AirPassengers time series, since the seasonal effect tends to increase as the trend increases, we can use a multiplicative model. As an example of an additive model we can use data from the “Seatbelts” dataset. The residual part of the model should be (approximately) random, which indicates that the model explained (most of) the significant patterns in the data (the “signal”), leaving out the “noise”. We can re-create the original time series starting from its elements (we don’t actually need to do that, it is just for illustrative purposes). 7.2.3 Compare Additive and Multiplicative Models Sometimes it could be hard to choose between additive or multiplicative models. In general, when seasonality or the variation around the trend-cycle component change proportionally to the level of the series (the trend, or the average), the multiplicative model works better. However, it might be difficult to assess the variability of the series from a time series plot. Exploratory methods, such as representing the variability in the data through box plots can help. The following “custom” function (which I called ts.year.boxplot) takes as argument a time series ts, and shows the spread of the data by year. A multiplicative time series like the AirPassengers shows a clear increasing spread as the level of the series goes up: The box plots of an additive time series look more regular. There is no evident systematic Below you can find another function (I called it compare.decomposition.methods) that compares the additive and multiplicative decomposition models applied to the same ts series. It creates plots of residuals (the time series plot, histogram, acf and pacf plots) and (roughly) measures the total residuals and residuals’ autocorrelation (lower values are better). It also creates a plot showing the different fit of the additive and multiplicative model to the data, and includes the boxplot introduced above. We can try looking at these plots and the total autocorrelation measure to get further hints into the most appropriate method. In this case, the multiplicative model looks better. ## ################################### ## TOTAL AUTOCORRELATION (ABSOLUTE VALUES) ## ################################### ## ADITTIVE MODEL = 7.19 ## MULTIPLICATIVE MODEL = 0.84 ## ## ################################### ## SUM OF RESIDUALS (ABSOLUTE VALUES) ## ################################### ## ADITTIVE MODEL = 98.35 ## MULTIPLICATIVE = 95.97 7.3 Adjust time series Sometimes the analyst is not interested in the trend or in the seasonal variation in the data, and might want to remove them, in order to let other underlying process to emerge more clearly. Other times, some components of time series can be misleading, leading to inflated or spurious correlations, and can be preferable to remove them before proceding with the analysis. 7.3.1 Seasonal adjusted data It is common to find seasonally adjusted data, that is time series from which the seasonal component has been removed. This happen quite often in economics, for instance (but in other disciplines as well), where certain growing trends can be considered trivial, and explained based on solid theory. Other parts of the series are instead considered more important, and removing the seasonal component allow them to emerge more clearly, as summarized by Granger, C. W. (1978), Seasonality: causation, interpretation, and implications. In Seasonal analysis of economic time series: Presumably, the seasonal is treated in this fashion, because it is economically uninportant, being dull, superficially easily explained, and easy to forecast but, at the same time, being statistically important in that it is a major contributor to the total variance of many series. The presence of the seasonal could be said to obscure movements in other components of greater economic significance. (…) It can be certainly be stated that, when considering the level of an economic variable, the low frequency components (the trend-cycle, ed.) are usually both statistically and economically important. (…) Because of their dual importance, it is desirable to view this component as clearly as possible and, thus, the interference from the season should be removed. (…) the preference for seasonally adjusted data is so that they can more clearly see the position of local trends or the place on the business cycle. It is certainly true that for any series containing a strong season, it is very difficult to observe these local trends without seasonal adjustment. Moreover, seasonality can lead to spurious correlations: (…) if the relationship between a pair of economic variables is to be analyzed, it is obviously possible to obtain a spurious relationship if the two series contain important seasonals. By adjusting series, one possible source of spurious relationship is removed. However, adjust for seasonality a series is application specific, and sometimes this part of the series can be of interest: Firms having seasonal fluctuations in demand for their products, for example, may need to make decisions based largely on the seasonal component (…) and a local government may try to partially control seasonal fluctuations in unemployment. (…) Only by having both the the adjusted and the unadjusted data available can these potential users gain the maximum benefit from all of the effort that goes into collecting the information. There are many different methods to adjust data for seasonality. A simple approach is based on the results of the decomposition process, and consists in subtracting (in the case of an additive decomposition model) the seasonal component from the original series, or dividing the original series by the seasonal component (in the case of a multiplicative model). Adjusting for seasonal variations makes it possible to observe potentially noteworthy fluctuations. In the case of the AirPassengers data, for instance, the seasonally adjusted plot shows more clearly an anomaly in the year 1960 that was not noticeable in the raw data. Below you can find another example with data from social media (you can download them here art1, and art2), consisting in posts published by pages of news media. By calculating a simple correlation between the original series and the de-seasonalized series, it can be observed that the correlation coefficients changes. ## [1] 0.9641655 ## [1] 0.7209823 7.3.2 Detrended series As the series can be adjusted for seasonality, it can also be adjusted for trend based on the same reasons. A similar process can also be used to remove the trend from the data, in particular in the case of a deterministic trend. In the case of a stochastic trend, instead, the usual practice is to detrend the data through differencing. Differencing means taking the first difference of consecutive points in time \\(x_t\\) - \\(x_{t-1}\\). In this way, the resulting series represents the relative change from one point in time to another. Detrending a time series can be important before applying some statistical techniques, for instance before calculating the correlation between two time series. Time series with a trend component can reveal spurious correlations, since correlations may exist just because two variables are trending up or down at the same time. By detrending the time series, it can be more appropriately measured if the change in one time series over time is related to the change in another time series. Detrending time series is also used when researchers consider irrelevant the trend. This is the case when the trend is considered an obvious characteristic of the process. For instance, economists can take for granted that there is an increasing trend in GDP due to inflation, and thus they may want to “clean” the data to eliminate this trivial trend. They are more interested in deviations from the growth, than in the growth that they consider a “normal” characteristic of the process. There are also statistical tests to ascertain the presence of a trend. A monotonic trend can be detected with the Mann–Kendall trend test. The null hypothesis is that the data come from a population with independent realizations and are identically distributed. For the two sided test, the alternative hypothesis is that the data follow a monotonic trend (read the help: ?mk.test). The function to calculate this test is included in the package “trend”. ## ## Mann-Kendall trend test ## ## data: AirPassengers ## z = 14.382, n = 144, p-value &lt; 2.2e-16 ## alternative hypothesis: true S is greater than 0 ## sample estimates: ## S varS tau ## 8.327000e+03 3.351643e+05 8.098232e-01 7.3.3 Other Decomposition Methods in R There are many different methods to decompose (and adjust) time series. Besides the classic decompose function, the following can be mentioned: STL (in the base-R stats library): Decompose a time series into seasonal, trend and irregular components using loess [X11], a method for decomposing quarterly and monthly data developed by the US Census Bureau and Statistics Canada, and the [SEATS] methods, implemented in the seasonal package. 7.4 White Noise and Stationarity We said that the residual part of the model should be (approximately) random, which indicates that the model explained most of the significant patterns in the data (the “signal”), leaving out the “noise”. The standard model of independent random variation in time series analysis is known as white noise (a term coined in an article published in Nature in 1922, where it was used to refer to series that contained all frequencies in equal proportions, analogous to white light). The charts below show how a white noise process looks like. When we introduced the concept of deterministic and stochastic trend, we said that the series showing the first type of trend are also called trend stationary, and the series showing the second type of trend are also called difference stationary. Both these names refer to the concept of stationarity. A process is stationary if it is homogeneous, that is, if it has no distinguished points in times or, in other words, its statistical qualities are the same for any point in time. There are more or less stringent definition of stationarity (strict and weak stationarity), and the most used for practical purposes is the so-called weak-stationarity. In this sense, a time series is said to be stationary if there is: no trend (no systematic change in mean, that is, time invariant mean), and no seasonality (no periodic variations); no change in variance over time (time invariant variance); no auto-correlation (we’ll return to this topic in the next chapters) White noise is an example of stationary time series. As you can see in the chart above, white noise time series is pretty regular, the mean is always the same (0) and there are no changes in variance over time: the plot looks much the same at any point in time! Also through differencing, the series can achive a stationary form. This is the case, in particular, of the series with a stochastic trend, that are also called difference-stationary, exactly because through differencing they become stationary. The process through which stationarity is reached is also called Pre-Whitening, and it can be used as a pre-processing phase before conducting correlation and regression analysis: Once the form(s) of serial dependency that best account for the series are identified, they are removed from the series. This is called prewhitening and is used to produce a series that is a “white noise” process (i.e., a process that is free of serial dependency with each value statistically independent of other values in the series). Once pre-whitening is accomplished the values of that series can be correlated with, used to predict, or predicted from, the values in other contemporaneous time series (usually also pre-whitened) representing other variables of interest. By removing serial dependency, the pre-whitening process makes these analyses free of correlated errors. It also removes the possibility that a common temporal trend or pattern is a confounding explanation for the observed association between the two variable series (VanLear, “Time Series Analysis”) The logic behind the process and the importance of white noise is also well explained in these sentences: This “residual” part of the data, indeed, can be used as a dependent variable, giving the analyst confidence that any time series properties in the data will not account for any observed correlation between the covariates and the dependent variable. In the univariate context, the white noise process is important because it is what we would like to “recover” from our data – after stripping away the ways in which a univariate series can essentially explain itself. By removing the time series properties of our data, leaving only white noise, we have a series that can then be explained by other sources of variation. Another way to conceptualize white noise is as the exogenous portion of the data-generating process. Each of our data series is a function of those forces that cause the series to rise or fall (the independent variables we normally include to test our hypotheses) and of time series properties that lead those forces to be more or less “sticky”. After we filter away those time series properties, we are left with the forces driving the data higher or lower. We often refer to these forces as shocks – and these shocks then reverberate in our data, sometimes for a short spell or a long spell or even infinitely. The goal of time series analysis is to separately model the time series properties (the reverberations) so the shocks (i.e., the white noise) can be captured. (Box-Steffensmeier, J. M., Freeman, J. R., Hitt, M. P., &amp; Pevehouse, J. C. (2014). Time series analysis for the social sciences. Cambridge University Press.) "],["correlations-and-arima.html", "Chapter8 Correlations and ARIMA 8.1 Auto-Correlation (ACF and PACF) 8.2 ARIMA models 8.3 Cross-correlation 8.4 Examples in literature", " Chapter8 Correlations and ARIMA 8.1 Auto-Correlation (ACF and PACF) In the previous chapter we said that a time series is said to be stationary if there is: no trend (no systematic change in mean, that is, time invariant mean), and no seasonality (no periodic variations); no change in variance over time (time invariant variance); no auto-correlation (we’ll return to this topic in the next chapters) Auto-correlation or serial correlation is an important characteristic of time series data and can be defined as the correlation of a variable with itself at different time points. Autocorrelation has many consequences. It prevents us to use traditional statistical methods such as linear regression, which assume that the observations are independent from each other. In presence of autocorrelation, the estimated standard errors of the parameter estimates will tend to be less than their true value. This will lead to erroneously high statistical significance being attributed to statistical tests (the p values will be smaller than they should be). In this section we introduce an important tool for the diagnosis of the properties of a time series, including autocorrelation: the correlogram. The accurate study of correlogram is a common step in many time series analysis procedures. 8.1.1 Correlogram: ACF and PACF The correlogram is a chart that presents one of two statistics: the autocorrelation function (ACF).The ACF statistic measures the correlation between \\(x_t\\) and \\(x_{t+k}\\) where k is the number of lead periods into the future. It measures the correlation between any two points based on a given interval. It is not strictly equivalent to the Pearson product moment correlation. In R, ACF is calculated and visualized with the function “acf”; the partial autocorrelation function (PACF). The PACF(k) is a measure of correlation between times series observations that are k units apart, after the correlation at intermediate lags has been controlled for or “partialed” out. In other words, the PACF measures the correlation between \\(x_t\\) and \\(x_{t+k}\\) after it has stripped out the effect of the intermediate x’s. In R, the PACF is calculated and visualized with the function “pacf”. It is useful to detect correlations that are not evident in ACF. Let’s consider, as an example, the correlogram of a random walk process. We know that this is a particular time series process in which the current values are combinations of the previous ones (\\(x_t = x_{t-1} + w_t\\), where \\(x_{t-1}\\) is the value immediately before x, and \\(w_t\\) is a random component). The resulting time series is characterized by a discernible pattern over time which is not exactly predictable (stochastic trend). The ACF of a random walk time series, indeed, shows a correlation between values in the series: even values not so close are notwithstanding correlated. Instead, the PACF, which removes the correlations between intermediate values, shows a correlation at lag 1, that is, it shows that the overall correlation depends on consequent values. The dotted blue lines signal the boundaries of statistical significance. We can clearly visualize the auto-correlation by using a simple scatterplot, by plotting two consecutive lines of points. The ACF or PACF of a white noise process is very different. We know that white noise is a stationary process, without distinguishable points in time and no correlation between points. Indeed, the ACF of white noise shows no correlation (the only line above statistical significance is at zero, which is nothing to be worried about, since it just means that each point is correlated with itself). In the PACF we can see that there is nothing above the dotted line (which means that there is nothing statistically significant). If we plot two consecutive lists of points by using a scatterplot, we can see there is no serial correlation (no pattern is visible): 8.2 ARIMA models The ACF and PACF plots can be used to diagnose the main characteristics of a time series and find a proper statistical model. We talk about univariate models, since they are models to describe a single time series. Univariate time series can be modeled as Auto Regressive (AR), Integrated (I), and Moving Average (MA) processes. These models are synthesized using the acronym ARIMA. When a seasonal (S) component is also taken into account, we also use the acronym SARIMA. 8.2.1 Auto Regressive (AR) models We just said that a time series is often characterized by auto-correlation, so we can clearly deduce that we can model it by using a regression model, that is, by regressing the time series on its past values. In this way we have an auto-regressive model: a regression of \\(x_{t}\\) on past terms \\(x_{t-k}\\) from the same series. In time series analysis, past terms \\(x_{t-k}\\) from a same series are called lags. The lagged values of a time series are its delayed values, where the delay can be of an arbitrary amount of time \\(k\\). For instance, considering a simple series of 4 data points distributed from time \\({t+0}\\) (first data point) to time \\({t+3}\\) (last data point) \\({x_{t+0}, x_{t+1}, x_{t+2}, x_{t+3}}\\), the corresponding lagged series, assuming \\(k=1\\), is \\({NA, x_{t+0}, x_{t+1}, x_{t+2}}\\). Notice that the first data point is missing since there is no data point behind it, and the other data points are shifted one time point ahead. An auto-regressive (AR) model can be described as follows (the \\(\\alpha\\) are coefficients, \\(t\\) are time points, \\(w\\) is a random component or white noise): \\[ {x_t} = \\alpha x_{t-1} + \\alpha x_{t-2} + \\alpha x_{t-k} + {w_t} \\] The ACF of an autoregressive process typically shows a slow and gradual decay in autocorrelation over time. The PACF of an autoregressive process shows a peak in correspondence with the order of the model. In the case of an AR(1) the peak is at time 1. In the case of an AR(3) the peak is at time 1, 2, and 3. Now we can see that the random walk process we have seen above is a particular case of auto-regressive model. In a random walk process, each value is the previous one plus a random part: \\[ x_t = x_{t-1} + w_t \\] Thus each point \\(x_t\\) is correlated with the previous one \\(x_{t-k}\\) where the lag value \\(k\\) is equal to 1 (\\({k=1}\\)). Therefore, a random walk process is an auto-regressive model of order 1, since just 1 lag is taken into consideration in the auto-regressive model (and with \\(\\alpha = 1\\)). The order of an auto-regressive model is indicated by parenthesis, e.g.: AR(1). The AR process can have different characteristics (and different ACF and PACF) based on the parameters. 8.2.2 Moving Average (MA) models We already know Moving Average as a method to smooth time series and detect a trend. When referring to Moving Average as a process (MA), we refer to a process in which the values of the series are a function of a weighted average of past errors. In other terms, a moving average (MA) process is a linear combination of the current white noise term and the \\(q\\) most recent past white noise terms: \\[ {x_t} = w_t + \\beta w_{t-1} + ... + \\beta w_{t-q} \\] The order of a MA process indicates the lags of white noise taken into account in the model (e.g: MA(3)). The ACF plot of a MA process shows a more clear cut-off after the term corresponding to the order ot the process. It is different from the ACF of an AR process, which shows a more gradual decay. The PACF of a MA process shows an up-and-down movement and does not shut off, but instead tapers toward 0 in some manner. 8.2.3 Integrated (I) process An integrated process is a non-stationary time series process that becomes stationary when transformed by differencing. In other words, an integrated process is a difference-stationary process, that is a process with a stochastic trends (see the previous chapter). 8.2.4 Seasonal (S) models We already introduced the seasonal model. For instance, a dataset showing a seasonal component is AirPassengers. The seasonality appears in the yearly fluctuations in the ACF and in the spikes occurring at 12 months from each other in the PACF. 8.2.5 Fit (S)ARIMA models The above examples represent simple processes, but real time series are often the result of more complex mixtures of different types of process, and therefore it is more complex to identify an appropriate model for the data. A popular methods to find the appropriate model is the Box-Jenkins method, a recursive process involving the analysis of a time series, the guess of possible (S)ARIMA models, the fit of the hypothesized models, and a meta-analysis to determine the best specification. Once a best-fitting model has been found, the correlogram of the residuals should be verified as white noise. The Box-Jenkins method could be time-consuming and requires some expertise. ACF/PACF can also become difficult to read in case of complex models, and their appropriate interpretation could require a lot of expertise as well. Fortunately, experts have developed automated methods that allow us to automatically found and fit an ARIMA model. This is the case of the auto.arima function implemented in the forecast package (a package for time series analysis and especially for forecasting, developed by Rob J. Hyndman, professor of statistics and time series analysis expert). ## Series: arima_112 ## ARIMA(1,1,2) ## ## Coefficients: ## ar1 ma1 ma2 ## 0.7649 0.7387 0.2484 ## s.e. 0.0356 0.0528 0.0534 ## ## sigma^2 = 1.035: log likelihood = -717.9 ## AIC=1443.8 AICc=1443.88 BIC=1460.65 As said above, to evaluate the fit of a model we should analyze the residuals, and ascertain they behave as white noise. The object resulting from the function auto.arima has a slot including the residuals. To ascertain that residuals are white noise we can plot its ACF and PACF (no spike should be significant) and also its histogram. The forecast package also implements the function checkresiduals to create nice and complete plots of residual diagnostics by using a simple function. Besides creating the plots, the function calulate the Ljung-Box test (default), or the Breusch-Godfrey test (if you specify test=“BG” inside the function): The Ljung-Box test (and also the Breusch–Godfrey test) is a diagnostic tool, applied to the residuals of a time series after fitting an ARIMA model, to test the lack of fit. The test examines the autocorrelations of the residuals. If there are no significant autocorrelations, it can be concluded that the model does not exhibit significant lack of fit. To pass the test, the p-value has to be above the significance level (usually 0.05) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,1,2) ## Q* = 5.8977, df = 7, p-value = 0.5517 ## ## Model df: 3. Total lags used: 10 8.2.5.1 SARIMA If we fit a model to the AirPassengers dataset, which has a seasonal component, we find a Seasonal Autoregressive Integrated Moving Average model (SARIMA). The seasonal component of the AirPassenger dataset is evident in the plot of the series and its ACF and PACF. The forecast package has a useful function ggtsdisplay to plot a time series along with its ACF and PACF. The seasonal part of the ARIMA model consists of terms that are similar to the non-seasonal components of the model, but involves lagged values of the seasonal period. ## Series: window(AirPassengers) ## ARIMA(2,1,1)(0,1,0)[12] ## ## Coefficients: ## ar1 ar2 ma1 ## 0.5960 0.2143 -0.9819 ## s.e. 0.0888 0.0880 0.0292 ## ## sigma^2 = 132.3: log likelihood = -504.92 ## AIC=1017.85 AICc=1018.17 BIC=1029.35 8.2.5.2 Forecasting Based on the ARIMA models we found, we can also try to forecast future values. We can use the function forecast of the homonym library. For instance, we could try to forecast the values of the AirPassenger dataset in the next four years. 8.3 Cross-correlation Cross-correlation is the correlation between the (lagged) values of a time series and the values of another series. Similarly to ACF and PACF, there is a specific plot that shows the cross-correlation between two time series, and a specific R function: ccf. The cross-correlation can be useful to understand wich lagged values of a X series can be used to predict the values of a Y series, and thus used, for instance, in a time series regression model. Unfourtunately, the problem with the cross-correlation function is that, as we said in the preceding sections, with autocorrelated data it is difficult to assess the dependence between two processes, and it is possible to find spurious correlations. Thus, it is pertinent to disentangle the linear association between X and Y from their autocorrelation. A useful device for doing this is prewhitening. The prewhitening method works as follows: determine an ARIMA time series model for the X-variable, and store the residuals from this model; fit the ARIMA X-model to the Y-variable, and keep the residuals; examines the CCF between the X and Y model residuals. We can implement this procedure writing all the necessary code, or by using the library forecast, or also, alternatively, the library TSA. To make an example, we apply the method to two simulated two series. The Y-variable is created in such a way that it is correlated with the lagged values at time \\(x_{t-3}\\) and \\(x_{t-4}\\). Therefore, we should find a correlation at those lags. The cross-correlation applied to the original series results in a plot where everything seems to be correlated. The “real” correlation at \\(x_{t-3}\\) and \\(x_{t-4}\\) is not discernible at all. By using the forecast library, we can calculate the pre-withened ccf as follows: Now, it’s clear that the X-variable is correlated with the Y-variable at \\(x_{t-3}\\) and \\(x_{t-4}\\). The previous steps show in some detail the steps involved in the pre-whitening strategy, but it is possible to use the original series with the prewhiten function of the TSA library. Although the function can take, as an argument, a pre-fitted ARIMA model, its greater advantage is that it can take care of all the necessary steps to prewithen the series. In particular, if no model is specified, the library automatically applies a simple AR model. Although this model can be just an approximation of the “true” model (which can be more complex), an approximation can be enough to pre-whiten the series and find a proper cross-correlation (that is, also a simpler and approximate model can do the job). 8.4 Examples in literature A few examples to exemplify the use of ARIMA and Cross-Correlation in the scientific literature, with specific reference to communication science. In Scheufele, B., Haas, A., &amp; Brosius, H. B. (2011). Mirror or molder? A study of media coverage, stock prices, and trading volumes in Germany. Journal of Communication, 61(1), 48-70, the authors investigate “the short-term relationship between media coverage, stock prices, and trading volumes of eight listed German companies”, by using ARIMA and cross-correlation, in particular asking: RQ2: Do cross-lagged correlations between media coverage and stock prices or trading volumes differ according to the amount and the valence of coverage? RQ3: Do cross-lagged correlations between media coverage and stock prices or trading volumes differ according to the type of media (Financial Web sites, daily newspapers, and stock market TV shows) which reports on the company or stock? To answer these questions, the authors made use of time series analysis. In particular, they: estimated cross-lagged correlations between media coverage and stock prices or trading volumes, respectively. Basically, two steps of time-series analysis can be distinguished: (a) In the first step, each media time-series and each time-series of trading volumes was adjusted by ARIMA (Autoregressive Integrated Moving Average) modeling separately. The differences between the original time-series and its ARIMA model are called residuals and were used for analysis. Like with ordinary least squares regression, these residuals should not be auto-correlated. If the residuals are not auto-correlated, time-series analysis speaks of White Noise. This modeling technique called prewhitening was necessary to avoid spurious correlations. (…) (b) In the next step, cross-correlations between each adjusted media time-series and each adjusted stock series were calculated. (…) The coefficient expresses the strength of correlation, whereas the lags offer an insight into dynamics: Correlations at positive (negative) lags indicate that changes in media coverage proceeded (succeeded) shifts in stock prices or trading volumes. In Groshek, J. (2010). A time-series, multinational analysis of democratic forecasts and Internet diffusion. International Journal of Communication, 4, 33, the author examines the democratic effects that the Internet has shown using macro- level, cross-national data in a sequence of time–series statistical tests: this study relies principally on macro-level time–series democracy data from an historical sample that includes 72 countries, reaching back as far as 1946 in some cases, but at least from 1954 to 2003. From this sample, a sequence of ARIMA (autoregressive integrated moving average) time–series regressions were modeled for each country for at least 40 years prior to 1994. These models were then used to generate statistically-forecasted democracy values for each country, in each year from 1994 to 2003. A 95% confidence interval with an upper and lower democracy score was then constructed around each of the forecasted values using dynamic mean squared errors. The actual democracy scores of each country for each year from 1994 to 2003 were then compared to the upper and lower values of the confidence interval. In the event that the actual democracy level of any country was greater than the upper value of the forecasted democracy score during the time period of 1994 to 2003, Internet diffusion was investigated in case studies as a possible causal mechanism. In other terms, the author used a forecasting approach to predict the values of the series from 1994 to 2003, in order to find statistically significant differences between the predicted and the actual values. These discrepancies were interpreted as caused by factors that were not present in the past, and possibly by the introduction of the Internet. The study found that, based on the results of the 72 countries reported here, the diffusion of the Internet should not be considered a democratic panacea, but rather a component of contemporary democratization processes "],["regression.html", "Chapter9 Regression 9.1 Static and Dynamic Models 9.2 Regression models 9.3 Model Selection (AIC, AICc, BIC) 9.4 Some examples in the literature", " Chapter9 Regression In this chapter we are going to see how to conduct a regression analysis with time series data. Regression analysis is a used for estimating the relationships between a dependent variable (DV) (also called outcome or response) and one or more independent variables (IV) (also called predictors or explanatory variables). A standard regression model \\(Y\\) = \\(\\beta\\) + \\(\\beta x\\) + \\(\\epsilon\\) has no time component. Differently, a time series regression model includes a time dimension and can be written, in a simple and general formulation, using just one explanatory variable, as follows: \\[ y_t = \\beta_0 + \\beta_1x_t + \\epsilon_t \\] In this equation, \\(y_t\\) is the time series we try to understand/predict (the dependent variable (DV)), \\(\\beta_0\\) is the intercept (a constant value that represents the expected mean value of \\(y_t\\) when \\(x_t = 0\\)), the coefficient \\(\\beta_1\\) is the slope, representing the average change in \\(y\\) at one unit increase in \\(x\\) (the independent variable (IV) or explanatory variable), and \\(\\epsilon_t\\) is the time series of residuals (the error term). A multiple regression, with more than one explanatory variable, can be written as follows: \\[ y_t = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t} + ... + \\beta_kx_{k,t} + \\epsilon_t \\] 9.1 Static and Dynamic Models From a time series analysis perspective, a general distinction can be made between “static” and “dynamic” regression models: A static regression model includes just contemporary relations between the explanatory variables (independent variables) and the response (dependent variable). This model could be appropriate when the expected value of the response changes immediately when the value of the explanatory variable changes. Considering a model with \\(k\\) independent variables {\\(x_1\\), \\(x_2\\), …, \\(x_k\\)}, a static (multiple) regression model, has the form just seen above: \\[ y_t = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t} + ... + \\beta_kx_{k,t} + \\epsilon_t \\] Each \\(\\beta\\) coefficient models the instant change in the conditional expected value of the response variable \\(y_t\\) as the value of \\(x_{k,t}\\) changes by one unit, keeping constant all the other predictors (i.e.: the other \\(x_{k,t}\\)): A dynamic regression model includes relations between both the current and the lagged (past) values of the explanatory (independent) variables, that is, the expected value of the response variable may change after a change in the values of the explanatory variables. \\[ \\begin{split} y_t &amp;= \\beta_0 + \\beta_{10} x_{1,t} + \\beta_{11} x_{1,t-1} + \\dots + \\beta_{1m} x_{1,t-m} \\\\ &amp;\\quad + \\beta_{20} x_{2,t} + \\beta_{21} x_{2,t-1} + \\dots + \\beta_{2m} x_{2,t-m} \\\\ &amp;\\quad + \\dots \\\\ &amp;\\quad + \\beta_{k0} x_{k,t} + \\beta_{k1} x_{k,t-1} + \\dots + \\beta_{km} x_{k,t-m} \\\\ &amp;\\quad + \\epsilon_t \\end{split} \\] Despite the differences between these two analytic perspectives, the term dynamic regression is also used, in the literature, in a more general way to refer to regression models with autocorrelated errors (also when they are used to analyze only contemporary relations between variables). 9.2 Regression models Except for the possible use of lagged regressors, which are typical of time series, the above described statistical models are standard regression models, commonly used with cross-sectional data. Standard linear regression models can sometimes work well enough with time series data, if specific conditions are met. Besides standard assumptions of linear regression1, a careful analysis should be done in order to ascertain that residuals are not autocorrelated, since this can cause problems in the estimated model. In this chapter we’ll see how to deal with autocorrelated residuals. However, even before that, it is important that the series are stationary, in order to avoid possible spurious correlations. 9.2.1 Stationarity We already discussed stationarity in the previous chapters. Here we can observe that time series can be nonstationary due to different reasons, thus different strategies can be employed to stationarize the data. For instance, a nonstationary series can be a series with unequal variance over time. A common way to try to fix the problem is by applying a log-transformation. Another reason for nonstationarity is the periodic variation due to seasonality (regular fluctuations in a time series that follow a specific time pattern, e.g.: social media activity during week-ends, Christmas effect in consumption, etc.). To remove the seasonal pattern, you might want to use a seasonally-adjusted time series. Otherwise, you could create a dummy variable for the seasonal period (that is, a variable that follows the seasonal pattern in the data in order to account, in the model, for these fluctuations). An important reason for nonstationarity is also the presence of a trend in the data. There are stochastic trends and deterministic trends. Deterministic trends are a fixed function of time, while stochastic trends change in an unpredictable way. Series with a deterministic trend are also called trend stationary because they can be stationary around a deterministic trend, and it could be possible to achieve stationarity by removing the time trend. In trend stationary processes, the shocks to the process are transitory and the process is mean reverting. Processes with a stochastic trend are also called difference stationary because they can become stationary through differencing. In series with stochastic trends we could see that shocks have permanent effects. When dealing with deterministic trend, we might want to work with detrended series. Otherwise, in regression analysis, it is more common to add a dummy variable consisting of a value that increases with time, to account for a linear deterministic time trend. This time-count variable will remove the deterministic trend from the dependent variable, allowing the other predictors to explain the remaining variance. When we have a series with a stochastic trend, we can achieve stationarity through differencing. 9.2.1.1 Tests for Stochastic and Deterministic Trend The correct detrending method depends on the type of trend. First differencing is appropriate for intergrated I(1) time series and time-trend regression is appropriate for trend stationary I(0) time series. In case of deterministic trend, differencing is the incorrect solution, while detrending the series in function of time (regressing the series on a variable such as time and saving the residuals) is the correct solution. Differencing when none is required (over-differencing) may induce dynamics into the series that are not part of the data-generating process (for instance, it could create a first-order moving average process). Specific statistical tests have been developed to distinguish between the two types of trends. In particular, unit root tests and stationary test can be used to determine if trending data should be first differenced or regressed on deterministic functions of time to render the data stationary. Considering a simple model like the following, where \\(Td\\) is a deterministic linear trend and \\(z_t\\) is an autoregressive process of order 1 AR(1). The difference between a process with stochastic and deterministic trend can be traced back to the parameter \\(|\\phi|\\): When \\(|\\phi| = 1\\), then \\(z_t\\) is a stochastic trend and \\(y_t\\) is an integrated process I(1) with drift (the so-called “drift” refers to the presence of a constant term, in this case \\(\\kappa\\)). When \\(\\phi &lt; 1\\), the process is not integrated (I(0)) and \\(y_t\\) exhibits a deterministic trend2: \\[ \\begin{split} y_t &amp;= Td_t + z_t \\\\ Td_t &amp;= \\kappa + \\delta_t \\\\ z_t &amp;= \\phi z_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\sim N(0, \\sigma^2) \\end{split} \\] Let’s simulate and visualize the above equation (\\(y_t = \\kappa + \\delta_t + \\phi z_{t-1} + \\epsilon_t\\)): Unit root tests are aimed at testing the null hypothesis that \\(|\\phi| = 1\\) (difference stationary), against the alternative hypothesis that \\(|\\phi| &lt; 1\\) (trend stationary). Stationarity tests take the null hypothesis that \\(y_t\\) is trend stationary, and are based on testing for a moving average element in \\(\\Delta z_t\\) (\\(\\Delta\\) represents the operation of differencing). \\[ \\begin{split} \\text{Original} \\\\ y_t &amp;= Td_t + z_t \\\\ Td_t &amp;= \\kappa + \\delta_t \\\\ z_t &amp;= \\phi z_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\sim N(0, \\sigma^2) \\end{split} \\] \\[ \\begin{split} \\text{First difference} \\\\ \\Delta y_t &amp;= \\Delta Td_t + \\Delta z_t \\\\ \\Delta Td_t &amp;= \\Delta \\kappa + \\Delta \\delta_t = \\delta \\\\ \\Delta z_t &amp;= \\phi \\Delta z_{t-1} + \\Delta \\epsilon_t = \\phi \\Delta z_{t-1} + \\epsilon_t - \\epsilon_{t-1} \\end{split} \\] \\(\\Delta z_t\\) can be also written as: \\[ \\Delta \\epsilon_t = \\phi \\Delta z_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1} \\] with \\(\\theta = -1\\). That is, when the series is trend stationary, taking the first difference results in overdifferencing and in the creation of a moving average (MA) term \\(\\theta \\epsilon_{t-1}\\). The creation of a moving average element, which is missing in the original series, is also why differencing a trend-stationary process is problematic. 9.2.1.1.1 KPSS Test A test to verify if the series is trend stationary is the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. It is one of the most commonly used stationarity test, and is implemented in the library tseries (function kpss.test). KPSS test the null hypothesis that the series is trend stationary. In this case, the p-value of the test is higher than 0.05, so the test cannot reject the null hypothesis of trend stationarity. That is to say, there are some evidence of trend-stationary process. ## ## KPSS Test for Trend Stationarity ## ## data: y_I0 ## KPSS Trend = 0.10404, Truncation lag ## parameter = 5, p-value = 0.1 By changing the null from “Trend” to “Level”, the KPSS test can also test the null hypothesis of level stationarity. A level stationary time series is a time series with a non-zero but constant mean, that is to say, without trend. ## ## KPSS Test for Level Stationarity ## ## data: y_I0 ## KPSS Level = 8.4092, Truncation lag ## parameter = 5, p-value = 0.01 In this case, the KPSS test for level stationarity reject the null hypothesis, that is to say, the process seems not to be level stationary. Considered together, the KPSS tests suggest that the series has a deterministic trend. If we use the KPSS test to test if the stochastic trend series we created above is trend or level stationary, the test rejects the null hypothesis (i.e.: reject the hypothesis of both a trend and level stationary process). ## ## KPSS Test for Trend Stationarity ## ## data: y_I1 ## KPSS Trend = 1.5, Truncation lag parameter = ## 5, p-value = 0.01 ## ## KPSS Test for Level Stationarity ## ## data: y_I1 ## KPSS Level = 7.7377, Truncation lag ## parameter = 5, p-value = 0.01 When a series has a stochastic trend, we can achieve stationarity through differencing. Indeed, the KPSS test does not reject the null hypothesis of level stationarity when applied to the the stochastic-trend series, once differenced. ## ## KPSS Test for Level Stationarity ## ## data: diff(y_I1) ## KPSS Level = 0.18432, Truncation lag ## parameter = 5, p-value = 0.1 In the above cases the KPSS results are correct, since we have simulated and tested a time series with a deterministic and stochastic trend. However, these kind of tests can also be wrong. For instance, it is possible they reject the null hypothesis when it is actually true (“Type I error”). For this reason, it can be useful to use more than one test. For instance, the KPSS can be used along with the Augmented Dickey-Fuller Test (ADF), a popular unit root test. 9.2.1.1.2 Augmented Dickey-Fuller (ADF) Test The Augmented Dickey-Fuller Test (ADF) is a popular unit root test. An R implementation of the test can be found in the library tseries (function adf.test). The null hypothesis is that the series has a unit root, and the alternative hypothesis is that the series is stationary or trend stationary. If we use the ADF test on the integrated series (which has a unit root), the test fails to reject the null hypothesis of unit root, which is correct. ## ## Augmented Dickey-Fuller Test ## ## data: y_I1 ## Dickey-Fuller = -1.7632, Lag order = 7, ## p-value = 0.6785 ## alternative hypothesis: stationary If we use the ADF test on the trend-stationary series (without unit root), the test rejects the null hypothesis of unit root, which is correct. ## ## Augmented Dickey-Fuller Test ## ## data: y_I0 ## Dickey-Fuller = -5.8397, Lag order = 7, ## p-value = 0.01 ## alternative hypothesis: stationary If we use the ADF test on the integrated series, after having transformed it through differencing, the test rejects the null hypothesis of unit root, which is correct. ## ## Augmented Dickey-Fuller Test ## ## data: diff(y_I1) ## Dickey-Fuller = -7.8913, Lag order = 7, ## p-value = 0.01 ## alternative hypothesis: stationary 9.2.1.1.3 Phillips-Perron Test Another unit root test is the Phillips-Perron test. It differs from the ADF test in some aspects (how it deals with serial correlation and heteroskedasticity in the errors). Also this test is implemented in the library tseries (funtion pp.test). Results of the test are similar to those of the ADF test: ## ## Phillips-Perron Unit Root Test ## ## data: y_I0 ## Dickey-Fuller Z(alpha) = -144.04, Truncation ## lag parameter = 5, p-value = 0.01 ## alternative hypothesis: stationary ## ## Phillips-Perron Unit Root Test ## ## data: y_I1 ## Dickey-Fuller Z(alpha) = -5.4817, Truncation ## lag parameter = 5, p-value = 0.8039 ## alternative hypothesis: stationary ## ## Phillips-Perron Unit Root Test ## ## data: diff(y_I1) ## Dickey-Fuller Z(alpha) = -489.82, Truncation ## lag parameter = 5, p-value = 0.01 ## alternative hypothesis: stationary In case of uncertainty, more than one test can be used. 9.2.2 Non-autocorrelated residuals We try to fit a linear regression model. First, we create two series \\(x\\) and \\(y\\), with \\(x\\) correlated with \\(y\\) at lags \\(x_{t-3}\\) and \\(x_{t-4}\\). The real model (in this case we know it because we created it through the above simulation), is as follows: \\[ y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \\epsilon_t \\\\ \\epsilon \\sim N(0, 1) \\] #### lm To fit a linear regression, we can use the function lm (the standard funtion to perform linear regression analysis in base R, no additional packages are necessary). The function summary prints the summary of the model, which includes the estimates (the “coefficients” of the variables), the standard errors, the statistical significance of the variables, and other information. ## ## Call: ## lm(formula = xy_series[, 1] ~ xy_series[, 2] + xy_series[, 3]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.57051 -0.73392 0.06238 0.74187 2.14794 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 14.96869 0.07238 206.80 ## xy_series[, 2] 0.85549 0.07680 11.14 ## xy_series[, 3] 1.42126 0.07678 18.51 ## Pr(&gt;|t|) ## (Intercept) &lt;2e-16 *** ## xy_series[, 2] &lt;2e-16 *** ## xy_series[, 3] &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.002 on 196 degrees of freedom ## Multiple R-squared: 0.873, Adjusted R-squared: 0.8717 ## F-statistic: 673.5 on 2 and 196 DF, p-value: &lt; 2.2e-16 We said that regression models sometimes work well enough with time series data, if specific conditions are met. Regards the conditions (or assumptions), in particular, the residuals of the models should have zero mean, they shouldn’t show any significant autocorrelation, and they should be normally distributed. To check whether these assumptions are met, we can visualize the plot of residuals, its ACF/PACF and histogram, and also test the residuals for possible autocorrelation using a statistical test like the Breusch-Godfrey test (this test is the default in the forecast library when a linear regression object lm is tested). To create the plots we can use the base R functions, or we can use the convenient checkresiduals function in the forecast package. In this case everything seems fine. ## ## Breusch-Godfrey test for serial correlation ## of order up to 10 ## ## data: Residuals ## LM test = 8.689, df = 10, p-value = 0.5618 If we look at the model summary printed above, we can see that the estimated model is the following (the standard deviation of residuals is misnamed as “residual standard error” in the summary of lm): \\[ y_t = 14.96869 + 0.85549x_{t-3} + 1.42126x_{t-4} + \\epsilon_t \\\\ \\epsilon \\sim N(0, 1.002^2) \\] The estimated model is also close to the “true” model: \\[ y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \\epsilon_t \\\\ \\epsilon \\sim N(0, 1) \\] #### dynml Instead of lm, the package dynml and the function with the same name (dynml) can be used to fit a dynamic regression models in R. One of the main advantages of this package is that it allows users to fit time series linear regression models without calculating the lagged values by hand. To add a lagged variable, it can simply be used the L (Lag) function. The L function takes as arguments the name of the variable and the lag length. For instance L(x, 4) corresponds to \\(x_{t-4}\\). ## ## Time series regression with &quot;ts&quot; data: ## Start = 5, End = 203 ## ## Call: ## dynlm(formula = y_series ~ L(x_series, 3) + L(x_series, 4)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.57051 -0.73392 0.06238 0.74187 2.14794 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 14.96869 0.07238 206.80 ## L(x_series, 3) 0.85549 0.07680 11.14 ## L(x_series, 4) 1.42126 0.07678 18.51 ## Pr(&gt;|t|) ## (Intercept) &lt;2e-16 *** ## L(x_series, 3) &lt;2e-16 *** ## L(x_series, 4) &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.002 on 196 degrees of freedom ## Multiple R-squared: 0.873, Adjusted R-squared: 0.8717 ## F-statistic: 673.5 on 2 and 196 DF, p-value: &lt; 2.2e-16 The dynlm function also permits to include trend (function trend) and seasonal (function season) components in the model (it is also possible to change the reference value for the seasonal period, see ?dynlm). Just to make an example of the code to perform a dynamic regression with dynlm: ## ## Time series regression with &quot;ts&quot; data: ## Start = 1949(7), End = 1960(12) ## ## Call: ## dynlm(formula = ap ~ trend(ap) + season(ap) + L(ap_x, 3)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.156306 -0.035098 0.007821 0.041535 0.141713 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 4.116392 0.224848 18.307 ## trend(ap) 0.105758 0.005596 18.899 ## season(ap)Feb -0.026936 0.024867 -1.083 ## season(ap)Mar 0.127650 0.026209 4.870 ## season(ap)Apr 0.111029 0.028324 3.920 ## season(ap)May 0.132132 0.031750 4.162 ## season(ap)Jun 0.248250 0.030001 8.275 ## season(ap)Jul 0.334578 0.027595 12.125 ## season(ap)Aug 0.333578 0.029138 11.448 ## season(ap)Sep 0.172679 0.026344 6.555 ## season(ap)Oct 0.034292 0.026311 1.303 ## season(ap)Nov -0.101556 0.027526 -3.689 ## season(ap)Dec -0.010445 0.024767 -0.422 ## L(ap_x, 3) 0.061495 0.022439 2.741 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## trend(ap) &lt; 2e-16 *** ## season(ap)Feb 0.280811 ## season(ap)Mar 3.32e-06 *** ## season(ap)Apr 0.000146 *** ## season(ap)May 5.86e-05 *** ## season(ap)Jun 1.71e-13 *** ## season(ap)Jul &lt; 2e-16 *** ## season(ap)Aug &lt; 2e-16 *** ## season(ap)Sep 1.35e-09 *** ## season(ap)Oct 0.194866 ## season(ap)Nov 0.000335 *** ## season(ap)Dec 0.673966 ## L(ap_x, 3) 0.007039 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05831 on 124 degrees of freedom ## Multiple R-squared: 0.9829, Adjusted R-squared: 0.9811 ## F-statistic: 546.9 on 13 and 124 DF, p-value: &lt; 2.2e-16 9.2.3 Regression with ARMA errors While in the previous case a standard linear model works well, it is often the case that residuals of times series regressions are autocorrelated, and a linear regression model can be suboptimal or even wrong. For instance, let’s create other two time series that are, as the previous ones, cross-correlated at lag 3 and 4, but with a bit more complicated structure. ## $ccf ## ## Autocorrelations of series &#39;X&#39;, by lag ## ## -19 -18 -17 -16 -15 -14 -13 ## -0.021 -0.029 0.012 0.065 0.024 0.008 0.076 ## -12 -11 -10 -9 -8 -7 -6 ## 0.107 0.001 -0.031 -0.024 -0.039 0.048 0.070 ## -5 -4 -3 -2 -1 0 1 ## -0.021 0.620 0.425 0.036 -0.001 0.052 0.080 ## 2 3 4 5 6 7 8 ## 0.068 -0.008 -0.011 0.096 0.124 0.062 -0.022 ## 9 10 11 12 13 14 15 ## 0.030 0.023 -0.035 -0.039 -0.008 0.016 -0.059 ## 16 17 18 19 ## -0.149 -0.120 -0.027 -0.020 ## ## $model ## ## Call: ## ar.ols(x = x) ## ## Coefficients: ## 1 2 3 4 5 ## 0.6362 0.1109 0.0496 -0.1533 -0.0240 ## 6 7 8 9 10 ## -0.0499 0.2041 0.0221 -0.0990 -0.0981 ## 11 12 13 14 ## 0.1703 -0.0558 -0.0812 0.1179 ## ## Intercept: 0.006091 (0.06307) ## ## Order selected 14 sigma^2 estimated as 0.7336 Considering the autocorrelated structure of the series, the true model can be written as follows: \\[ \\begin{split} y_t &amp;= 15 + 0.8 x_{t-3} + 1.5 x_{t-4} + \\eta_t \\\\ \\eta_t &amp;= 0.7 \\eta_{t-1} + \\epsilon_t + 0.6 \\epsilon_{t-1} \\\\ \\epsilon &amp;\\sim N(0, 1) \\end{split} \\] It is possible to calculate the regression using the lm function, calculating the lagged variables by hand, or to use the dynml library and function. ## ## Call: ## lm(formula = xy2_series[, 1] ~ xy2_series[, 3:4]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9200 -1.4508 0.0667 1.5395 5.1083 ## ## Coefficients: ## Estimate ## (Intercept) 14.9005 ## xy2_series[, 3:4]x2Lagged.xLag3 1.0407 ## xy2_series[, 3:4]x2Lagged.xLag4 1.5171 ## Std. Error t value ## (Intercept) 0.1486 100.273 ## xy2_series[, 3:4]x2Lagged.xLag3 0.1595 6.523 ## xy2_series[, 3:4]x2Lagged.xLag4 0.1599 9.488 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## xy2_series[, 3:4]x2Lagged.xLag3 6.14e-10 *** ## xy2_series[, 3:4]x2Lagged.xLag4 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.028 on 189 degrees of freedom ## (12 observations deleted due to missingness) ## Multiple R-squared: 0.6735, Adjusted R-squared: 0.67 ## F-statistic: 194.9 on 2 and 189 DF, p-value: &lt; 2.2e-16 ## ## Time series regression with &quot;ts&quot; data: ## Start = 5, End = 196 ## ## Call: ## dynlm(formula = y2_series ~ L(x2_series, 3) + L(x2_series, 4)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9200 -1.4508 0.0667 1.5395 5.1083 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 14.9005 0.1486 100.273 ## L(x2_series, 3) 1.0407 0.1595 6.523 ## L(x2_series, 4) 1.5171 0.1599 9.488 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## L(x2_series, 3) 6.14e-10 *** ## L(x2_series, 4) &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.028 on 189 degrees of freedom ## Multiple R-squared: 0.6735, Adjusted R-squared: 0.67 ## F-statistic: 194.9 on 2 and 189 DF, p-value: &lt; 2.2e-16 The estimated model is the following: \\[ \\begin{split} y_t &amp;= 14.9005 + 1.0407 x_{t-3} + 1.5171 x_{t-4} + \\epsilon_t \\\\ \\epsilon &amp;\\sim N(0, 2.028^2) \\end{split} \\] The original series can also be visualized with the fitted values (the values resulting from the model), to visually inspect how well the model represents the original series. The differences between the original and the fitted series are the residuals. The diagnostic plots of the residuals show the presence of autocorrelation, and the Breusch-Godfrey test is highly significant (its value is far lower than the critical value \\(\\alpha = 0.05\\)) ## ## Breusch-Godfrey test for serial correlation ## of order up to 10 ## ## data: Residuals ## LM test = 149.45, df = 10, p-value &lt; 2.2e-16 In this case, it’s better to take into account the residuals’ autocorrelation by using a regression model capable to handle autocorrelated time series structures. In the previous chapter we said that ARIMA models are a special type of regression model, in which the dependent variable is the time series itself, and the independent variables are all lags of the time series. This model is capable to take into account the autocorrelated structure of time series. ARIMA is a modeling technique that can be applied to a single time series, but it can be extended to include additional, exogenous variables. The ARIMA model including exogenous regressors (i.e.: other time series besides the lagged dependent variable) is like a multiple regression models for time series. In particular, it can be considered a regression model capable to control for autocorrelation in residuals. It is possible to use more than one option to fit an ARIMA model with external regressors. A convenient option is provided by the function auto.arima, in the package forecast. This library has an argument xreg which can be use with a numerical vector or matrix of external regressors, which must have the same number of rows as y (see ?auto.arima). ## Series: xy2_series[, 1] ## Regression with ARIMA(1,0,1) errors ## ## Coefficients: ## ar1 ma1 intercept x2Lagged.xLag3 ## 0.6863 0.6491 14.8532 0.9506 ## s.e. 0.0555 0.0528 0.3607 0.0757 ## x2Lagged.xLag4 ## 1.5732 ## s.e. 0.0762 ## ## sigma^2 = 0.9482: log likelihood = -265.76 ## AIC=543.52 AICc=543.97 BIC=563.06 The resulting model seems to be more appropriate than the previous one, fitted by using just a “classic” linear regression. This is clear also by comparing the two models through the AIC criterion (Akaike information criterion). The AIC value is used to compare the goodness-of-fit of different models fitted to the same dataset. The lower the AIC value, the better the fit (see also the next paragraph). The auto.arima function prints the AIC value by default, while this value is not given with the lm function. To get it, we need to use the AIC function. ## [1] 821.4495 In this case, the ARIMA regression model results a far better model (AIC=543.52) compared with the classic linear model (AIC=821.45). \\[ \\begin{split} y_t &amp;= 14.8532 + 0.9506 x_{t-3} + 1.5732 x_{t-4} + \\eta_t \\\\ \\eta_t &amp;= 0.6863 \\eta_{t-1} + \\epsilon_t + 0.6491 \\epsilon_{t-1} \\\\ \\epsilon &amp;\\sim N(0, 0.9482) \\end{split} \\] Diagnostic analysis of the residuals, shows that there is no concerning sign of autocorrelation in the residuals, which looks like white noise. Also the test for autocorrelated errors is not significant (the default test for autocorrelation when testing an ARIMA models with external regressors in the forecast package is the Ljung-Box test)3). ## ## Ljung-Box test ## ## data: Residuals from Regression with ARIMA(1,0,1) errors ## Q* = 6.3861, df = 8, p-value = 0.6041 ## ## Model df: 2. Total lags used: 10 Also by visually inspect the original series along with the fitted series (the values resulting from the model), it can be seen that the model is better than the previous one. We can also compare the fitted versus original values by using a scatterplot. A better model produces a thinner diagonal line. The auto.arima function does not give the statistical significance of the coefficients (the approach adopted by the forecast library is different, based on the choice of the best model to do forecasting), but it is possible to get that by using the function coeftest in the library lmtest. ## ## z test of coefficients: ## ## Estimate Std. Error z value ## ar1 0.686330 0.055525 12.361 ## ma1 0.649134 0.052850 12.283 ## intercept 14.853152 0.360689 41.180 ## x2Lagged.xLag3 0.950588 0.075705 12.557 ## x2Lagged.xLag4 1.573242 0.076224 20.640 ## Pr(&gt;|z|) ## ar1 &lt; 2.2e-16 *** ## ma1 &lt; 2.2e-16 *** ## intercept &lt; 2.2e-16 *** ## x2Lagged.xLag3 &lt; 2.2e-16 *** ## x2Lagged.xLag4 &lt; 2.2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.2.4 Count regression models The models described above are mostly used with continuous variables (expressed as numeric or double in the R data format). However, it is often the case that time series are composed of integer values, or count data (expressed as integer). Sometimes, the above mentioned methods work well also with this type of data (for instance, when the counts are large). Other times, time series model developed for count data can be a better choice (for instance, when the series include mostly small integer values). Two of the most common statistical models to deal with count data are based on the Poisson and the Negative Binomial distributions. These probability distributions are the ones that are usually employed to model count data. There are a few libraries to fit count time series regression models in R. We take into consideration tscount, and its function tsglm. We consider, as an example of the tscount function to fit count time series regression models, the dataset “Seatbelts” (monthly number of killed drivers of light goods vehicles in Great Britain between January 1969 and December 1984), following the paper that describes the tscount library. The authors of the package write in the paper (par. 7.2) describing the library: This time series is part of a dataset which was first considered by Harvey and Durbin (1986) for studying the effect of compulsory wearing of seatbelts introduced on 31 January 1983. The dataset, including additional covariates, is available in R in the object Seatbelts. In their paper Harvey and Durbin (1986) analyze the numbers of casualties for drivers and passengers of cars, which are so large that they can be treated with methods for continuous-valued data. The monthly number of killed drivers of vans analyzed here is much smaller (its minimum is 2 and its maximum 17) and therefore methods for count data are to be preferred. We are going to fit a model aimed at capturing a first order autoregressive AR(1) term and a yearly seasonality by a 12th order autoregressive term. The function tsglm allows users to declare the autoregressive and seasonal autoregressive terms in a convenient way (in the following part of the function: model = list(past_obs = c(1, 12))). It is possible to check the residuals with the usual plots. The function summary can be used to get the parameter estimates for the model (in this case the function can also employ a parametric bootstrap procedure (B) to obtain standard errors and confidence intervals of the regression parameters. The authors use B=500 in the original paper, since in their experience this value yields stable results. Higher B values can be more precise but require time to be calculated). ## ## Call: ## tsglm(ts = timeseries_until1981, model = list(past_obs = c(1, ## 12)), xreg = regressors_until1981, link = &quot;log&quot;, distr = &quot;poisson&quot;) ## ## Coefficients: ## Estimate Std.Error CI(lower) ## (Intercept) 1.83284 0.367830 1.11191 ## beta_1 0.08672 0.080913 -0.07187 ## beta_12 0.15288 0.084479 -0.01269 ## PetrolPrice 0.83069 2.303555 -3.68420 ## linearTrend -0.00255 0.000653 -0.00383 ## CI(upper) ## (Intercept) 2.55377 ## beta_1 0.24530 ## beta_12 0.31846 ## PetrolPrice 5.34557 ## linearTrend -0.00127 ## Standard errors and confidence intervals (level = 95 %) obtained ## by normal approximation. ## ## Link function: log ## Distribution family: poisson ## Number of coefficients: 5 ## Log-likelihood: -396.1765 ## AIC: 802.3529 ## BIC: 817.6022 ## QIC: 802.3529 The model is as follows: \\[ log(\\lambda_t) = 1.83 + 0.09Y_{t-1} + 0.15Y_{t-12} + 0.83X_t - 0.003t \\] In the above equation notice that, the Poisson regression, models the logarithm of the Y values at times t (expressed as \\(log(\\lambda_t)\\)). Another example (using the dataset you can download here): ## ## Autocorrelations of series &#39;X&#39;, by lag ## ## -1.2500 -1.1667 -1.0833 -1.0000 -0.9167 -0.8333 ## -0.127 0.064 -0.013 0.008 0.033 0.022 ## -0.7500 -0.6667 -0.5833 -0.5000 -0.4167 -0.3333 ## -0.041 -0.278 0.120 -0.313 0.417 0.282 ## -0.2500 -0.1667 -0.0833 0.0000 0.0833 0.1667 ## -0.061 -0.025 -0.075 -0.012 -0.180 0.063 ## 0.2500 0.3333 0.4167 0.5000 0.5833 0.6667 ## 0.039 -0.154 -0.070 -0.040 0.029 0.068 ## 0.7500 0.8333 0.9167 1.0000 1.0833 1.1667 ## -0.008 -0.045 -0.221 -0.068 -0.039 0.112 ## 1.2500 ## 0.177 Besides checking the residuals, it is possible to plot the PIT histogram, provided by the function pit in tscount: A PIT histogram is a tool for evaluating the statistical consistency between the probabilistic forecast and the observation. The predictive distributions of the observations are compared with the actual observations. If the predictive distribution is ideal the result should be a flat PIT histogram with no bin having an extraordinary high or low level. For more information about PIT histograms see the references listed below. In the library are included other diagnostic tools and metrics that can help choosing between poisson and negative binomial models (see the paper for further information). The function summary prints the coefficients of the model and their confidence interval. ## ## Call: ## tsglm(ts = qanon, model = list(past_obs = 1), xreg = reg, link = &quot;log&quot;, ## distr = &quot;poisson&quot;) ## ## Coefficients: ## Estimate Std.Error CI(lower) ## (Intercept) 0.58373 0.18036 0.230232 ## beta_1 0.83746 0.04834 0.742715 ## fake_news_lag4 0.00892 0.00274 0.003546 ## fake_news_lag5 0.00638 0.00354 -0.000554 ## fake_news_lag6 -0.01617 0.00272 -0.021506 ## CI(upper) ## (Intercept) 0.9372 ## beta_1 0.9322 ## fake_news_lag4 0.0143 ## fake_news_lag5 0.0133 ## fake_news_lag6 -0.0108 ## Standard errors and confidence intervals (level = 95 %) obtained ## by normal approximation. ## ## Link function: log ## Distribution family: poisson ## Number of coefficients: 5 ## Log-likelihood: -239.6582 ## AIC: 489.3163 ## BIC: 500.2646 ## QIC: 489.3163 9.3 Model Selection (AIC, AICc, BIC) Statistical modeling is, usually, a recursive process that requires to fit several different models and, eventually, to select the most appropriate one. For instance, the Box and Jenkins approach employed to find an appropriate ARIMA model for a time series (see the previous chapter), requires the fitting of multiple models to find the most suitable one based on the data. Similarly, the “auto.arima” function in the library forecast, that automatizes the search for an appropriate ARIMA model, conducts a search over possible model. To compare the models and select the most appropriate one, it is necessary to use some criteria. In the example above we have employed the AIC criterion. Other similar criteria are the AICc, and the BIC. They all can be used to find the most appropriate model, by comparing the goodness-of-fit of different models fitted to the same dataset. For instance, the documentation of the “auto.arima” function says that the function “returns best ARIMA model according to either AIC, AICc or BIC value”. The AIC criterion is the acronym for Akaike information criterion). The lower the AIC value, the better the fit (see also the next paragraph). The AICc criterion, is the same, but with a correction for small sample size. When the sample is small it can be used in place of the AIC criterion. As the sample size increases, the AICc converges to the AIC. The BIC criterion is the Bayesian Information Criterion (or Schwartz’s Bayesian Criterion) and has a stronger penalty than the AIC for overparametrized models (more complex models, with several predictors). These criteria can also be used when searching for an appropriate regression model, to compare several different models including different lags of the variables. When comparing models by using these criteria, it is important that the models are fitted to the same dataset, otherwise the results are not comparable. This is an important aspect to take into account when using lagged predictors. For instance, you may want to try a model including one lagged predictor \\(x_{t-1}\\) and a model including two lagged predictors \\(x_{t-1}\\) and \\(x_{t-2}\\), and to compare them in order to select the best one according to AIC, AICc or the BIC criterion. However, when you add lagged predictor you loose data points. ## Time Series: ## Start = 1 ## End = 42 ## Frequency = 1 ## y xLag0 xLag1 ## 1 1.075501365 0.48505236 NA ## 2 -0.276740732 -1.20993027 0.48505236 ## 3 -0.646751445 0.01724609 -1.20993027 ## 4 0.102585633 0.70085715 0.01724609 ## 5 -0.072519610 -0.40170226 0.70085715 ## 6 1.619712305 -0.48727576 -0.40170226 ## 7 0.687751998 -0.76436248 -0.48727576 ## 8 0.234929596 2.97223380 -0.76436248 ## 9 -0.430528202 -0.43578545 2.97223380 ## 10 0.638039146 -1.35197954 -0.43578545 ## 11 -0.217226469 -0.52868886 -1.35197954 ## 12 0.020494309 -0.06027948 -0.52868886 ## 13 -1.913357840 -0.03222441 -0.06027948 ## 14 -0.526191144 0.55290101 -0.03222441 ## 15 0.416282618 -0.25966064 0.55290101 ## 16 0.457627862 -1.01400343 -0.25966064 ## 17 -0.275639625 -0.86452761 -1.01400343 ## 18 -0.353882346 0.11040086 -0.86452761 ## 19 -0.901762561 -0.02452574 0.11040086 ## 20 -1.052145243 -2.51548495 -0.02452574 ## 21 0.832626641 0.44375488 -2.51548495 ## 22 0.056411159 1.56218440 0.44375488 ## 23 0.838708652 0.97180894 1.56218440 ## 24 0.344611778 1.58747428 0.97180894 ## 25 -1.088605832 -1.84530863 1.58747428 ## 26 -0.527820628 -1.27107477 -1.84530863 ## 27 3.061137014 2.03899229 -1.27107477 ## 28 -0.520213835 -0.72914933 2.03899229 ## 29 0.248244201 0.93447357 -0.72914933 ## 30 0.046969519 -0.75324932 0.93447357 ## 31 0.249449596 -0.68448064 -0.75324932 ## 32 1.041843001 -0.59473197 -0.68448064 ## 33 -0.338376495 -0.12412231 -0.59473197 ## 34 0.201287727 -1.39014027 -0.12412231 ## 35 0.602711731 -1.23556534 -1.39014027 ## 36 -0.225018148 -1.28500792 -1.23556534 ## 37 -0.010113590 -1.05225339 -1.28500792 ## 38 -0.193597244 1.36496602 -1.05225339 ## 39 0.009188137 -1.07721180 1.36496602 ## 40 1.441899269 0.67538151 -1.07721180 ## 41 NA NA 0.67538151 ## 42 NA NA NA ## xLag2 ## 1 NA ## 2 NA ## 3 0.48505236 ## 4 -1.20993027 ## 5 0.01724609 ## 6 0.70085715 ## 7 -0.40170226 ## 8 -0.48727576 ## 9 -0.76436248 ## 10 2.97223380 ## 11 -0.43578545 ## 12 -1.35197954 ## 13 -0.52868886 ## 14 -0.06027948 ## 15 -0.03222441 ## 16 0.55290101 ## 17 -0.25966064 ## 18 -1.01400343 ## 19 -0.86452761 ## 20 0.11040086 ## 21 -0.02452574 ## 22 -2.51548495 ## 23 0.44375488 ## 24 1.56218440 ## 25 0.97180894 ## 26 1.58747428 ## 27 -1.84530863 ## 28 -1.27107477 ## 29 2.03899229 ## 30 -0.72914933 ## 31 0.93447357 ## 32 -0.75324932 ## 33 -0.68448064 ## 34 -0.59473197 ## 35 -0.12412231 ## 36 -1.39014027 ## 37 -1.23556534 ## 38 -1.28500792 ## 39 -1.05225339 ## 40 1.36496602 ## 41 -1.07721180 ## 42 0.67538151 Thus, when you fit models with different lags, you have to fit them on the same dataset. In this case, for instance, you have to skip the NA rows, and use just the rows from 3 to 40. Then you can compare the model, for instance, using the AIC criterion, and choose the model with the smallest value. ## [1] 95.11836 ## [1] 94.31932 ## [1] 96.07911 Finally, you fit the model using all the available data. ## Series: example_data[, 1] ## Regression with ARIMA(0,0,0) errors ## ## Coefficients: ## xreg ## 0.2447 ## s.e. 0.1120 ## ## sigma^2 = 0.6511: log likelihood = -47.67 ## AIC=99.34 AICc=99.66 BIC=102.72 Besides these criteria, there are also other strategies for model selection. 9.4 Some examples in the literature There are several examples of the use of time series regression models in the literature in the field of communication science. For instance, in The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates4, the authors examined whether the UN climate change conferences are conducive to an emergence of a transnational public sphere by triggering issue convergence and increased transnational interconnectedness across national media debates. They authors detail the method they follows in this way: […] Given the autoregressive nature and other properties of time series, an ordinary least squares regression analysis would violate the normality of error and the independence of observations assumption (Wells et al., 2019). Instead, we applied the dynamic regression approach (Gujarati &amp; Porter, 2009; Hyndman &amp; Athanasopoulos, 2018), which assumes that the error term follows an autoregressive integrated moving average (ARIMA) model (…). we found the best ARIMA structure of the error term by using the auto.arima function from the forecast R package (Hyndman &amp; Khandakar, 2008). It searches for an ARIMA structure that can explain the most variance according to the Akaike information criterion (Akaike, 1973). In this case they use the term “dynamic regression” to refer to a time series regression with ARIMA errors, but they did not include lagged values of their variables, thus analyzing contemporary relationships between variables. The found, for instance, that events taking place on a supranational level of governance (…) consistently led to spikes in media attention across countries. In contrast, a bottom-up effort such as Fridays for Future showed an inconsistent relationship with media attention across the four countries. In Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event5, the authors used both standard regression and regression with ARIMA errors to show that “online incivility — operationalized as the use of foul language — grew as volume of political discussions and levels of cyberbalkanization increased. Incivility led to higher levels of opinion polarization.”. Also in this case the authors analyze a “static process”, that is, focus on contemporary relationships between variables. In Beyond cognitions: A longitudinal study of online search salience and media coverage of the president6, the authors used regression models with ARIMA errors to examine shifts in newswire coverage and search interest among Internet users in President Obama during the first two years of his administration (2009-2010). In this case, the authors analyze relationships between variables taking into account lagged values, thus adopting a “dynamic process” perspective. For instance, they write: RQ2 sought to determine the time span of linkages between coverage volume and search volume. (…) ARIMA models were run to gauge the dynamics of mutual influence between these two time series. The first model examined the effect of coverage volume on search volume over time (i.e., basic agenda setting) (…) presidential public relations, was included as an additional input series. The first model, with search volume being a single dependent variable, was identified through a close examination of autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs). This analysis revealed a classic autoregressive model for the series (1, 0, 0). […] According to the results, shifts in aggregate search volume over this two-year period were significantly predicted by coverage volume over the prior five weeks (p &lt; .010)* and by presidential public relations efforts in the preceding two, three (p &lt; .001), and five weeks (p &lt; .005). The ARIMA model with two predictors was correctly specified (Ljung–Box Q = 18.132, p = .381) and it explained roughly 35% of the observed variation in the series. In AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993–20077, the authors examined the effect of newspaper coverage of HIV/AIDS on HIV testing behavior in a U.S. population., using a lagged regression to support causal order claims by ensuring that newspaper coverage precedes the testing behavior with the inclusion of the 1-month lagged newspaper coverage variable in the model. Counterintuitively, they found that the news media coverage had a negative effect on testing behavior: For every additional 100 HIV/AIDS risk related newspaper stories published in this group of U.S. newspapers each month, there was a 1.7% decline in HIV testing levels in the following month, with a higher negative effects on African Americans. 1) Linearity: The relationship between X and Y must be linear; 2) Independence of errors: There is not a relationship between the residuals and the Y variable; 3) Normality of errors: The residuals must be approximately normally distributed; 4) Equal variances: The variance of the residuals is the same for all values of X↩︎ Reference of this part is Zivot E., Wang J. (2003), Unit Root Tests, in Modeling Financial Time Series with S-Plus®. Springer, New York↩︎ There are many tests for detecting autocorrelation. Besides the already mentioned Breusch-Godfrey test and Ljung-Box test, other popular tests are the Durbin Watson test, and the Box–Pierce test. Each test has its own characteristics. For instance, the Durbin-Watson test is a popular way to test for autocorrelation, but it shouldn’t be used with lagged dependent variables. In this case it can be used the Breusch-Godfrey test↩︎ Wozniak, A., Wessler, H., Chan, C. H., &amp; Lück, J. (2021). The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates. International Journal of Communication, 15(27)↩︎ Lee, F. L., Liang, H., &amp; Tang, G. K. (2019). Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event. International Journal of Communication, 13, 20.↩︎ Ragas, M. W., &amp; Tran, H. (2013). Beyond cognitions: A longitudinal study of online search salience and media coverage of the president. Journalism &amp; Mass Communication Quarterly, 90(3), 478-499.↩︎ Stevens, R., &amp; Hornik, R. C. (2014). AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993–2007. Journal of health communication, 19(8), 893-906↩︎ "],["intervention-analysis.html", "Chapter10 Intervention Analysis 10.1 Types of intervention 10.2 Intervention analysis with ARIMA", " Chapter10 Intervention Analysis In this chapter we are going to learn about intervention analysis (sometimes also called interrupted time-series analysis) and to see how to conduct a intervention analysis. Intervention analysis is typically conducted with the Box &amp; Jenkins ARIMA framework and traditionally uses a method introduced by Box and Tiao (1975)8, who provided a framework for assessing the effect of an intervention on a time series under study. As summarized by Box and Tiao: Given a known intervention, is there evidence that change in the series of the kind expected actually occurred, and, if so, what can be said of the nature and magnitude of the change?. In other words Intervention analysis estimates the effect of an external or exogenous intervention on a time-series. To conduct such an analysis, it is necessary to know the date of the intervention. Intervention analysis is a “quasi-experimental” design and an interesting approach to test whether exogenous shocks, such as, for instance, the introduction of a new policy, impact on a time series process in a significant way, that is, by changing the mean function or trend of a time series. Behind intervention analysis there is the causal hypothesis that observations after a treatment (the “intervention”) have a different level or slope from those before the intervention/interruption. Besides intervention or interrupted time-series analysis, the analysis can be conducted through the segmented regression method. However, as in the case of traditional regression models applied to time series data, this approach does not take into account the autocorrelated structure of time series. Other methods include more complex computational approaches. 10.1 Types of intervention There are different types of interventions. For instance, an intervention can have an abrupt impact determining a permanent or temporary change, a sudden and short-lived change due to an event, or a more gradual yet permanent change. 10.2 Intervention analysis with ARIMA To exemplify an intervention analysis we are going to reproduce the example in the paper Interrupted time series analysis using autoregressive integrated moving average (ARIMA) models: a guide for evaluating large-scale health interventions. The data to run the analysis can be downloaded here. The example evaulates the impact of a health policy intervention (an Australian health policy intervention that restricted the conditions under which a particular medicine (quetiapine) could be subsidised). The same methodological process can be applied to evaluate any intervention in any context. The case study is described as follows: (…) due to growing concerns about inappropriate prescribing, after January 1, 2014 new prescriptions for this tablet strength could not include refills. Our primary outcome was the number of monthly dispensings of 25 mg quetiapine, of which we had 48 months of observations (January 2011 to December 2014). Thus, data comprises 48 months of observations, and the date of the intervention is January 1, 2014. There is also seasonality in the process: In Australia, medicine dispensing claims have significant yearly seasonality. Medicines are subsidised for citizens and eligible residents through the Pharmaceutical Benefits Scheme (PBS), with people paying an out-of-pocket co-payment towards the cost of their medicines, while the remainder is subsidised. If a person’s (or family’s) total out-of-pocket costs reach the “Safety Net threshold” for the calendar year, they are eligible for a reduced co-payment for the remainder of that year. Thus, there is an incentive for people reaching their Safety Net to refill their medicines more frequently towards the end of the year. Hence, we see an increase in prescriptions at the end of the year, followed by a decrease in January. The researchers hypothesize the nature of the intervention as follows (see the picture below): (…) due to the nature of the intervention we postulated there would be an immediate drop in dispensings post-intervention (step change), as well as a change in slope (ramp). Thus, we included variables representing both types of impacts in our model. For both impacts, h = 0 and r = 0. In the sentence above, h describes when the effect happens while r represents the decay pattern (see the picture below). First we load the data, converting it to a time series format, and we visualize the time series along with a vertical lines representing the date of the intervention. Next, we have to create the dummy variables representing our intervention. This can be tricky in R. In this case, the authors convert the time of the ts object in a more human-readable format through the as.yearmon function (this is a zoo function and you can use it by loading the xts library). ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [24] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 ## [47] 1 1 The above vectors is a dummy variable for the intervention. It has value equal zero before the date of the intervention, and 1 after that. Next, in this specific case, we also want to create a variable representing a constant increasing change, capturing an increasing effect of the intervention over time. Also in this case the creation of the variable can be a little tricky. We create two vectors by using the rep and the seq function, and concatenate them by using the c function. The argument of the rep function are two integers x and times (rep(x, times)), and the function creates a vectors that repeat (“rep”) the x values the number of times specified by times. We have 36 months before the intervention, and we assign them the value zero. ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [24] 0 0 0 0 0 0 0 0 0 0 0 0 0 Instead, we use the seq function to create a vectors with increasing values. This part of the variable represent a gradual increase after the intervention (we have 12 months of data after the intervention). The function seq takes the three arguments from, to, and by: from and to are the starting and end values of the sequence, by is the increment of the sequence. In our case we create a sequence of values that increases from 1 to 12 by 1. ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 To create the variable we need, we concatenate both the function with the c function, as follows: ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [16] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [31] 0 0 0 0 0 0 1 2 3 4 5 6 7 8 9 ## [46] 10 11 12 We search for an appropriate ARIMA model for the data by using the auto.arima function (forecast package). We include the variables we have created as external regressors. ## ## Ljung-Box test ## ## data: Residuals from Regression with ARIMA(2,1,0)(0,1,1)[12] errors ## Q* = 9.5692, df = 7, p-value = 0.2143 ## ## Model df: 3. Total lags used: 10 The resulting model is an ARIMA(2,1,0)(0,1,1)[12]. ## Series: quet.ts ## Regression with ARIMA(2,1,0)(0,1,1)[12] errors ## ## Coefficients: ## ar1 ar2 sma1 step ## -0.873 -0.6731 -0.6069 -3284.7792 ## s.e. 0.124 0.1259 0.3872 602.3362 ## ramp ## -1396.6523 ## s.e. 106.6329 ## ## sigma^2 = 648828: log likelihood = -284.45 ## AIC=580.89 AICc=583.89 BIC=590.23 We use the information retrieved from the auto.arima function to fit the same ARIMA model to the data, without including the intervention (the variables we created), using just the data up to the date of the intervention (up to January 2014). To do that, we use the window function in order to restrict the set of data we consider, indicating December 2013 as the end of our series. Next, we forecast the 12 months we didn’t include (starting from January 2014 until the end of the period of observation, December 2014) by using the forecast function (library forecast). The logic behind this operation is to see what would have happened to the series in the absence of the intervention. In other words, we use the prediction as a counterfactual in order to describe a possible effect of the intervention on the series, by determining how the observed values diverge from this forecast. ## quet.ts fc.ts ## Jan 2011 16831 NA ## Feb 2011 17234 NA ## Mar 2011 20546 NA ## Apr 2011 19226 NA ## May 2011 21136 NA ## Jun 2011 20939 NA ## Jul 2011 21103 NA ## Aug 2011 22897 NA ## Sep 2011 22162 NA ## Oct 2011 22184 NA ## Nov 2011 23108 NA ## Dec 2011 25967 NA ## Jan 2012 20123 NA ## Feb 2012 21715 NA ## Mar 2012 24497 NA ## Apr 2012 21720 NA ## May 2012 25053 NA ## Jun 2012 23915 NA ## Jul 2012 24972 NA ## Aug 2012 26183 NA ## Sep 2012 24163 NA ## Oct 2012 26172 NA ## Nov 2012 26642 NA ## Dec 2012 29086 NA ## Jan 2013 24002 NA ## Feb 2013 24190 NA ## Mar 2013 26052 NA ## Apr 2013 26707 NA ## May 2013 29077 NA ## Jun 2013 26927 NA ## Jul 2013 30300 NA ## Aug 2013 29854 NA ## Sep 2013 28824 NA ## Oct 2013 31519 NA ## Nov 2013 32084 NA ## Dec 2013 33160 NA ## Jan 2014 24827 29127.50 ## Feb 2014 23285 29671.28 ## Mar 2014 23884 31156.37 ## Apr 2014 21921 31339.65 ## May 2014 22715 33843.48 ## Jun 2014 19919 31809.61 ## Jul 2014 20560 34498.50 ## Aug 2014 18961 34774.18 ## Sep 2014 18780 33302.09 ## Oct 2014 17998 35641.85 ## Nov 2014 16624 36184.57 ## Dec 2014 18450 37792.03 By plotting the data, we can visualize the predicted values in the absence of the intervention (red dashed line) as well as the observed values (blue line). It seems that the health policy considerably impacted the analyzed prescriptions. Coming back to our initial ARIMA model including the intervention variables, calculating also the confidence intervals and the significance of the coefficients by using the coeftest and the confint function in the lmtest library, we can quantify the impact of the policy. ## Series: quet.ts ## Regression with ARIMA(2,1,0)(0,1,1)[12] errors ## ## Coefficients: ## ar1 ar2 sma1 step ## -0.873 -0.6731 -0.6069 -3284.7792 ## s.e. 0.124 0.1259 0.3872 602.3362 ## ramp ## -1396.6523 ## s.e. 106.6329 ## ## sigma^2 = 648828: log likelihood = -284.45 ## AIC=580.89 AICc=583.89 BIC=590.23 ## ## z test of coefficients: ## ## Estimate Std. Error z value Pr(&gt;|z|) ## ar1 -0.87301 0.12396 -7.0427 1.885e-12 ## ar2 -0.67314 0.12587 -5.3480 8.893e-08 ## sma1 -0.60694 0.38722 -1.5674 0.117 ## step -3284.77920 602.33616 -5.4534 4.942e-08 ## ramp -1396.65226 106.63287 -13.0978 &lt; 2.2e-16 ## ## ar1 *** ## ar2 *** ## sma1 ## step *** ## ramp *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 2.5 % 97.5 % ## ar1 -1.1159671 -0.6300565 ## ar2 -0.9198339 -0.4264433 ## sma1 -1.3658815 0.1520046 ## step -4465.3363731 -2104.2220250 ## ramp -1605.6488440 -1187.6556789 The estimated step change was − 3285 dispensings (95% CI − 4465 to − 2104) while the estimated change in slope was − 1397 dispensings per month (95% CI − 1606 to − 1188). (The figure, ndr) shows the values predicted by our ARIMA model in absence of the intervention (counterfactual) compared with the observed values. This means that the change in subsidy for 25 mg quetiapine in January 2014 was associated with an immediate, sustained decrease of 3285 dispensings, with a further decrease of 1397 dispensings every month. In other words, there were 4682 (3285 + 1397) fewer dispensings in January 2014 than predicted had the subsidy changes not been implemented. In February 2014, there were 6079 fewer dispensings (3285 + 2*1397). Importantly, our findings should only be considered valid for the duration of the study period (i.e. until December 2014). Box, G. E., &amp; Tiao, G. C. (1975). Intervention analysis with applications to economic and environmental problems. Journal of the American Statistical association, 70(349), 70-79↩︎ "],["interrupted-time-series-analysis-using-segmented-regression.html", "Chapter11 Interrupted time series analysis using segmented regression", " Chapter11 Interrupted time series analysis using segmented regression Segmented regression is another common way for analyzing the impact of an intervention. Two good papers explaining the methods of segmented regression are, for example: Bernal, J. L., Cummins, S., &amp; Gasparrini, A. (2017). Interrupted time series regression for the evaluation of public health interventions: a tutorial. International journal of epidemiology, 46(1), 348-355. Wagner, A. K., Soumerai, S. B., Zhang, F., &amp; Ross‐Degnan, D. (2002). Segmented regression analysis of interrupted time series studies in medication use research. Journal of clinical pharmacy and therapeutics, 27(4), 299-309. As explained by the latter, “Segmented regression analysis uses statistical models to estimate level and trend in the pre-intervention segment and changes in level and trend after the intervention (or interventions).”. More exactly, a segmented regression model is structured as follow: \\[Y = b_0 + b_1Time + b_2Intervention + b_3TimeSinceIntervention + e\\] It includes at least: an outcome variable (Y); a variable that indicates the time 1,2,…,t passed from the start of the series; a dummy variable (0/1) for observation collected before (0) or after (1) the intervention; a variable measuring the time 1,2,…,t passed since the intervention has occured, and which is equal to zero before the intervention. The interpretation of coefficients is as follows: \\(b_0\\) is the baseline level at Time 0; The Time (\\(b_1\\)) coefficient indicates the trend (the slope) before the intervention ( change in outcome associated with a time unit increase). The Intervention (\\(b_2\\)) coefficient indicates the immediate effect (level change) induced by the intervention (from the last observation before the intervention to the first one after). The Time Since Intervention (\\(b_3\\)) coefficient indicates the “sustained effect”, i.e., the change in trend after the intervention (the effect for each time point that passes after the intervention). It measures the difference between the slope of the line before and after the intervention. It is also possible to calculate the slope of the line after the intervention by summing up the coefficients of Time and Time Since Treatment (\\(b_1 + b_3\\)) A good tutorial on this tecnique can be found at the following link: https://ds4ps.org/pe4ps-textbook/docs/p-020-time-series.html. "],["var.html", "Chapter12 VAR 12.1 VAR modeling hands-on tutorial 12.2 VAR fitting 12.3 Granger causality test", " Chapter12 VAR VAR is an acronym that stands for Vector Autoregressive Model. It is a common method for the analysis of multivariate time series. It can be conceived as a way to model a system of time series. In a VAR model, there is no rigid distinction between independent and dependent variables, but each variable is both dependent and independent. Besides these endogenous variables that dynamically interact, a VAR model can include exogenous variables. Exogenous variables can have an impact on the endogenous variables, while the opposite is not true. To make a simple example, the time series of fans sold by month may be influenced by the quantity of fans produced and distributed by the industry, and by the monthly temperature. While the purchase and production of fans can interact (endogenous variables), the weather is not impacted by these processes, but just impact them as an external force (exogenous variable). To make another example (from the paper The Impact of the Politicization of Health on Online Misinformation and Quality Information on Vaccines), the political debate on a certain topic - for instance the political debate that led to the promulgation of the law on mandatory vaccinations in Italy - could be considered an exogenous variable that impacts both the news coverage of the topic and the spread of problematic information on Twitter. While news media coverage and Twitter discussions can be considered part of the same communication system and dependent on each other (news media could set the discussion agenda on Twitter, but also social media can stimulate news media coverage) it could be assumed that the political debate that led to the promulgation of the law on mandatory vaccinations was independent from the Twitter discussions on the topic. Stationary tests are usually applied to ascertain that variables are not integrated (the “I” in the ARIMA model). If this is the case, variables are differenced before starting the VAR analysis. Other preliminary analysis (for instance testing for cointegration) and pre-processing can be performed before the analysis. Next, the number of lags to be used has to be selected. This number can be automatically identify through automated methods (lag-length selection criteria methods). The model is then evaluated. Results from a VAR model are usually complicated, and the researchers relies on statistical methods such as Granger causality test. Granger causality test, a test developed by the nobel prize winner Clive Granger, has been applied to study agenda setting processes. A variable \\(X\\) is said to “Granger cause” another variable \\(Y\\) if \\(Y\\) can be better predicted from the past values of \\(X\\) and \\(Y\\) together than from the past of \\(Y\\) alone. The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969. Ordinarily, regressions reflect “mere” correlations, but Clive Granger argued that causality (…) could be tested for by measuring the ability to predict the future values of a time series using prior values of another time series. Since the question of “true causality” is deeply philosophical, and because of the post hoc ergo propter hoc fallacy of assuming that one thing preceding another can be used as a proof of causation, (…) the Granger test finds only “predictive causality”. An R package to perform VAR modeling is vars. Practical applications of VAR modeling, including Granger causality, can be found, for instance, in the paper Assembling the Networks and Audiences of Disinformation: How Successful Russian IRA Twitter Accounts Built Their Followings,2015–2017 or Coordinating a Multi-Platform Disinformation Campaign: Internet Research Agency Activity on Three U.S. Social Media Platforms, 2015 to 2017 at this link. 12.1 VAR modeling hands-on tutorial 12.1.1 Assumption of stationarity Like most time series techniques, VAR assumes the series to be stationary. To recap: A stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary — it does not matter when you observe it, it should look much the same at any point in time. […] In general, a stationary time series will have no predictable patterns in the long-term. Time plots will show the series to be roughly horizontal (although some cyclic behaviour is possible), with constant variance. Rob J. Hyndman and George Athanasopoulos, “Forecasting: Principles and Practice”. Below are some examples of stationary and non-stationary series. As we already learnt, some pre-processing steps are usually carried out to make it stationary when the series is not stationary. For instance, a series with a linear trend can be made stationary by removing the trend. A seasonal series can be seasonally adjusted. A random-walk-like series can be adjusted using differencing. Visual inspection and statistical tests can be performed to support findings on the characteristics of a time series. Some of these pre-processing steps may be unnecessary when using the vars library, because its function VAR (which is used to fit VAR models) allows you to include trend and seasonality components. 12.1.2 Other assumptions of VAR models: distribution of residuals Besides stationarity and linearity between variables and their lagged values, VAR cannot be performed in the presence of structural breaks. Structural breaks are abrupt changes in the series’ mean or overall process. You can deal with structural breaks in different ways. For example, you might want to split the series into different phases, using the structural breaks as breakpoints. Other crucial assumptions of VAR are about residuals, which are required to be: Non serially correlated Normal distributed Homoskedastic Statistical tests included in the vars package can be used to check these assumptions. Another fundamental assumption of VAR models is the absence of cointegration between series. Two series may be cointegrated only when they are both integrated of the same order. Integrated series achieve stationarity after being differentiated. The number of differentiation needed to achieve stationarity is the order of integration (usually, it is one: I(1) is the notation). Integrated series are non-stationary, but it can happen that there is a linear combination of integrated series that is stationary. In that case, they are said to be cointegrated. Although each series follows a seemingly random path, over time, they are characterized by an equilibrium, as if they were driven by a common underlying process. Cointegration has been explained with the help of the story of the drunk and his dog: In fact, the drunk is not the only creature whose behavior follow a random walk. Puppies, too, wander aimlessly when unleashed. Each new scent that crosses the puppy’s nose dictates a direction for the pup’s next step. […] But what if the dog belongs to the drunk? The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But periodically she intones “Oliver, where are you?”, and Oliver interrupts his aimless wandering to bark. He hears her; she hears him. He thinks: “Oh I can’t let her get too far off; she’ll lock me out”. She thinks, “Oh, I can’t let him get too far off; he’ll wake me up in the middle of the night with his barking”. Each assessed how far away the other is and moves to partially close that gap. Now neither drunk nor dog follows a random walk; each has added what we formally call an error-correction-mechanism to her or his steps. […] The paths of the drunk and the dog are still non-stationary. Significantly, despite the nonstationarity of the paths, one might still say, “If you find her, the dog is unlikely to be very far away”. If this is right, then the distance between the two paths is stationary, and the walks of the woman and her dog are said to be cointegrated (…). […] Notice that cointegration is a probabilistic concept. The dog is not on a leash, which would enforce a fixed distance between the drunk and the dog. The distance between the drunk and the dog is instead a random variable. But a stationary one, despite the nonstationarity of the two paths. Murray, M. P. (1994). A drunk and her dog: an illustration of cointegration and error correction. The American Statistician, 48(1), 37-39. Statistical tests are used to determine whether cointegration is present. They have to be performed only when the series are integrated and have the same order of integration (again, cointegration can only exists if at least two series are integrated of the same order). Then, cointegration tests are performed. In presence of cointegration, Vector Error Correction Models (VECM) are used instead of VAR. VECM is basically a VAR model with an “error correction term”. This topic is not (yet) covered in this tutorial. 12.2 VAR fitting Let’s make a simple example. Upload some data: ## date_time anti pro ## 1 2021-03-12 23:59:00 37 14085 ## 2 2021-03-13 23:59:00 157 5219 ## 3 2021-03-14 23:59:00 40 5106 ## 4 2021-03-15 23:59:00 25 6898 ## 5 2021-03-16 23:59:00 24 2974 ## 6 2021-03-17 23:59:00 61 3230 Let’s visually inspect the series: As the variance does not look constant, so we can tentatively log-transform the series and work with log-transformed data. To check for stationarity, we can perform a test such as the Augmented Dickey-Fuller Test. Integration is suggested only for the “anti” series. ## ## Augmented Dickey-Fuller Test ## ## data: ts_tweets$pro ## Dickey-Fuller = -3.9314, Lag order = 3, ## p-value = 0.02193 ## alternative hypothesis: stationary ## ## Augmented Dickey-Fuller Test ## ## data: ts_tweets$anti ## Dickey-Fuller = -2.101, Lag order = 3, ## p-value = 0.5338 ## alternative hypothesis: stationary The series becomes stationary after differencing. ## Warning in tseries::adf.test(anti): p-value ## smaller than printed p-value ## ## Augmented Dickey-Fuller Test ## ## data: anti ## Dickey-Fuller = -4.7232, Lag order = 3, ## p-value = 0.01 ## alternative hypothesis: stationary We can recreate the dataset including the differentiated series. Using a first difference transformation, we lose the first data point. ## [1] 41 ## [1] 42 Indeed, the first point in the differentiated series is obtained by subtracting the first point from the second one. The second point in the differentiated series is obtained by subtracting the second point from the third one, and so on. Hence, the differentiated dataset will start from the second point in time, not the first one as in the original series. Therefore, we need to drop the first data point from the dataset. To fit the VAR model we use the library vars. VAR models require the researcher to specify the number of lags to include in the model. Roughly speaking, lags are auto-regressive predictors. For example, considering a VAR model with two variables \\(X\\) and \\(Y\\), using 1 lag means that we use the value \\(X_{t-1}\\) and \\(Y_{t-1}\\) to predict \\(X_t\\) and \\(Y_t\\), and 2 lags that we use \\(X_{t-1}\\), \\(X_{t-2}\\), \\(Y_{t-1}\\), and \\(Y_{t-2}\\). The choice of lags is important. The function VARselect implements four different statistical tests to find the “best” number of lags. In this case, all the criteria agree on one lag. ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 1 1 1 1 ## ## $criteria ## 1 2 3 ## AIC(n) -2.92630038 -2.81403228 -2.64199556 ## HQ(n) -2.83582733 -2.66324387 -2.43089178 ## SC(n) -2.64875447 -2.35145577 -1.99438843 ## FPE(n) 0.05366012 0.06030449 0.07235601 ## 4 5 6 ## AIC(n) -2.42923055 -2.5769659 -2.77801920 ## HQ(n) -2.15781141 -2.2452314 -2.38596933 ## SC(n) -1.59659281 -1.5592975 -1.57532025 ## FPE(n) 0.09118664 0.0810697 0.06940455 ## 7 8 9 ## AIC(n) -2.68597480 -2.68018280 -2.5499629 ## HQ(n) -2.23360957 -2.16750221 -1.9769669 ## SC(n) -1.29824525 -1.10742264 -0.7921721 ## FPE(n) 0.08132081 0.08986526 0.1167941 ## 10 ## AIC(n) -2.4424493 ## HQ(n) -1.8091380 ## SC(n) -0.4996280 ## FPE(n) 0.1564827 The function to fit the model is VAR, and the number of lags is specified using the p parameter. You can also include trend and/or seasonality, but in this case it doesn’t seem useful. It is also possible to include exogenous variables. Exogenous variables are usually defined as variables external to the system: they may affect the system, but are not affected by the system. As in the VARselect function, we only use the columns containing the data (the first column in the dataset contains dates). The model looks like this. As you can see, it is a system of linear regression models that includes lagged predictors. Before analyzing the results we need to confirm that the assumptions of the models are met. ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: pro, anti ## Deterministic variables: const ## Sample size: 40 ## Log Likelihood: -53.79 ## Roots of the characteristic polynomial: ## 0.4477 0.2503 ## Call: ## vars::VAR(y = ts_tweets[, c(&quot;pro&quot;, &quot;anti&quot;)], p = 1) ## ## ## Estimation results for equation pro: ## ==================================== ## pro = pro.l1 + anti.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## pro.l1 0.25131 0.16894 1.488 0.145 ## anti.l1 -0.05992 0.06722 -0.891 0.379 ## const 6.27985 1.41950 4.424 8.22e-05 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.3364 on 37 degrees of freedom ## Multiple R-Squared: 0.06146, Adjusted R-squared: 0.01073 ## F-statistic: 1.212 on 2 and 37 DF, p-value: 0.3093 ## ## ## Estimation results for equation anti: ## ===================================== ## anti = pro.l1 + anti.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## pro.l1 0.01161 0.38475 0.030 0.97610 ## anti.l1 -0.44871 0.15309 -2.931 0.00576 ** ## const -0.10750 3.23284 -0.033 0.97365 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.7661 on 37 degrees of freedom ## Multiple R-Squared: 0.2029, Adjusted R-squared: 0.1598 ## F-statistic: 4.709 on 2 and 37 DF, p-value: 0.01507 ## ## ## ## Covariance matrix of residuals: ## pro anti ## pro 0.1132 0.0862 ## anti 0.0862 0.5870 ## ## Correlation matrix of residuals: ## pro anti ## pro 1.0000 0.3345 ## anti 0.3345 1.0000 serial.test is a test for serially correlated errors. The test is okay when not significant. The test is not significant. ## ## Portmanteau Test (asymptotic) ## ## data: Residuals of VAR object var_fit ## Chi-squared = 33.632, df = 60, p-value = ## 0.9977 normality.test performs tests for the normality of residuals. The tests are okay when not significant, like in this case. ## $JB ## ## JB-Test (multivariate) ## ## data: Residuals of VAR object var_fit ## Chi-squared = 3.9951, df = 4, p-value = ## 0.4067 ## ## ## $Skewness ## ## Skewness only (multivariate) ## ## data: Residuals of VAR object var_fit ## Chi-squared = 1.8704, df = 2, p-value = ## 0.3925 ## ## ## $Kurtosis ## ## Kurtosis only (multivariate) ## ## data: Residuals of VAR object var_fit ## Chi-squared = 2.1247, df = 2, p-value = ## 0.3456 Another fundamental test is performed by the function stability. The test is okay if the series stay within the red bars. It is okay. arch.test assesses the null hypothesis that a series of residuals exhibits no heteroscedasticity. Also this test is not significant, which is good. ## ## ARCH (multivariate) ## ## data: Residuals of VAR object var_fit ## Chi-squared = 44.188, df = 45, p-value = ## 0.5062 As all the assumptions are met, we can take a look at the overall fit. It doesn’t look amazing, but the assumptions are met, so let’s assume it is good enough and let’s go ahead with the analysis. The not-so-good fit is probably related to the fact that both variables in the VAR system are not highly correlated. Most coefficients are not statistically significant and the overall R-square is small. Generally, the coefficient table is not interpreted. Instead, VAR models are interpreted using other tools, such as the Granger causality test. ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: pro, anti ## Deterministic variables: const ## Sample size: 40 ## Log Likelihood: -53.79 ## Roots of the characteristic polynomial: ## 0.4477 0.2503 ## Call: ## vars::VAR(y = ts_tweets[, c(&quot;pro&quot;, &quot;anti&quot;)], p = 1) ## ## ## Estimation results for equation pro: ## ==================================== ## pro = pro.l1 + anti.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## pro.l1 0.25131 0.16894 1.488 0.145 ## anti.l1 -0.05992 0.06722 -0.891 0.379 ## const 6.27985 1.41950 4.424 8.22e-05 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.3364 on 37 degrees of freedom ## Multiple R-Squared: 0.06146, Adjusted R-squared: 0.01073 ## F-statistic: 1.212 on 2 and 37 DF, p-value: 0.3093 ## ## ## Estimation results for equation anti: ## ===================================== ## anti = pro.l1 + anti.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## pro.l1 0.01161 0.38475 0.030 0.97610 ## anti.l1 -0.44871 0.15309 -2.931 0.00576 ** ## const -0.10750 3.23284 -0.033 0.97365 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.7661 on 37 degrees of freedom ## Multiple R-Squared: 0.2029, Adjusted R-squared: 0.1598 ## F-statistic: 4.709 on 2 and 37 DF, p-value: 0.01507 ## ## ## ## Covariance matrix of residuals: ## pro anti ## pro 0.1132 0.0862 ## anti 0.0862 0.5870 ## ## Correlation matrix of residuals: ## pro anti ## pro 1.0000 0.3345 ## anti 0.3345 1.0000 12.3 Granger causality test We use the causality function to perform the Granger causality test. The Granger causality test is the most basic inferential tool available in VAR analysis. A time series X is considered a Granger cause of another time series Y if past values of X and Y predicts Y significantly better than past values of Y alone. This analysis is frequently performed in communication studies focused on agenda setting phenomena. Let’s see if there’s Granger causality between our series. The researcher needs to specify the “cause” variable (sometimes, the researcher may have a hypothesis about that). In this case, we try both. There is no granger causality effect but some contemporaneous relationship (instant causation). ## $Granger ## ## Granger causality H0: pro do not ## Granger-cause anti ## ## data: VAR object var_fit ## F-Test = 0.00090982, df1 = 1, df2 = 74, ## p-value = 0.976 ## ## ## $Instant ## ## H0: No instantaneous causality between: pro ## and anti ## ## data: VAR object var_fit ## Chi-squared = 4.0242, df = 1, p-value = ## 0.04485 ## $Granger ## ## Granger causality H0: anti do not ## Granger-cause pro ## ## data: VAR object var_fit ## F-Test = 0.79448, df1 = 1, df2 = 74, p-value ## = 0.3756 ## ## ## $Instant ## ## H0: No instantaneous causality between: anti ## and pro ## ## data: VAR object var_fit ## Chi-squared = 4.0242, df = 1, p-value = ## 0.04485 "],["readings-and-bibliographical-references.html", "Chapter13 Readings and Bibliographical References 13.1 Mandatory 13.2 Other readings 13.3 Useful resources", " Chapter13 Readings and Bibliographical References 13.1 Mandatory Shin, Y. (2017). Time series analysis in the social sciences: the fundamentals. Univ of California Press. 13.2 Other readings Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, A., &amp; Yang, J. (2019). The Temporal Turn in Communication Research: Time Series Analyses Using Computational Approaches. International Journal of Communication (19328036), 13. Brodersen KH, Gallusser F, Koehler J, Remy N, Scott SL. Inferring causal impact using Bayesian structural time-series models. Annals of Applied Statistics, 2015, Vol. 9, No. 1, 247-274. Gaubatz, K. T. (2014). A Survivor’s Guide to R: An Introduction for the Uninitiated and the Unnerved. SAGE Publications. Liboschik, T., Fokianos, K., &amp; Fried, R. (2017). tscount: An R package for analysis of count time series following generalized linear models. Journal of Statistical Software, 82(1), 1-51. Schaffer, A. L., Dobbins, T. A., &amp; Pearson, S. A. (2021). Interrupted time series analysis using autoregressive integrated moving average (ARIMA) models: a guide for evaluating large-scale health interventions. BMC medical research methodology, 21(1), 1-12. Zivot E., Wang J. (2003) Unit Root Tests. In: Modeling Financial Time Series with S-Plus®. Springer, New York, NY. https://doi.org/10.1007/978-0-387-21763-5_4 13.3 Useful resources Cross Validated for statistics-related questions Stackoverflow for coding-related questions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
