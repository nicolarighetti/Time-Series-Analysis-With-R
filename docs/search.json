[{"path":"index.html","id":"time-series-analysis-with-r","chapter":"1 Time Series Analysis With R","heading":"1 Time Series Analysis With R","text":"Welcome book  Time Series Analysis R.book provides practical introduction analyzing time series data using R. guides reader :characteristics specificity time series dataThe characteristics specificity time series dataUsing free statistical software R conduct time series analysisUsing free statistical software R conduct time series analysisKey univariate multivariate techniques analyzing time seriesKey univariate multivariate techniques analyzing time seriesBy end book, readers understand unique aspects time series data able perform simple analyses  R using methods presented .Note: book first created students International Master Communication Science University Vienna.","code":""},{"path":"index.html","id":"citation","chapter":"1 Time Series Analysis With R","heading":"1.1 Citation","text":"use book work, please cite :Righetti, N. (2025). Time Series Analysis R. Available : https://nicolarighetti.github.io/Time-Series-Analysis--R/","code":""},{"path":"getting-started-with-r.html","id":"getting-started-with-r","chapter":"2 Getting started with R","heading":"2 Getting started with R","text":"","code":""},{"path":"getting-started-with-r.html","id":"rstudio-interface-and-data","chapter":"2 Getting started with R","heading":"2.1 RStudio Interface and Data","text":"","code":""},{"path":"getting-started-with-r.html","id":"download-and-install-rstudio","chapter":"2 Getting started with R","heading":"2.1.1 Download and Install RStudio","text":"course based statistical software R. R easier use development environment RStudio (works Windows, Apple, OS).possible download free version RStudio Desktop official websites.might also use free online version RStudio registering RStudio Cloud free plan. However, free plan gives just 15 hours per months. lessons take 4.5 hours per month, since also need practice, best choice install RStudio R computer.Now going see get started RStudio Desktop.First, download install free version RStudio Desktop open software.","code":""},{"path":"getting-started-with-r-1.html","id":"getting-started-with-r-1","chapter":"3 Getting started with R","heading":"3 Getting started with R","text":"","code":""},{"path":"getting-started-with-r-1.html","id":"rstudio-interface-and-data-1","chapter":"3 Getting started with R","heading":"3.1 RStudio Interface and Data","text":"","code":""},{"path":"getting-started-with-r-1.html","id":"download-and-install-rstudio-1","chapter":"3 Getting started with R","heading":"3.1.1 Download and Install RStudio","text":"course based statistical software R. R easier use within RStudio, works Windows, macOS, operating systems.possible download free version RStudio Desktop official websites. might also use free online version RStudio registering RStudio Cloud free plan. However, free plan gives just 15 hours per months.Now let’s see get started RStudio Desktop. First, download install free version RStudio Desktop open software.","code":""},{"path":"getting-started-with-r-1.html","id":"create-a-rstudio-project-and-import-data","chapter":"3 Getting started with R","heading":"3.1.2 Create a RStudio Project and Import data","text":"starting data analysis project RStudio, create new dedicated environment keep scripts (files containing code perform analysis), data sets, outputs analysis (plots tables). dedicated workspace simply called project.create new project RStudio, follow steps:click File (top left);, click New Project;select New Directory, New Project;Choose folder project give name project. can use name Time-Series-Analysis--R.create new folder project main folder specified previous step. folder, find file .Rproj, name project assigned. work project, just need open .Rproj file.","code":""},{"path":"getting-started-with-r-1.html","id":"create-a-script","chapter":"3 Getting started with R","heading":"3.1.3 Create a Script","text":"project created, can open new script save .script file containing code. can create first script named basic-r-syntax, test basic code going see. script saved extension .r.can open, change, save file every time work . Saving code important; otherwise, write code every time work project!Click watch “Create Save Scripts”Click watch “Update Scripts Run Code”","code":""},{"path":"getting-started-with-r-1.html","id":"the-rstudio-user-interface","chapter":"3 Getting started with R","heading":"3.1.4 The RStudio User Interface","text":"interface RStudio organized four main quadrants:","code":"- The top-left quadrant is the editor. Here you can create or open a script and compose the R commands.\n- The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. There is also the *Files* tab, where you can navigate files and folders and find, for instance, the data sets you want to upload.\n- On the bottom left is the R Console window, where the code gets executed and the output is produced. You can run the commands, sending the code from the editor to the console, by highlighting it and hitting the *Run* button, or the Ctrl-Enter key combination. It is also possible to type and run commands directly into the console window (in this case, nothing will be saved).\n- The bottom-right quadrant is a window for graphics output. Here you can visualize your plots. There are also tabs for R packages and the R Help facility."},{"path":"getting-started-with-r-1.html","id":"load-and-save-data","chapter":"3 Getting started with R","heading":"3.1.5 Load and Save Data","text":"load data R, can click Files window top-right quadrant, navigate files folders, found data set file, can just click follow semi-automated import procedure.Click watch “Import Data”Otherwise, can upload data set using function. instance, import csv file, one common formats data sets, can use function read.csv. main argument function path file want upload. specify file path, consider working within specific environment; , working directory folder project (can double-check working directory running command getwd()). Thus, indicate path data set want upload, can write dot followed slash ./, followed path data set inside working directory. instance, case , data set saved folder named data inside working directory. name data set tweets_vienna extension .csv. Therefore, code upload file follows:save data, options. Generally, want save data set, can opt .csv .rds format. .rds format readable R, .csv format “universal” (can read Excel, instance).save file .csv, can use function write.csv. main arguments function name object saved, path folder object saved, name want assign file.save .rds file, procedure similar, saveRDS function employed. read .rds file, appropriate function readRDS.code , can notice hash mark sign followed text. comment. Comments textual content used describe code order make easier understand reuse. Comments written hash mark sign (#) text written hash mark sign ignored R: can read comments, R consider code.","code":"\nfake_news <- read.csv(\"./data/fake-news-stories-over-time-20210111144200.csv\")\nwrite.csv(fake_news, file = \"./data/fake_news.csv\")\nsaveRDS(fake_news, file = \"./data/fake_news.rds\")\n\nfake_news <- readRDS(\"./data/fake_news.rds\")   # read a .rds file"},{"path":"getting-started-with-r-1.html","id":"create-new-folders","chapter":"3 Getting started with R","heading":"3.1.6 Create new Folders","text":"good practice create, main folder project, sub-folders dedicated different types files used project, folder data data sets.create new folder, can go Files window RStudio interface, click New Folder, give name.","code":""},{"path":"getting-started-with-r-1.html","id":"basic-r","chapter":"3 Getting started with R","heading":"3.2 Basic R","text":"","code":""},{"path":"getting-started-with-r-1.html","id":"objects","chapter":"3 Getting started with R","heading":"3.2.1 Objects","text":"object R entity composed name value.arrow (<-) sign used create objects assign value object (change “update” previous value).Example: create object name object_consisting_of_a_number value equal 2:Enter name object console run command: value assigned object displayed.object equal value. Therefore, instance, object numerical value can used perform arithmetical operations.value object can transformed:object can also represent function.Example: create object sum (addition) function:function can now applied two numerical values:Actually, don’t need function, since mathematical functions already implemented R.value object can number, function, vector. Vectors sequences values.vector numbers can argument mathematical operations.R objects matrix, list, data.frame.matrix table composed rows columns containing numerical values.list just list objects. instance, list includes numerical value, vector numbers, matrix.data.frame like matrix can contain numbers also types data, characters (textual type data) factors (unordered categorical variables, gender, ordered categories, low, medium, high).Data sets usually stored data.frames. instance, import csv Excel file R, corresponding R object data.frame.access specific column data.frame, can use name data.frame, dollar symbol $, name column.possible add columns data.frame writing:name data.framethe dollar signa name new columnthe arrow sign <-vector values stored new column (length equal vectors composing data.frame)possible visualize first rows data.frame using function head.","code":"\nobject_consisting_of_a_number <- 2\nobject_consisting_of_a_number## [1] 2\nobject_consisting_of_a_number * 10## [1] 20\nobject_consisting_of_a_number <- object_consisting_of_a_number * 10\n\nobject_consisting_of_a_number## [1] 20\nfunction_sum <- function(x, y){\n  result <- x + y\n  return(result)\n}\nfunction_sum(5, 2)## [1] 7\nsum(5, 2)## [1] 7\n5 + 7## [1] 12\n2 * 3## [1] 6\n3^2## [1] 9\nsqrt(9)## [1] 3\nvector_of_numbers <- c(1,2,3,4,5,6,7,8,9,10) \nvector_of_numbers##  [1]  1  2  3  4  5  6  7  8  9 10\nvector_of_numbers * 2##  [1]  2  4  6  8 10 12 14 16 18 20\nvector_of_numbers + 3##  [1]  4  5  6  7  8  9 10 11 12 13\na_matrix <- matrix(data = 1:50, nrow = 10, ncol = 5)\n\na_matrix##       [,1] [,2] [,3] [,4] [,5]\n##  [1,]    1   11   21   31   41\n##  [2,]    2   12   22   32   42\n##  [3,]    3   13   23   33   43\n##  [4,]    4   14   24   34   44\n##  [5,]    5   15   25   35   45\n##  [6,]    6   16   26   36   46\n##  [7,]    7   17   27   37   47\n##  [8,]    8   18   28   38   48\n##  [9,]    9   19   29   39   49\n## [10,]   10   20   30   40   50\na_list <- list(object_consisting_of_a_number, vector_of_numbers, a_matrix)\n\na_list## [[1]]\n## [1] 20\n## \n## [[2]]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## \n## [[3]]\n##       [,1] [,2] [,3] [,4] [,5]\n##  [1,]    1   11   21   31   41\n##  [2,]    2   12   22   32   42\n##  [3,]    3   13   23   33   43\n##  [4,]    4   14   24   34   44\n##  [5,]    5   15   25   35   45\n##  [6,]    6   16   26   36   46\n##  [7,]    7   17   27   37   47\n##  [8,]    8   18   28   38   48\n##  [9,]    9   19   29   39   49\n## [10,]   10   20   30   40   50\n# this is an object (vector) consisting of a series of numerical values\nnumerical_vector <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)\nnumerical_vector##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14\n# this is another object (vector) consisting of a series of categorical values\ncategorical_vector <- c(\"Monday\", \"Tuesday\", \"Monday\", \"Tuesday\", \"Monday\", \"Wednesday\",\"Thursday\", \"Wednesday\", \"Thursday\", \"Saturday\", \"Sunday\", \"Friday\", \"Saturday\", \"Sunday\")\ncategorical_vector##  [1] \"Monday\"    \"Tuesday\"   \"Monday\"    \"Tuesday\"   \"Monday\"    \"Wednesday\"\n##  [7] \"Thursday\"  \"Wednesday\" \"Thursday\"  \"Saturday\"  \"Sunday\"    \"Friday\"   \n## [13] \"Saturday\"  \"Sunday\"\n# this is an object consisting of a data.frame, created combining vectors through the function \"data.frame\"\na_dataframe <- data.frame(\"first_variable\" = numerical_vector,\n                          \"second_variable\" = categorical_vector)\na_dataframe##    first_variable second_variable\n## 1               1          Monday\n## 2               2         Tuesday\n## 3               3          Monday\n## 4               4         Tuesday\n## 5               5          Monday\n## 6               6       Wednesday\n## 7               7        Thursday\n## 8               8       Wednesday\n## 9               9        Thursday\n## 10             10        Saturday\n## 11             11          Sunday\n## 12             12          Friday\n## 13             13        Saturday\n## 14             14          Sunday\na_dataframe$first_variable##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14\na_dataframe$second_variable##  [1] \"Monday\"    \"Tuesday\"   \"Monday\"    \"Tuesday\"   \"Monday\"    \"Wednesday\"\n##  [7] \"Thursday\"  \"Wednesday\" \"Thursday\"  \"Saturday\"  \"Sunday\"    \"Friday\"   \n## [13] \"Saturday\"  \"Sunday\"\na_dataframe$a_new_variable <- c(12, 261, 45, 29, 54, 234, 45, 42, 6, 267, 87, 3, 12, 9)\na_dataframe##    first_variable second_variable a_new_variable\n## 1               1          Monday             12\n## 2               2         Tuesday            261\n## 3               3          Monday             45\n## 4               4         Tuesday             29\n## 5               5          Monday             54\n## 6               6       Wednesday            234\n## 7               7        Thursday             45\n## 8               8       Wednesday             42\n## 9               9        Thursday              6\n## 10             10        Saturday            267\n## 11             11          Sunday             87\n## 12             12          Friday              3\n## 13             13        Saturday             12\n## 14             14          Sunday              9\nhead(a_dataframe)##   first_variable second_variable a_new_variable\n## 1              1          Monday             12\n## 2              2         Tuesday            261\n## 3              3          Monday             45\n## 4              4         Tuesday             29\n## 5              5          Monday             54\n## 6              6       Wednesday            234"},{"path":"getting-started-with-r-1.html","id":"functions","chapter":"3 Getting started with R","heading":"3.2.2 Functions","text":"function coded operation applies object (e.g.: number, textual feature etc.) transform based specific rules. function name (name function) arguments. Among arguments function always object value, instance numerical value, content function applied , possible arguments (either mandatory optional).Functions operations applied objects give certain output. E.g.: arithmetical operation “addition” function applies two numbers give, output, sum. arguments “sum” function numbers added together.name function written parentheses, arguments function inside parentheses:Arguments functions can numbers also textual features. instance, function paste creates string composed strings takes arguments.R can sometimes find “nested” syntax, can confusing. best practice keep things simple possible.sum , functions manipulate transform objects. Data wrangling, data visualization, well data analysis, performed functions.","code":"\nsum(5, 3)## [1] 8\npaste(\"the\", \"cat\", \"is\", \"at\", \"home\")## [1] \"the cat is at home\"\n# this comment, written after the hash mark, describe what is going on here: two \"paste\" function nested together have been used (improperly! because they make the code more complicated than necessary) to show how functions can be nested together. It would have been better to use the \"paste\" function just one time!\npaste(paste(\"the\", \"cat\", \"is\", \"at\", \"home\"), \"and\", \"sleeps\", \"on\", \"the\", \"sofa\")## [1] \"the cat is at home and sleeps on the sofa\""},{"path":"getting-started-with-r-1.html","id":"data-types","chapter":"3 Getting started with R","heading":"3.2.3 Data Types","text":"Variables can different R formats, :double: numbers include decimals (0.1, 5.676, 121.67). format appropriate continuous variables;integer: 1, 2, 3, 10, 400. format suitable count data;factors: categorical variables. Factors can ordered (e.g.: level agreement: “high”, “medium”, “low”), (e.g.: hair colors “blond”, “dark brown”, “brown”);characters: textual labels;logicals: format logical values (.e.: TRUE FALSE)dates: used represent days;POSIX: class R format represent dates times.\nFigure 3.1: R data formats. Tables Gaubatz, K. T. (2014). Survivor’s Guide R: Introduction Uninitiated Unnerved. SAGE Publications.\nbetter specify appropriate type data importing data set. example , data format specified using import process RStudio.Notice data type “date” requires users specify additional information regarding format dates. Indeed, dates can written many different ways, read dates R necessary specify structure date. example, dates format Year-Month-Day, represented R “%Y-%m-%d” (details provided another section book).Click watch “Import Data Specify Data Types”","code":""},{"path":"getting-started-with-r-1.html","id":"excercise","chapter":"3 Getting started with R","heading":"3.2.4 Excercise","text":"Upload data set “election news small”, using appropriate data format;Open script “basic-r-script” perform following operations:\nCheck first rows data set;\nAccess single columns;\nSave data frame name “election_news_small_test” folder “data” using function “write.csv” (review procedure go section “Load Save Data” book);\nComment code (comments written hash sign #);\nSave script.\nCheck first rows data set;Access single columns;Save data frame name “election_news_small_test” folder “data” using function “write.csv” (review procedure go section “Load Save Data” book);Comment code (comments written hash sign #);Save script.","code":""},{"path":"basic-data-wrangling-with-tidyverse.html","id":"basic-data-wrangling-with-tidyverse","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4 Basic Data Wrangling with Tidyverse","text":"Data wrangling process transforming mapping data one “raw” data form another format intent making appropriate valuable variety downstream purposes analytics. goal data wrangling assure quality useful data. Data analysts typically spend majority time process data wrangling compared actual analysis data.Another definition follows: Data wrangling process profiling transforming datasets ensure actionable set analysis tasks. One central goal make data usable: put data form can parsed manipulated analysis tools. Another goal ensure data responsive intended analyses: data contain necessary information, acceptable level description correctness, support successful modeling decision-making.“manipulate” data sets R:use basic R functions;employ specific libraries tidyverse. Tidyverse R library composed functions allow users perform basic advanced data science operations. https://www.tidyverse.org.R, library (“package”) coherent collection functions, usually created specific purposes.work tidyverse library, necessary install first, using following command: install.packages(“tidyverse”).installed tidyverse (library), necessary load , can work functions current R session:Besides using function install.packages(NAME---LIBRARY) using line code, also possible use RStudio interface.Click watch “Install Load Libraries”","code":"\n# to load a library used the command library(NAME-OF-THE-LIBRARY)\nlibrary(tidyverse)"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"the-pipe-operator","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.1 The Pipe Operator %>%","text":"Tidyverse peculiar syntax makes use -called pipe operator %>%, like following example:manipulate data sets can rely functions included dplyr: grammar data manipulation, providing consistent set verbs help solve common data manipulation challenges, mutate, rename, summarize.Click watch “Import Data Specify Data Types (Dates Times)”","code":"\na_dataframe %>%\n  group_by(second_variable) %>%\n  summarize(mean = mean(a_new_variable))## # A tibble: 7 × 2\n##   second_variable  mean\n##   <chr>           <dbl>\n## 1 Friday            3  \n## 2 Monday           37  \n## 3 Saturday        140. \n## 4 Sunday           48  \n## 5 Thursday         25.5\n## 6 Tuesday         145  \n## 7 Wednesday       138\nlibrary(readr)\ntweets <- read_csv(\"data/tweets_covid_small.csv\", \n    col_types = cols(created_at = col_datetime(format = \"%Y-%m-%d %H:%M:%S\"), \n        retweet_count = col_integer()))\nhead(tweets)## # A tibble: 6 × 4\n##   created_at          screen_name   source             retweet_count\n##   <dttm>              <chr>         <chr>                      <int>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0\n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0\n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0\n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"mutate","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.2 Mutate","text":"function mutate adds new variables data.frame overwrites existing variables.","code":"\ntweets <- tweets %>%\n  mutate(log_retweet_count = log(retweet_count))\n  \nhead(tweets)## # A tibble: 6 × 5\n##   created_at          screen_name   source             retweet_count log_retweet_count\n##   <dttm>              <chr>         <chr>                      <int>             <dbl>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0           -Inf   \n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0           -Inf   \n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3              1.10\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0           -Inf   \n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4              1.39\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96              4.56"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"rename","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.3 Rename","text":"rename function change name columns (sometimes can useful).previous two steps can performed time, concatenating operations pipe %>% operator.check data format variables stored data.frame, can use command str().Sometimes variables stored data.frame wrong format (see paragraph “data type”), may want convert new format. purpose, can use function mutate along functions .integer, .numeric, .character, .factor, .logical, .Date, .POSIXct() depending desired data format (possible advisable upload data paying attention type data. upload data correct format, can skip step).","code":"\ntweets <- tweets %>%\n  # rename (new_name = old_name)\n  rename(device = source)\n\nhead(tweets)## # A tibble: 6 × 5\n##   created_at          screen_name   device             retweet_count log_retweet_count\n##   <dttm>              <chr>         <chr>                      <int>             <dbl>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0           -Inf   \n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0           -Inf   \n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3              1.10\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0           -Inf   \n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4              1.39\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96              4.56\n# load again the data set\nlibrary(readr)\ntweets <- read_csv(\"data/tweets_covid_small.csv\", \n    col_types = cols(created_at = col_datetime(format = \"%Y-%m-%d %H:%M:%S\"), \n        retweet_count = col_integer()))\n\ntweets <- tweets %>%\n  mutate(log_retweet_count = log(retweet_count+1)) %>%\n  rename(device = source)\n\nhead(tweets)## # A tibble: 6 × 5\n##   created_at          screen_name   device             retweet_count log_retweet_count\n##   <dttm>              <chr>         <chr>                      <int>             <dbl>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0              0   \n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0              0   \n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3              1.39\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0              0   \n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4              1.61\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96              4.57\nstr(tweets)## tibble [100 × 5] (S3: tbl_df/tbl/data.frame)\n##  $ created_at       : POSIXct[1:100], format: \"2021-03-24 08:53:52\" \"2021-03-24 08:53:25\" ...\n##  $ screen_name      : chr [1:100] \"DoYourThingUK\" \"DoYourThingUK\" \"AlexS1595\" \"MakesworthAcc\" ...\n##  $ device           : chr [1:100] \"Twitter for iPhone\" \"Twitter for iPhone\" \"Twitter for iPhone\" \"Twitter Web App\" ...\n##  $ retweet_count    : int [1:100] 0 0 3 0 4 96 0 1 0 3 ...\n##  $ log_retweet_count: num [1:100] 0 0 1.39 0 1.61 ...\ntweets %>%\n  mutate(device = as.character(device)) %>%\n  head()## # A tibble: 6 × 5\n##   created_at          screen_name   device             retweet_count log_retweet_count\n##   <dttm>              <chr>         <chr>                      <int>             <dbl>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0              0   \n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0              0   \n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3              1.39\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0              0   \n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4              1.61\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96              4.57"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"summarize-and-group_by","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.4 Summarize and group_by","text":"aggregate data calculate summary values (instance, average number tweets day), can use function group_by (aggregate data, instance day) summarize calculate summary values.also possible create one summary variables .","code":"\ntweets_summary <- tweets %>%\n  group_by(screen_name) %>%\n  summarize(average_retweets = mean(retweet_count))\n\nhead(tweets_summary)## # A tibble: 6 × 2\n##   screen_name     average_retweets\n##   <chr>                      <dbl>\n## 1 2EXvoZ6nublpw1F              164\n## 2 AdilHaiderMD                  80\n## 3 AlexS1595                      3\n## 4 Andecave                      20\n## 5 ApKido                       150\n## 6 BBVA_Trader                    0\ntweets_summary <- tweets %>%\n  group_by(screen_name) %>%\n  summarize(average_retweets = mean(retweet_count),\n            average_log_retweets = mean(log_retweet_count))\n\nhead(tweets_summary)## # A tibble: 6 × 3\n##   screen_name     average_retweets average_log_retweets\n##   <chr>                      <dbl>                <dbl>\n## 1 2EXvoZ6nublpw1F              164                 5.11\n## 2 AdilHaiderMD                  80                 4.39\n## 3 AlexS1595                      3                 1.39\n## 4 Andecave                      20                 3.04\n## 5 ApKido                       150                 5.02\n## 6 BBVA_Trader                    0                 0"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"count-occurrences","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.4.1 Count occurrences","text":"useful operation perform summarizing data count occurrences certain variable. instance, count number tweets sent user, can use function n() inside summarize function.","code":"\ntweets_summary <- tweets %>%\n  group_by(screen_name) %>%\n  summarize(average_retweets = mean(retweet_count),\n            average_log_retweets = mean(log_retweet_count),\n            number_of_tweets = n())\n\nhead(tweets_summary)## # A tibble: 6 × 4\n##   screen_name     average_retweets average_log_retweets number_of_tweets\n##   <chr>                      <dbl>                <dbl>            <int>\n## 1 2EXvoZ6nublpw1F              164                 5.11                1\n## 2 AdilHaiderMD                  80                 4.39                1\n## 3 AlexS1595                      3                 1.39                1\n## 4 Andecave                      20                 3.04                1\n## 5 ApKido                       150                 5.02                1\n## 6 BBVA_Trader                    0                 0                   1"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"arrange","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.5 Arrange","text":"explore data set, can useful sort data (e.g., lowest highest value variable). tidyverse, can order data.frame using function arrange.sort data highest lowest value (descending order), minus sign (desc function) added.Without minus sign (“desc” command), data sorted lowest highest value.","code":"\ntweets_summary  %>%\n  arrange(-number_of_tweets) %>%\n  head()## # A tibble: 6 × 4\n##   screen_name     average_retweets average_log_retweets number_of_tweets\n##   <chr>                      <dbl>                <dbl>            <int>\n## 1 iprdhzb                    0.667                0.462                3\n## 2 DoYourThingUK              0                    0                    2\n## 3 MakesworthAcc              2                    0.805                2\n## 4 benphillips76              3.5                  1.45                 2\n## 5 viralvideovlogs            3.5                  1.45                 2\n## 6 2EXvoZ6nublpw1F          164                    5.11                 1\ntweets_summary  %>%\n  arrange(desc(average_retweets)) %>%\n  head()## # A tibble: 6 × 4\n##   screen_name    average_retweets average_log_retweets number_of_tweets\n##   <chr>                     <dbl>                <dbl>            <int>\n## 1 Oliver_Miguel1             1988                 7.60                1\n## 2 Lil_3arbiii                1627                 7.40                1\n## 3 Kittyhawk681               1285                 7.16                1\n## 4 JulesFox12                 1091                 7.00                1\n## 5 rosaesaa26                  983                 6.89                1\n## 6 lewisabzueta                822                 6.71                1\ntweets_summary  %>%\n  arrange(number_of_tweets) %>%\n  head()## # A tibble: 6 × 4\n##   screen_name     average_retweets average_log_retweets number_of_tweets\n##   <chr>                      <dbl>                <dbl>            <int>\n## 1 2EXvoZ6nublpw1F              164                 5.11                1\n## 2 AdilHaiderMD                  80                 4.39                1\n## 3 AlexS1595                      3                 1.39                1\n## 4 Andecave                      20                 3.04                1\n## 5 ApKido                       150                 5.02                1\n## 6 BBVA_Trader                    0                 0                   1"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"filter","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.6 Filter","text":"function filter keeps cases (“rows”) want focus . arguments function conditions must fulfilled filter data: ) name column want filter, b) values kept.examples , notice use double equal sign ==, also quotation marks indicate modalities categorical variable.also possible use several conditions time.","code":"\ntweets %>%\n  filter(retweet_count >= 500) %>%\n  arrange(-retweet_count)## # A tibble: 10 × 5\n##    created_at          screen_name     device          retweet_count log_retweet_count\n##    <dttm>              <chr>           <chr>                   <int>             <dbl>\n##  1 2021-03-24 08:53:31 Oliver_Miguel1  Twitter for An…          1988              7.60\n##  2 2021-03-24 08:53:48 Lil_3arbiii     Twitter for iP…          1627              7.40\n##  3 2021-03-24 08:53:48 Kittyhawk681    Twitter Web App          1285              7.16\n##  4 2021-03-24 08:53:37 JulesFox12      Twitter for An…          1091              7.00\n##  5 2021-03-24 08:53:42 rosaesaa26      Twitter for An…           983              6.89\n##  6 2021-03-24 08:53:42 lewisabzueta    Twitter for An…           822              6.71\n##  7 2021-03-24 08:53:42 jonvthvn08      Twitter for iP…           768              6.65\n##  8 2021-03-24 08:53:34 florent61647053 Twitter for An…           768              6.65\n##  9 2021-03-24 08:53:37 Ritu89903967    Twitter for An…           709              6.57\n## 10 2021-03-24 08:53:33 Hurica3         Twitter for iP…           575              6.36\ntweets %>%\n  filter(retweet_count == 1988) ## # A tibble: 1 × 5\n##   created_at          screen_name    device            retweet_count log_retweet_count\n##   <dttm>              <chr>          <chr>                     <int>             <dbl>\n## 1 2021-03-24 08:53:31 Oliver_Miguel1 Twitter for Andr…          1988              7.60\ntweets %>%\n  filter(device == \"Twitter for Android\")## # A tibble: 33 × 5\n##    created_at          screen_name     device          retweet_count log_retweet_count\n##    <dttm>              <chr>           <chr>                   <int>             <dbl>\n##  1 2021-03-24 08:53:49 marcin_lukawski Twitter for An…             1             0.693\n##  2 2021-03-24 08:53:49 LebodyRanya     Twitter for An…             0             0    \n##  3 2021-03-24 08:53:47 anshunandanpra4 Twitter for An…             2             1.10 \n##  4 2021-03-24 08:53:44 insoumise007    Twitter for An…             5             1.79 \n##  5 2021-03-24 08:53:43 Metamorfopsies  Twitter for An…             0             0    \n##  6 2021-03-24 08:53:43 keepsmiling_130 Twitter for An…           164             5.11 \n##  7 2021-03-24 08:53:43 lovebresil01    Twitter for An…            81             4.41 \n##  8 2021-03-24 08:53:42 LightHealing    Twitter for An…             1             0.693\n##  9 2021-03-24 08:53:42 lewisabzueta    Twitter for An…           822             6.71 \n## 10 2021-03-24 08:53:42 rosaesaa26      Twitter for An…           983             6.89 \n## # ℹ 23 more rows\ntweets %>%\n  filter(device == \"Twitter for Android\",\n         retweet_count > 200) %>%\n  arrange(-retweet_count)## # A tibble: 8 × 5\n##   created_at          screen_name     device           retweet_count log_retweet_count\n##   <dttm>              <chr>           <chr>                    <int>             <dbl>\n## 1 2021-03-24 08:53:31 Oliver_Miguel1  Twitter for And…          1988              7.60\n## 2 2021-03-24 08:53:37 JulesFox12      Twitter for And…          1091              7.00\n## 3 2021-03-24 08:53:42 rosaesaa26      Twitter for And…           983              6.89\n## 4 2021-03-24 08:53:42 lewisabzueta    Twitter for And…           822              6.71\n## 5 2021-03-24 08:53:34 florent61647053 Twitter for And…           768              6.65\n## 6 2021-03-24 08:53:37 Ritu89903967    Twitter for And…           709              6.57\n## 7 2021-03-24 08:53:27 aspeaker66      Twitter for And…           331              5.81\n## 8 2021-03-24 08:53:42 JamesAn26254230 Twitter for And…           201              5.31"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"select","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.7 Select","text":"select used keep columns original data.frame. instance, can apply function keep just columns device retweet_count.","code":"\ntweets %>%\n  dplyr::select(device, retweet_count) %>%\n  head()## # A tibble: 6 × 2\n##   device             retweet_count\n##   <chr>                      <int>\n## 1 Twitter for iPhone             0\n## 2 Twitter for iPhone             0\n## 3 Twitter for iPhone             3\n## 4 Twitter Web App                0\n## 5 Twitter Web App                4\n## 6 Twitter for iPad              96"},{"path":"basic-concepts.html","id":"basic-concepts","chapter":"5 Basic Concepts","heading":"5 Basic Concepts","text":"","code":""},{"path":"basic-concepts.html","id":"time-series","chapter":"5 Basic Concepts","heading":"5.1 Time Series","text":"time series serially sequenced set values representing variable value different points time (VanLear, “Time Series Analysis”). consists measures collected time, regular time intervals, unit observation, resulting set ordered values. regularity frequency time series (can , instance, hourly, weekly, monthly, quarterly, yearly etc.).Time series data different cross-sectional data, set data observed sample units taken given point time, time dimension relevant can ignored. Cross-sectional data snapshot population interest one particular point time, time series show dynamical evolution variable time. Panel data combine cross-sectional time series data observing units time.Time fundamental variable time series. often relevant types statistical analyses. Also sociological perspective (psychological well), can see past events influence future behaviors. Oftentimes, can make reasonable prediction future social behaviors just observing past behaviors. Actually, social reproduction behaviors time predictability future social behaviors based past experience shared knowledge essential social order, thus, fundamental dimension human society.statistical perspective, impact time resulting repeated measurements time single subject unit, introduce dependency among data points prevents use common statistical techniques. cross-sectional data, observations assumed independent: values observed one unit influence values observed units. Time series observations different nature: time series collection independent observations, observations taken independent units, collection successive observations unit. Observations taken across units time (without regards time), across time unit.dealing time series data, time important factor taken account. introduces new dimension data. instance, can calculate variable increases decreases time, peaks given moment time, regular intervals. consider just , much, variable correlated another variable, correlation time among , peaks one variable precedes peaks one, much time requires variable impact another one, much impact changes time.Importantly, dealing time series data, acknowledge sampling adjacent points time introduces correlation data. serial dependency creates correlated errors violates assumptions many traditional statistical analyses can bias estimation error confidence intervals significance tests. characteristic time series data, general, precludes use common statistical approaches linear regression correlation analysis, assume observations independent.application “standard” statistical techniques time series data might lead foolish, totally unreliable results. instance, statisticians George Udny Yule wrote:«fairly familiar knowledge sometimes obtain quantities varying time (time-variables) quite high correlations attach physical significance whatever, although ordinary test correlation held certainly “significant.” (…) occurrence “nonsense-correlations” makes one mistrust serious arguments sometimes put forward basis correlations time-series. […] successive x’s y’s sample longer form random series, series successive terms closely related one another, usual conceptions (correlation, ed.) accustomed fail totally entirely apply» (Yule, G.U. (1926). sometimes get nonsense-correlations Time-Series? study sampling nature time-series. Journal royal statistical society, 89(1), 1-63.)funny website reporting spurious time series correlation tylervigen.com.Despite can funny see improbable correlations, keep mind adopting right approach analyze data serious issue research. paper American Journal Political Science, can read, instance:results analysis strongly suggest way event counts analyzed hundreds important political science studies produced statistically substantively unreliable results. Misspecification, inefficiency, bias, inconsistency, insufficiency, problems result unknowing application two common methods without theoretical justification empirical utility type data.Due peculiarity time series data, time series analysis developed specific statistical methodology appropriate analysis time-dependent data. Time series analysis aims providing understanding underlying processes patterns change time unit observation relations variables observed time, handling time structure data proper way.","code":""},{"path":"basic-concepts.html","id":"time-series-analysis","chapter":"5 Basic Concepts","heading":"5.2 Time Series Analysis","text":"Time series analysis approach employed many disciplines. Almost every field study data characterized time development, every phenomenon temporal dimension can conceived time series analyzed time series analysis methods. Time series analysis important part data analysis disciplines economics, analyze, instance, inflation trends; marketing, analyze number clients store number accesses e-commerce website; demography, study growth national population time trends population ageing; engineering, analyze radio frequencies; neurology, analyze brain waves detected electroencephalograms. Political science can interested studying patterns alternation political parties government, digital communication can use time series analysis study series tweets using hashtag, news media coverage certain topic, trends user searches search engines, provided Google Trends.use time series analysis communication science, can observed :“Many major theories models field contain time central player: two-step flow, cultivation, spiral--silence, agenda-setting, framing, communication mediation models, name (Nabi & Oliver, 2009). articulates set processes play time: Messages work way media systems networks, citizens perceive world around decide communicate, , make choices participation, presumably product process includes communication exposure. Indeed, words animate field—effect, flow, influence, dynamic, cycle—reveal understanding communication process, processes temporal dimensions (Box-Steffensmeier, Freeman, Hitt, & Pevehouse, 2014). perspective time series analysis can help expand notions time’s role dynamics. see several ways can become attentive time field”. Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, ., & Yang, J. (2019). Temporal Turn Communication Research: Time Series Analyses Using Computational Approaches. International Journal Communication (19328036), 13.“One common applications time series analyses mass communication agenda-setting research. approach correlate national news coverage topic time public opinion public policy topic, often estimate lagged effects decay effects time. Likewise, trends cycles television programming, viewing, advertising, explored time series analyses. interpersonal literature, popular one important applications time series analysis investigation mutual adaptation form patterns reciprocity compensation conversational partners course interaction.” (C. Arthur VanLear, “Time Series Analysis”, Allen, M. (Ed.). (2017). SAGE encyclopedia communication research methods. Sage Publications).general, can distinguish least following objectives time series analysis study:DESCRIPTION: Description process characterized intrinsic temporal dimension. Simple examples related questions : upward trend? peak certain point time? regular pattern recurring every year, particular moment time? Descriptive questions like can answered via descriptive time series analysis.EVALUATION: Evaluation impact certain event, occurring particular point time, process. instance: change social media moderation policy, led banning accounts linked conspiracy theories, impact quantity fake news shared online users? Specific time series techniques can used perform kind analysis.EXPLANATION: Explanation phenomenon characterized time series structure basis related variables. instance: quantity news shared Facebook help explain polarization debate online? volume news media articles topic help explain growth debate online topic? Inferential statistical techniques, regression models developed time series, used answer questions like .FORECASTING: Prediction future values process. instance: can expect news media coverage certain topic keep growing near future? subject time series forecasting.can also distinguish univariate multivariate time series analysis. Time series analysis can used explain temporal dependencies within processes. temporal dependency within social process, mean current value variable , part, function previous values variable. analyze univariate structure time series, univariate techniques used. Temporal dependency social processes, conversely, indicates current value variable part function previous values variables. Multivariate time series analysis used explain relations time series.","code":""},{"path":"basic-concepts.html","id":"stochastic-and-deterministic-processes","chapter":"5 Basic Concepts","heading":"5.3 Stochastic and Deterministic Processes","text":"general distinction can made time series, based deterministic non-deterministic nature.deterministic time series one can explicitly expressed analytic expression. random probabilistic parts. always possible exactly predict future behavior, state behaved past. Deterministic processes pretty rare dealing individual social behaviors! Predicting future behaviors crowd, person, social group, can reasonably possible, sometimes, based past behaviors contextual information, since human behavior partly influenced past. However, totally determined past. always certain degree uncertainty prediction; human behaviors , generally speaking, fully predictable.Social individual behaviors, therefore, non-deterministic. non-deterministic time series fully described analytic expression. random, probabilistic component, prevents behavior explicitly described. possible say, probabilistic terms, future behavior might . However, always residual, unpredictable, component. time series may considered non-deterministic also information necessary describe explicitly available, although might principle, nature generating process, part , inherently random. can say time series analyzed social science always, least, stochastic component makes totally deterministic.Since non-deterministic time series random component, follow probabilistic rather deterministic laws. Random data defined explicit mathematical relations, rather statistical terms, , probability distributions parameters mean variance. Non-deterministic time series can analyzed assuming manifestations probabilistic stochastic processes.","code":""},{"path":"time-series-objects.html","id":"time-series-objects","chapter":"6 Time Series Objects","heading":"6 Time Series Objects","text":"","code":""},{"path":"time-series-objects.html","id":"time-series-objects-1","chapter":"6 Time Series Objects","heading":"6.1 Time Series Objects","text":"Every object manipulate R characterized specific structure. Objects’ structures vary depending type object: list, matrix, data.frame, different objects different structures. Every structure manipulation methods. instance, can accessed analyzed using different functions strings code.R many different types object. get overview can refer handbook, R manual, chapter 5 free online book. course going learn data structures deal conducting time series analysis R, , structure time series objects data sets.Time series data sets R can represented different objects. Specific libraries (coherent collections functions) can give different structures time series data sets.many R libraries handling working time series objects. general useful perform specific analysis. page can find comprehensive list R libraries time series analysis.now, just need know different libraries can create time series objects different structures can manipulated different functions. means objects can analyzed functions, well always compatibility R libraries.\nMany functions developed reference specific libraries objects, require particular object structure. consequence, creating time series object certain structure certain library can imply can use functions perform certain type analysis. words, specific type objects introduce specific constrains data analysis (visualization), wise plan advance necessary analyses, select necessary libraries data structure.now introduce three types objects commonly used store analyze time series data:data.frame (base R)ts object (base R)xts object (created library xts).analyze structures objects strengths limitations. next chapter, ’ll also learn methods available visualize .","code":""},{"path":"time-series-objects.html","id":"time-series-as-data-frames","chapter":"6 Time Series Objects","heading":"6.1.1 Time Series as Data Frames","text":"Data frames (data.frame) common data set structure R. data.frame simply table cases variables (row case column variable).see example data.frame containing time series data, can upload data set containing number news articles mentioning keyword “elections” published USA news media. retrieved data set MediaCloud, free open source platform studying media ecosystem tracks millions stories published online. can download data set link.can upload .csv file using function read.csv. main argument read.csv function path file.using function class can see data.frame.can check first rows data.frame using function head, shows first rows data set, get idea structure simple data.frame.data.frame contains time series data: first column contains dates, columns contain values observations. can also see data.frame seems contain daily data, row corresponds specific day. data frame also includes, column “count”, number news articles mentioning keyword “elections”, column “total_count”, total number news articles topics, column “ratio” proportion news articles mentioning keyword (count/total_count).function head (tail) can impractical data.frame including lot columns, better use function str check structure data.frame.can see output function str, format column date Factor. format , case, automatically attributed R, (already said) can specified importing data.Factor appropriate format categorical variables, R includes specific format dates times. case just date, can convert variable type date. can change format variable using function .Date.can also perform operation tidyverse, using function mutate.data.frame common format data sets, including time series data sets. can many things data stored format, creating plots performing various types analysis. However, handle time series R specific data formats.","code":"\nelections_news <- read.csv(\"./data/elections-stories-over-time-20210111144254.csv\")\nclass(elections_news)## [1] \"data.frame\"\nhead(elections_news)##         date count total_count      ratio\n## 1 2015-01-01   373       25611 0.01456405\n## 2 2015-01-02   387       31932 0.01211950\n## 3 2015-01-03   289       24646 0.01172604\n## 4 2015-01-04   322       25513 0.01262102\n## 5 2015-01-05   567       39982 0.01418138\n## 6 2015-01-06   626       42366 0.01477600\nstr(elections_news)## 'data.frame':    2192 obs. of  4 variables:\n##  $ date       : chr  \"2015-01-01\" \"2015-01-02\" \"2015-01-03\" \"2015-01-04\" ...\n##  $ count      : int  373 387 289 322 567 626 507 521 531 346 ...\n##  $ total_count: int  25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ...\n##  $ ratio      : num  0.0146 0.0121 0.0117 0.0126 0.0142 ...\nelections_news$date <- as.Date(elections_news$date)\nstr(elections_news)## 'data.frame':    2192 obs. of  4 variables:\n##  $ date       : Date, format: \"2015-01-01\" \"2015-01-02\" ...\n##  $ count      : int  373 387 289 322 567 626 507 521 531 346 ...\n##  $ total_count: int  25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ...\n##  $ ratio      : num  0.0146 0.0121 0.0117 0.0126 0.0142 ...\nlibrary(tidyverse)\n\nelections_news <- elections_news %>%\n  mutate(date = as.Date(date))"},{"path":"time-series-objects.html","id":"time-series-as-ts-objects","chapter":"6 Time Series Objects","heading":"6.1.2 Time Series as TS objects","text":"basic object created handle time series R object class ts. name stands “Time Series”.example ts object already present R name “AirPassengers”, time series data set ts format. can load data set function data.applying function class can see object class ts.AirPassengers small data set can print data set see structure, example standard structure ts object.calling str function get synthetic information object.AirPassengers data set univariate time series representing monthly totals international airline passengers 1949 1960. every time series, start date end date. also frequency, frequency observations taken.characteristics differentiate ts object data.frame. structure data.frame lacks start end date, frequency value.functions start, end, frequency, can applied ts object check values. started saying functions work objects types objects. example. functions, indeed, work ts objects just part structures, arguments usually specified kind object created. work applied data.frame object, since data.frame structure include start end date, frequency observations. can seen ts structure much specific time series data.Importantly, frequency time series assumed regular time. applies time series general, just ts objects. case, time series starts January 1949 ends December 1969, monthly frequency. Monthly frequency indicated ts “12”, meaning 12 months. Indeed, reference unit ts object year. , quarterly data, instance, frequency equal 4.create ts object necessary follow specific steps use specific functions. exemplify process creation ts object take example data contained AirPassengers data set, store data.frame (don’t need learn , just copy paste code). data.frame data set format probably start , can useful see create ts object starting data.frame.create “ts” time series object starting data.frame, need:specify column contains observations. case, column name “Passengers”.need specify start end date, case format year/month, can just years case yearly data. ts format start/end date following: c=(YEAR, MONTH). c represents concatenate function, concatenates year month single vector.Finally, indicate frequency time series observations. frequency specified based time period year, case frequency equal 12, monthly observation, meaning 12 observation per year.’s important notice yearly, quarterly, monthly data work fine ts structure, fine grained data create complications totally suitable ts structure.due fact time series objects require frequency observations regular ts observations regular reference year. Unfortunately, time series spans many years composed constant number days, since number days sometimes 365 time 366, case leap years. limitation ts objects. However, dealing monthly data data frequency lower one month (quarterly data), ts works great.","code":"\ndata(AirPassengers)\nclass(AirPassengers)## [1] \"ts\"\nAirPassengers##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n## 1949 112 118 132 129 121 135 148 148 136 119 104 118\n## 1950 115 126 141 135 125 149 170 170 158 133 114 140\n## 1951 145 150 178 163 172 178 199 199 184 162 146 166\n## 1952 171 180 193 181 183 218 230 242 209 191 172 194\n## 1953 196 196 236 235 229 243 264 272 237 211 180 201\n## 1954 204 188 235 227 234 264 302 293 259 229 203 229\n## 1955 242 233 267 269 270 315 364 347 312 274 237 278\n## 1956 284 277 317 313 318 374 413 405 355 306 271 306\n## 1957 315 301 356 348 355 422 465 467 404 347 305 336\n## 1958 340 318 362 348 363 435 491 505 404 359 310 337\n## 1959 360 342 406 396 420 472 548 559 463 407 362 405\n## 1960 417 391 419 461 472 535 622 606 508 461 390 432\nstr(AirPassengers)##  Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\nstart(AirPassengers)[1]## [1] 1949\nend(AirPassengers)[1]## [1] 1960\nfrequency(AirPassengers)[1]## [1] 12\ndate <- seq.Date(from = as.Date(\"1949-01-01\"), \n                 to = as.Date(\"1960-12-01\"), by=\"month\")\npassengers <- as.vector(AirPassengers)\n\ndata_frame_format <- data.frame(\"Date\" = date, \n                                \"Passengers\" = passengers)\nts_format <- ts(data = data_frame_format$Passengers, \n                start=c(1949, 01), \n                end=c(1960, 12), \n                frequency = 12)\nts_format##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n## 1949 112 118 132 129 121 135 148 148 136 119 104 118\n## 1950 115 126 141 135 125 149 170 170 158 133 114 140\n## 1951 145 150 178 163 172 178 199 199 184 162 146 166\n## 1952 171 180 193 181 183 218 230 242 209 191 172 194\n## 1953 196 196 236 235 229 243 264 272 237 211 180 201\n## 1954 204 188 235 227 234 264 302 293 259 229 203 229\n## 1955 242 233 267 269 270 315 364 347 312 274 237 278\n## 1956 284 277 317 313 318 374 413 405 355 306 271 306\n## 1957 315 301 356 348 355 422 465 467 404 347 305 336\n## 1958 340 318 362 348 363 435 491 505 404 359 310 337\n## 1959 360 342 406 396 420 472 548 559 463 407 362 405\n## 1960 417 391 419 461 472 535 622 606 508 461 390 432\nclass(ts_format)## [1] \"ts\""},{"path":"time-series-objects.html","id":"time-series-as-xtszoo-objects","chapter":"6 Time Series Objects","heading":"6.1.3 Time Series as XTS/ZOO objects","text":"Time series can stored object class xts/zoo. class objects created library xts, related extension package zoo (another package deal time series data). libraries, requires installed loaded.xts object flexible ts one.\ncan create xts time series starting data.frame just created. Similarly required ts, need specify:column data.frame (vector) containing data;column data.frame (vector) containing dates/times (date/time format);frequency observations.can use data.frame already created AirPassengers data create new xts object.Also structure object, like ts one, includes range dates time series, starting ending date.","code":"\n# install.packages(\"xts\")\nlibrary(xts)\nxts_format <- xts(x = data_frame_format$Passengers, \n                  order.by = data_frame_format$Date, \n                  frequency = 12)\nclass(xts_format)## [1] \"xts\" \"zoo\"\nstr(xts_format)## An xts object on 1949-01-01 / 1960-12-01 containing: \n##   Data:    double [144, 1]\n##   Index:   Date [144] (TZ: \"UTC\")\nhead(xts_format)##            [,1]\n## 1949-01-01  112\n## 1949-02-01  118\n## 1949-03-01  132\n## 1949-04-01  129\n## 1949-05-01  121\n## 1949-06-01  135"},{"path":"plot-time-series.html","id":"plot-time-series","chapter":"7 Plot Time Series","heading":"7 Plot Time Series","text":"","code":""},{"path":"plot-time-series.html","id":"plot-time-series-objects","chapter":"7 Plot Time Series","heading":"7.1 Plot Time Series Objects","text":"lecture going learn plot time series data.take account three main functions: ggplot tidyverse library, plot.ts base R, plot.xts xts library. Ggplot probably versatile function perspective graphical results can obtained, also complex, ordinary visualization, plot.ts probably easiest tool.Plotting time series important part analysis permits visualizing exploring data, univariate perspective (focusing characteristics single time series) multivariate perspective (focusing characteristics many time series, relations ). visualize explore relations time series, ’ll learn plot single time series well many different time series .","code":""},{"path":"plot-time-series.html","id":"plot.ts","chapter":"7 Plot Time Series","heading":"7.2 plot.ts","text":"can visualize time series using function plot.ts() applied time series data ts format.example ts time series provided AirPassengers dataset, already included R (can load data running data(“AirPassengers”)).can add many details plot, title, label y axis x axis, change colors plot (use colors() see list standard colors R), size line.can use plot.ts plot two () time series together, useful operation take look relations. different time series must structure (starting date, ending date, frequency), stored ts object.merge one “ts” object two time series already ts format, can employ function ts.union(). can plot time series plot, create two different plots, using option “plot.type” specifying single multiple.lwd control line width (line size):line width, positive number, defaulting 1. interpretation device-specific, devices implement line widths less one.\nlty control line type:Line types can either specified integer (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) one character strings “blank”, “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, “twodash”, “blank” uses ‘invisible lines’ (.e., draw ).parameter nc can control number columns used display data.explore long time series can useful focus limited time window. , can subset data using function window. function extracts subset data observed specified start end time. can use function window plot.ts function (alternatively, can create new object applying function window first, plot new object).window function, can also specify frequency, series re-sampled new frequency. instance, re-sampling quarterly frequency, function keeps observations made January, April, July, October, re-sampling six-month frequency, keeps observations made January July.can use window function also one time series.learn something graphical options plot.ts, can open read help page using ?plot.ts. question mark followed name function opens help page function.","code":"\ndata(\"AirPassengers\")\nplot.ts(AirPassengers)\nplot.ts(AirPassengers, \n     main = \"PASSENGERS\",\n     xlab = \"1949-1960 (monthly data)\",\n     ylab = \"Passengers (1000's)\",\n     col = \"violetred3\", \n     lwd=5)\n# create a \"toy\" time series with the same lenght of the AirPassenger one\nAirPassengers_2 <- AirPassengers + 100\nAirPassengers_3 <- AirPassengers + 300\n\nAirPassengers_multi <- ts.union(AirPassengers, AirPassengers_2, AirPassengers_3)\n\nplot.ts(AirPassengers_multi, \n        main = \"Three time series\",\n        xlab = \"TIME\", ylab = \"VALUES\",\n        col = c(\"blue\", \"red\", \"black\"), \n        lwd=c(1, 1, 1), lty=c(1, 2, 3),\n        plot.type = \"single\")\nplot.ts(AirPassengers_multi, \n        main = \"Three time series\",\n        xlab = \"TIME\", ylab = \"VALUES\",\n        col = \"blue\", \n        lwd=4,\n        plot.type = \"multiple\")\nplot.ts(AirPassengers_multi, \n        main = \"Three time series\",\n        xlab = \"TIME\", ylab = \"VALUES\",\n        col = \"orange\",\n        lwd=4,\n        plot.type = \"multiple\",\n        nc=3)\nplot.ts(window(AirPassengers, start=c(1950, 01), end=c(1954, 12)), \n     main = \"PASSENGERS (1950-1955)\",\n     xlab = \"1950-1955 (monthly data)\",\n     ylab = \"Passengers (1000's)\",\n     col = \"violetred3\", \n     lwd=5)\nplot.ts(window(AirPassengers, start=c(1950, 01), end=c(1954,12), frequency = 4), \n        main = \"PASSENGERS (1950-1955) - QUARTERLY DATA\",\n        xlab = \"1950-1955 (Quarterly data)\",\n        ylab = \"Passengers (1000's)\",\n        col = \"violetred3\", \n        lwd=5)\nplot.ts(window(AirPassengers_multi, start=c(1950, 01), end=c(1954,12), frequency = 4), \n        main = \"PASSENGERS (1950-1955) - QUARTERLY DATA\",\n        xlab = \"1950-1955 (Quarterly data)\",\n        ylab = \"Passengers (1000's)\",\n        col = \"violetred3\", \n        lwd=5)"},{"path":"plot-time-series.html","id":"plot.xts","chapter":"7 Plot Time Series","heading":"7.3 plot.xts","text":"plot xts object can similarly use plot.xts function.can create xts object xts function (see previous chapter), already ts object, can also convert xts object using function .xtsBy using multi.panel=TRUE, multi.panel=FALSE can plot time series panel using different panels.subset data, order visualize focus just one part series, instead function window, write dates squared brackets examples .can also change frequency observations using specific functions xts library.using function periodicity can find frequency time series.function .period can re-sample data “seconds”, “minutes”, “hours”, “days”, “weeks”, “months”, “quarters”, “years”. can re-sample data higher lower frequency, lower higher one. instance, monthly data, can aggregate data quarterly yearly data, create weekly hourly time series.result .period function contain open (first) close (last) value given period, well maximum minimum new period, reflected new high low, respectively.can plot values, select value using square brackets comma, followed number column want plot (see table , 4 columns). notation way access columns (rows) data.frame matrix. write name data.frame matrix, squared brackets indicate index rows, first position comma, index columns, second position, comma. , instance, access value second column second row data.set “data”, can write data[2,2], access values third column first row, can write data[1,3]. leave blank space column row space, get values column rows. Therefore, writing data[,2], data[,3], get values column 2 3, respectively.can also re-sample calculate average (another statistics) new period. instance, example re-sample data year calculate average. also possible calculate statistics , instance, median, just writing “median” instead “mean”.can find additional details xts plot.xts reading help functions. link can find synthetic presentation functions xts library.","code":"\nlibrary(xts)\n\nAirPassengers_xts <- as.xts(AirPassengers)\n\nplot.xts(AirPassengers_xts,\n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         col = \"steelblue2\", \n         lwd=5)\nAirPassengers_multi_xts <- as.xts(AirPassengers_multi)\n\nplot.xts(AirPassengers_multi_xts,\n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = T)\nplot.xts(AirPassengers_multi_xts,\n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = F)\nplot.xts(AirPassengers_multi_xts[\"1950-01/1954-12\"], \n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = F)\nplot.xts(AirPassengers_xts[\"1950-01/1956-06\"], \n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = F)\nperiodicity(AirPassengers_xts)## Monthly periodicity from Jan 1949 to Dec 1960\nto.period(AirPassengers_xts, period=\"years\")##          AirPassengers_xts.Open AirPassengers_xts.High AirPassengers_xts.Low\n## Dec 1949                    112                    148                   104\n## Dec 1950                    115                    170                   114\n## Dec 1951                    145                    199                   145\n## Dec 1952                    171                    242                   171\n## Dec 1953                    196                    272                   180\n## Dec 1954                    204                    302                   188\n## Dec 1955                    242                    364                   233\n## Dec 1956                    284                    413                   271\n## Dec 1957                    315                    467                   301\n## Dec 1958                    340                    505                   310\n## Dec 1959                    360                    559                   342\n## Dec 1960                    417                    622                   390\n##          AirPassengers_xts.Close\n## Dec 1949                     118\n## Dec 1950                     140\n## Dec 1951                     166\n## Dec 1952                     194\n## Dec 1953                     201\n## Dec 1954                     229\n## Dec 1955                     278\n## Dec 1956                     306\n## Dec 1957                     336\n## Dec 1958                     337\n## Dec 1959                     405\n## Dec 1960                     432\nplot.xts(to.period(AirPassengers_xts, period=\"years\")[,2],\n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = F)\nindex_years <- endpoints(AirPassengers_xts, on = \"year\")\nAirPassengers_xts_year_avg <- period.apply(AirPassengers_xts, INDEX=index_years, FUN=mean)## NOTE: `period.apply(..., FUN = mean)` operates by column, unlike other math\n##   functions (e.g. median, sum, var, sd). Please use `FUN = colMeans` instead,\n##   and use `FUN = function(x) mean(x)` to take the mean of all columns. Set\n##   `options(xts.message.period.apply.mean = FALSE)` to suppress this message.\nplot.xts(AirPassengers_xts_year_avg,\n         main = \"PASSENGERS (Year Average)\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = \"blue\",\n         multi.panel = F)"},{"path":"plot-time-series.html","id":"ggplot","chapter":"7 Plot Time Series","heading":"7.4 ggplot","text":"Ggplot2 tidyverse library data visualization. can use create time series plots many types plot.upload dataset first, set appropriate time format date.place dataset (“elections_news”) inside ggplot function. Notice ggplot syntax similar tidyverse one, uses plus sign instead pipe one (%>%).create line plot ggplot necessary use geom_line function. function requires two parameters: data x-axis data y-axis. parameters written inside aes function. can also specify colors size line. using additional functions, plus sign, can also set labels x- y-axes, title, subtitle, caption plot. can also change overall aspect plot using one themes included library.can also plot one series. instance, can create two plots, use function grid.arrange, library gridExtra combine plots together.can also plot two two series plot.focus shorter time window, can use dplyr function filter. Besides filtering data, add function “scale_x_datetime”, control labels x-axis, specifying want use monthly labels.can also use ggplot annotate date. create annotations ggplot can use function “annotate”, label data point, “geom_segment”, trace lines connecting data points labels.can learn annotation ggplot : https://ggplot2-book.org/annotations.html\nlines : https://ggplot2.tidyverse.org/reference/geom_segment.html","code":"\nggplot(elections_news) +\n  geom_line(aes(x = date, y = ratio), color = \"snow4\", size = 0.5) +\n  ylab(\"News Articles\") +\n  xlab(\"Date\") +\n  labs(title = \"Time Series of News Articles on Elections\",\n       subtitle = \"Data from MediaCloud\",\n       caption = \"Data Analysis II\") +\n  theme_classic()\n# install.packages(\"gridExtra\")\nlibrary(gridExtra)\n\np1 <- elections_news %>%\n  ggplot() +\n  geom_line(aes(x = date, y = ratio), col = \"black\", size = 0.5) +\n  ylab(\"News Articles (ratio)\") +\n  xlab(\"Date\") +\n  ggtitle(\"MediaCloud Data on Elections (Daily)\") \n\np2 <- elections_news %>%\n  ggplot() +\n  geom_line(aes(x = date, y = count), col=\"red\", size=0.5) +\n  ylab(\"News Articles (count)\") +\n  xlab(\"Date\") +\n  ggtitle(\"MediaCloud Data on Elections (Daily)\") \n\ngrid.arrange(p1,p2)\nelections_news <- elections_news %>%\n  mutate(time_series_data_2 = count*2,\n         time_series_data_3 = count*4)\n\nggplot(elections_news) +\n  geom_line(aes(x = date, y = count), col = \"black\", size = 0.5) +\n  geom_line(aes(x = date, y = time_series_data_2), col = \"blue\", size = 0.5) +\n  geom_line(aes(x = date, y = time_series_data_3), col=\"red\", size=0.5) +\n  ylab(\"\") +\n  xlab(\"Date\") +\n  ggtitle(\"MediaCloud Data on Elections (Daily)\") \nelections_news %>%\n  filter(date >= \"2016-01-01\" & date < \"2017-01-01\") %>%\n  ggplot() +\n  geom_line(aes(x = date, y = count), col = \"black\", size = 0.5) +\n  scale_x_date(breaks=\"month\", date_labels =\"%Y-%m\") +\n  theme(axis.text.x = element_text(angle = 45, hjust=1)) +\n  ylab(\"News Articles (Ratio)\") +\n  xlab(\"Day\") +\n  ggtitle(\"MediaCloud Data on Elections (Monthly) - 2016\") \nggplot(elections_news) +\n  geom_line(aes(x = date, y = count), col = \"grey50\", size = 0.25) +\n  ylim(c(0, 15000)) +\n  # 1 EVENT\n  annotate(\"label\", x = as.Date(\"2018-11-01\"), y = 14500, \n           label = \"Midterm Elections\\nNovember 2018\", color = \"white\", fill=\"orange\", fontface=\"bold\", size=3) +\n  # add a line. You can also use an arrow by adding in geom_segment: \n  # arrow = line(length = unit(0.2, \"cm\"), ends = \"last\") \n  geom_segment(aes(x = as.Date(\"2018-11-01\"), xend = as.Date(\"2018-11-01\"), y = 0, yend = 14500), \n               color = \"orange\", size = 0.2, linetype = 1) +\n  # 2 EVENT\n  annotate(\"label\", x = as.Date(\"2019-05-01\"), y = 12000, \n           label = \"Pennsylvania Elections\\nMay 2019\", color = \"white\", fill=\"orange\", fontface=\"bold\", size=3) +\n  geom_segment(aes(x = as.Date(\"2019-05-01\"), xend = as.Date(\"2019-05-01\"), y = 0, yend = 12000), \n               color = \"orange\", size = 0.2, linetype = 1) +\n  # 3 EVENT\n  annotate(\"label\", x = as.Date(\"2016-11-01\"), y = 12000, \n           label = \"Presidential Elections\\nNovember 2016\", color = \"white\", fill=\"orange\", fontface=\"bold\", size=3) +\n  geom_segment(aes(x = as.Date(\"2016-11-01\"), xend = as.Date(\"2016-11-01\"), y = 0, yend = 12000), \n               color = \"orange\", size = 0.2, linetype = 1) +\n  # 4 EVENT\n  annotate(\"label\", x = as.Date(\"2020-11-01\"), y = 12000, \n           label = \"Presidential\\nElections\\nNovember\\n2020\", color = \"white\", fill=\"orange\", fontface=\"bold\", size=3) +\n  geom_segment(aes(x = as.Date(\"2020-11-01\"), xend = as.Date(\"2020-11-01\"), y = 0, yend = 12000), \n               color = \"orange\", size = 0.2, linetype = 1) +\n  ylab(\"News Articles\") +\n  xlab(\"Date\") +\n  labs(title = \"MediaCloud Data on Elections\",\n       subtitle = \"Peaks annotated with relevant political events\",\n       caption = \"Advanced Data Analysis\n                  University of Vienna\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        plot.subtitle = element_text(hjust = 0.5),\n        plot.caption = element_text(face = \"italic\")) +\n  theme_gray()## Warning in geom_segment(aes(x = as.Date(\"2018-11-01\"), xend = as.Date(\"2018-11-01\"), : All aesthetics have length 1, but the data has 2192 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing a\n##   single row.## Warning in geom_segment(aes(x = as.Date(\"2019-05-01\"), xend = as.Date(\"2019-05-01\"), : All aesthetics have length 1, but the data has 2192 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing a\n##   single row.## Warning in geom_segment(aes(x = as.Date(\"2016-11-01\"), xend = as.Date(\"2016-11-01\"), : All aesthetics have length 1, but the data has 2192 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing a\n##   single row.## Warning in geom_segment(aes(x = as.Date(\"2020-11-01\"), xend = as.Date(\"2020-11-01\"), : All aesthetics have length 1, but the data has 2192 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing a\n##   single row."},{"path":"correlations-and-arima.html","id":"correlations-and-arima","chapter":"8 Correlations and ARIMA","heading":"8 Correlations and ARIMA","text":"","code":""},{"path":"correlations-and-arima.html","id":"auto-correlation-acf-and-pacf","chapter":"8 Correlations and ARIMA","heading":"8.1 Auto-Correlation (ACF and PACF)","text":"previous chapter said time series said stationary :trend (systematic change mean, , time invariant mean), seasonality (periodic variations);change variance time (time invariant variance);auto-correlation (’ll return topic next chapters)Auto-correlation serial correlation important characteristic time series data can defined correlation variable different time points.Autocorrelation many consequences. prevents us use traditional statistical methods linear regression, assume observations independent . presence autocorrelation, estimated standard errors parameter estimates tend less true value. lead erroneously high statistical significance attributed statistical tests (p values smaller ).section introduce important tool diagnosis properties time series, including autocorrelation: correlogram. accurate study correlogram common step many time series analysis procedures.","code":""},{"path":"correlations-and-arima.html","id":"correlogram-acf-and-pacf","chapter":"8 Correlations and ARIMA","heading":"8.1.1 Correlogram: ACF and PACF","text":"correlogram chart presents one two statistics:autocorrelation function (ACF).ACF statistic measures correlation \\(x_t\\) \\(x_{t+k}\\) k number lead periods future. measures correlation two points based given interval. strictly equivalent Pearson product moment correlation. R, ACF calculated visualized function “acf”;partial autocorrelation function (PACF). PACF(k) measure correlation times series observations k units apart, correlation intermediate lags controlled “partialed” . words, PACF measures correlation \\(x_t\\) \\(x_{t+k}\\) stripped effect intermediate x’s. R, PACF calculated visualized function “pacf”. useful detect correlations evident ACF.Let’s consider, example, correlogram random walk process. know particular time series process current values combinations previous ones (\\(x_t = x_{t-1} + w_t\\), \\(x_{t-1}\\) value immediately x, \\(w_t\\) random component). resulting time series characterized discernible pattern time exactly predictable (stochastic trend). ACF random walk time series, indeed, shows correlation values series: even values close notwithstanding correlated.Instead, PACF, removes correlations intermediate values, shows correlation lag 1, , shows overall correlation depends consequent values. dotted blue lines signal boundaries statistical significance.can clearly visualize auto-correlation using simple scatterplot, plotting two consecutive lines points.ACF PACF white noise process different. know white noise stationary process, without distinguishable points time correlation points. Indeed, ACF white noise shows correlation (line statistical significance zero, nothing worried , since just means point correlated ).PACF can see nothing dotted line (means nothing statistically significant).plot two consecutive lists points using scatterplot, can see serial correlation (pattern visible):","code":"\nRandom_Walk <- arima.sim(n = 499, model = list(order = c(0,1,0)))\n\nacf(Random_Walk)\npacf(Random_Walk)\nplot(Random_Walk[1:499], Random_Walk[2:500])\nWhite_Noise <- arima.sim(n = 500, model = list(order = c(0,0,0)))\n\nacf(White_Noise)\npacf(White_Noise)\nplot(White_Noise[1:499], White_Noise[2:500])"},{"path":"correlations-and-arima.html","id":"arima-models","chapter":"8 Correlations and ARIMA","heading":"8.2 ARIMA models","text":"ACF PACF plots can used diagnose main characteristics time series find proper statistical model. talk univariate models, since models describe single time series. Univariate time series can modeled Auto Regressive (AR), Integrated (), Moving Average (MA) processes. models synthesized using acronym ARIMA. seasonal (S) component also taken account, also use acronym SARIMA.","code":""},{"path":"correlations-and-arima.html","id":"auto-regressive-ar-models","chapter":"8 Correlations and ARIMA","heading":"8.2.1 Auto Regressive (AR) models","text":"just said time series often characterized auto-correlation, can clearly deduce can model using regression model, , regressing time series past values. way auto-regressive model: regression \\(x_{t}\\) past terms \\(x_{t-k}\\) series.time series analysis, past terms \\(x_{t-k}\\) series called lags. lagged values time series delayed values, delay can arbitrary amount time \\(k\\). instance, considering simple series 4 data points distributed time \\({t+0}\\) (first data point) time \\({t+3}\\) (last data point) \\({x_{t+0}, x_{t+1}, x_{t+2}, x_{t+3}}\\), corresponding lagged series, assuming \\(k=1\\), \\({NA, x_{t+0}, x_{t+1}, x_{t+2}}\\). Notice first data point missing since data point behind , data points shifted one time point ahead.auto-regressive (AR) model can described follows (\\(\\alpha\\) coefficients, \\(t\\) time points, \\(w\\) random component white noise):\\[\n{x_t} = \\alpha x_{t-1} + \\alpha x_{t-2} + \\alpha x_{t-k} + {w_t}\n\\]ACF autoregressive process typically shows slow gradual decay autocorrelation time.PACF autoregressive process shows peak correspondence order model. case AR(1) peak time 1.case AR(3) peak time 1, 2, 3.Now can see random walk process seen particular case auto-regressive model. random walk process, value previous one plus random part:\\[\nx_t = x_{t-1} + w_t\n\\]Thus point \\(x_t\\) correlated previous one \\(x_{t-k}\\) lag value \\(k\\) equal 1 (\\({k=1}\\)). Therefore, random walk process auto-regressive model order 1, since just 1 lag taken consideration auto-regressive model (\\(\\alpha = 1\\)). order auto-regressive model indicated parenthesis, e.g.: AR(1).AR process can different characteristics (different ACF PACF) based parameters.","code":"\nAR_1 <- arima.sim(n = 500, list(order = c(1,0,0), ar = 0.90))\nplot(AR_1, main = \"AR(1)\")\nacf(AR_1)\npacf(AR_1)\nAR_3 <- arima.sim(n = 500, list(order = c(3,0,0), ar = c(0.3, 0.3, 0.3)))\nplot(AR_3, main = \"AR(3)\")\npacf(AR_3)"},{"path":"correlations-and-arima.html","id":"moving-average-ma-models","chapter":"8 Correlations and ARIMA","heading":"8.2.2 Moving Average (MA) models","text":"already know Moving Average method smooth time series detect trend. referring Moving Average process (MA), refer process values series function weighted average past errors. terms, moving average (MA) process linear combination current white noise term \\(q\\) recent past white noise terms:\\[\n{x_t} = w_t + \\beta w_{t-1} + ... + \\beta w_{t-q}\n\\]order MA process indicates lags white noise taken account model (e.g: MA(3)).ACF plot MA process shows clear cut-term corresponding order ot process. different ACF AR process, shows gradual decay.PACF MA process shows --movement shut , instead tapers toward 0 manner.","code":"\nMA_3 <- arima.sim(n = 500, list(order = c(0,0,3), ma = c(0.3, 0.3, 0.3)))\nplot(MA_3, main = \"MA(3)\")\nacf(MA_3)\npacf(MA_3)"},{"path":"correlations-and-arima.html","id":"integrated-i-process","chapter":"8 Correlations and ARIMA","heading":"8.2.3 Integrated (I) process","text":"integrated process non-stationary time series process becomes stationary transformed differencing. words, integrated process difference-stationary process, process stochastic trends (see previous chapter).","code":"\nI_1 <- arima.sim(n = 500, list(order = c(0,1,0)))\nplot(I_1, main = \"I(1)\")\nplot(diff(I_1), main = \"I(1) after 'differencing'\")"},{"path":"correlations-and-arima.html","id":"seasonal-s-models","chapter":"8 Correlations and ARIMA","heading":"8.2.4 Seasonal (S) models","text":"already introduced seasonal model. instance, dataset showing seasonal component AirPassengers. seasonality appears yearly fluctuations ACF spikes occurring 12 months PACF.","code":"\ndata(\"AirPassengers\")\n\n# this function par(mfrow=c(..., ...))\n# is used to combine more than one plot\n# in the same frame. The two numerical values\n# indicates number of rows and columns\n# the frame is made of\npar(mfrow=c(1,2))\n\nacf(AirPassengers, lag.max = 48)\npacf(AirPassengers, lag.max = 48)"},{"path":"correlations-and-arima.html","id":"fit-sarima-models","chapter":"8 Correlations and ARIMA","heading":"8.2.5 Fit (S)ARIMA models","text":"examples represent simple processes, real time series often result complex mixtures different types process, therefore complex identify appropriate model data.popular methods find appropriate model Box-Jenkins method, recursive process involving analysis time series, guess possible (S)ARIMA models, fit hypothesized models, meta-analysis determine best specification. best-fitting model \nfound, correlogram residuals verified white noise.Box-Jenkins method time-consuming requires expertise. ACF/PACF can also become difficult read case complex models, appropriate interpretation require lot expertise well. Fortunately, experts developed automated methods allow us automatically found fit ARIMA model. case auto.arima function implemented forecast package (package time series analysis especially forecasting, developed Rob J. Hyndman, professor statistics time series analysis expert).said , evaluate fit model analyze residuals, ascertain behave white noise. object resulting function auto.arima slot including residuals. ascertain residuals white noise can plot ACF PACF (spike significant) also histogram.forecast package also implements function checkresiduals create nice complete plots residual diagnostics using simple function.Besides creating plots, function calulate Ljung-Box test (default), Breusch-Godfrey test (specify test=“BG” inside function):Ljung-Box test (also Breusch–Godfrey test) diagnostic tool, applied residuals time series fitting ARIMA model, test lack fit. test examines autocorrelations residuals. significant autocorrelations, can concluded model exhibit significant lack fit. pass test, p-value significance level (usually 0.05)","code":"\nset.seed(7623)\narima_112 <- arima.sim(n = 500, list(order = c(1,1,2), ar = 0.8, ma = c(0.7, 0.2)))\nplot(arima_112, main = \"ARIMA(1,1,2)\")\n# Install the package if you haven't installed it yet\n# install.packages(\"forecast\")\nlibrary(forecast)\n\narima_fit <- auto.arima(arima_112)\n\narima_fit## Series: arima_112 \n## ARIMA(1,1,2) \n## \n## Coefficients:\n##          ar1     ma1     ma2\n##       0.7649  0.7387  0.2484\n## s.e.  0.0356  0.0528  0.0534\n## \n## sigma^2 = 1.035:  log likelihood = -717.9\n## AIC=1443.8   AICc=1443.88   BIC=1460.65\nlayout(matrix(c(1,2,3,3), nrow = 2))\nacf(arima_fit$residuals)\npacf(arima_fit$residuals)\nhist(arima_fit$residuals, main = \"Histogram of residuals\")\ncheckresiduals(arima_fit)## \n##  Ljung-Box test\n## \n## data:  Residuals from ARIMA(1,1,2)\n## Q* = 5.8977, df = 7, p-value = 0.5517\n## \n## Model df: 3.   Total lags used: 10"},{"path":"correlations-and-arima.html","id":"sarima","chapter":"8 Correlations and ARIMA","heading":"8.2.5.1 SARIMA","text":"fit model AirPassengers dataset, seasonal component, find Seasonal Autoregressive Integrated Moving Average model (SARIMA). seasonal component AirPassenger dataset evident plot series ACF PACF. forecast package useful function ggtsdisplay plot time series along ACF PACF.seasonal part ARIMA model consists terms similar non-seasonal components model, involves lagged values seasonal period.","code":"\nggtsdisplay(AirPassengers)\nAirPassengers_sarima <- auto.arima(window(AirPassengers))\nAirPassengers_sarima## Series: window(AirPassengers) \n## ARIMA(2,1,1)(0,1,0)[12] \n## \n## Coefficients:\n##          ar1     ar2      ma1\n##       0.5960  0.2143  -0.9819\n## s.e.  0.0888  0.0880   0.0292\n## \n## sigma^2 = 132.3:  log likelihood = -504.92\n## AIC=1017.85   AICc=1018.17   BIC=1029.35"},{"path":"correlations-and-arima.html","id":"forecasting","chapter":"8 Correlations and ARIMA","heading":"8.2.5.2 Forecasting","text":"Based ARIMA models found, can also try forecast future values. can use function forecast homonym library. instance, try forecast values AirPassenger dataset next four years.","code":"\nAirPassengers_forecast <- forecast(AirPassengers_sarima, h=48, level = 90)\nplot(AirPassengers_forecast, main = \"AirPassengers forecast\")\nAirPassengers_sarima <- auto.arima(AirPassengers)"},{"path":"correlations-and-arima.html","id":"cross-correlation","chapter":"8 Correlations and ARIMA","heading":"8.3 Cross-correlation","text":"Cross-correlation correlation (lagged) values time series values another series. Similarly ACF PACF, specific plot shows cross-correlation two time series, specific R function: ccf.cross-correlation can useful understand wich lagged values X series can used predict values Y series, thus used, instance, time series regression model.Unfourtunately, problem cross-correlation function , said preceding sections, autocorrelated data difficult assess dependence two processes, possible find spurious correlations.Thus, pertinent disentangle linear association X Y autocorrelation. useful device prewhitening. prewhitening method works follows:determine ARIMA time series model X-variable, store residuals model;fit ARIMA X-model Y-variable, keep residuals;examines CCF X Y model residuals.can implement procedure writing necessary code, using library forecast, also, alternatively, library TSA.make example, apply method two simulated two series. Y-variable created way correlated lagged values time \\(x_{t-3}\\) \\(x_{t-4}\\). Therefore, find correlation lags.cross-correlation applied original series results plot everything seems correlated. “real” correlation \\(x_{t-3}\\) \\(x_{t-4}\\) discernible .using forecast library, can calculate pre-withened ccf follows:Now, ’s clear X-variable correlated Y-variable \\(x_{t-3}\\) \\(x_{t-4}\\).previous steps show detail steps involved pre-whitening strategy, possible use original series prewhiten function TSA library. Although function can take, argument, pre-fitted ARIMA model, greater advantage can take care necessary steps prewithen series. particular, model specified, library automatically applies simple AR model. Although model can just approximation “true” model (can complex), approximation can enough pre-whiten series find proper cross-correlation (, also simpler approximate model can job).","code":"\nx_series <- arima.sim(n = 200, list(order = c(1,1,0), ar = 0.7, sd=1))\nz <- ts.intersect(x_series, stats::lag(x_series, -3), stats::lag(x_series, -4)) \ny_series <- 15 + 0.8*z[,2] + 1.5*z[,3] + rnorm(197,0,1)\nccf(x_series, y_series, na.action = na.omit)\n# fit an ARIMA model\nx_model <- auto.arima(x_series)\n# keep the residuals (\"white noise\")\nx_residuals <- x_model$residuals\n\n# fit the same ARIMA model to the Y-series\n# by using the \"Arima\" function in forecast\ny_model <- Arima(y_series, model=x_model)\n# keep the residuals\ny_filtered <- residuals(y_model)\n\n# apply the ccf to the residuals\nccf(x_residuals, y_filtered)\n# install.packages(\"TSA\")\nlibrary(TSA)\nprewhiten(x_series, y_series)"},{"path":"correlations-and-arima.html","id":"examples-in-literature","chapter":"8 Correlations and ARIMA","heading":"8.4 Examples in literature","text":"examples exemplify use ARIMA Cross-Correlation scientific literature, specific reference communication science.Scheufele, B., Haas, ., & Brosius, H. B. (2011). Mirror molder? study media coverage, stock prices, trading volumes Germany. Journal Communication, 61(1), 48-70, authors investigate “short-term relationship media coverage, stock prices, trading volumes eight listed German companies”, using ARIMA cross-correlation, particular asking:RQ2: cross-lagged correlations media coverage stock prices trading volumes differ according amount valence coverage?\nRQ3: cross-lagged correlations media coverage stock prices trading volumes differ according type media (Financial Web sites, daily newspapers, stock market TV shows) reports company stock?answer questions, authors made use time series analysis. particular, :estimated cross-lagged correlations media coverage stock prices trading volumes, respectively. Basically, two steps time-series analysis can distinguished: () first step, media time-series time-series trading volumes adjusted ARIMA (Autoregressive Integrated Moving Average) modeling separately. differences original time-series ARIMA model called residuals used analysis. Like ordinary least squares regression, residuals auto-correlated. residuals auto-correlated, time-series analysis speaks White Noise. modeling technique called prewhitening necessary avoid spurious correlations. (…) (b) next step, cross-correlations adjusted media time-series adjusted stock series calculated. (…) coefficient expresses strength correlation, whereas lags offer insight dynamics: Correlations positive (negative) lags indicate changes media coverage proceeded (succeeded) shifts stock prices trading volumes.Groshek, J. (2010). time-series, multinational analysis democratic forecasts Internet diffusion. International Journal Communication, 4, 33, author examines democratic effects Internet shown using macro- level, cross-national data sequence time–series statistical tests:study relies principally macro-level time–series democracy data historical sample includes 72 countries, reaching back far 1946 cases, least 1954 2003. sample, sequence ARIMA (autoregressive integrated moving average) time–series regressions modeled country least 40 years prior 1994. models used generate statistically-forecasted democracy values country, year 1994 2003. 95% confidence interval upper lower democracy score constructed around forecasted values using dynamic mean squared errors. actual democracy scores country year 1994 2003 compared upper lower values confidence interval.\nevent actual democracy level country greater upper value forecasted democracy score time period 1994 2003, Internet diffusion investigated case studies possible causal mechanism.terms, author used forecasting approach predict values series 1994 2003, order find statistically significant differences predicted actual values. discrepancies interpreted caused factors present past, possibly introduction Internet.study found , based results 72 countries reported , diffusion Internet considered democratic panacea, rather component contemporary democratization processes","code":""},{"path":"intervention-analysis.html","id":"intervention-analysis","chapter":"9 Intervention Analysis","heading":"9 Intervention Analysis","text":"chapter going learn intervention analysis (sometimes also called interrupted time-series analysis) see conduct intervention analysis.Intervention analysis typically conducted Box & Jenkins ARIMA framework traditionally uses method introduced Box Tiao (1975)1, provided framework assessing effect intervention time series study.summarized Box Tiao: Given known intervention, evidence change series kind expected actually occurred, , , can said nature magnitude change?. words Intervention analysis estimates effect external exogenous intervention time-series. conduct analysis, necessary know date intervention.Intervention analysis “quasi-experimental” design interesting approach test whether exogenous shocks, , instance, introduction new policy, impact time series process significant way, , changing mean function trend time series.Behind intervention analysis causal hypothesis observations treatment (“intervention”) different level slope intervention/interruption.Besides intervention interrupted time-series analysis, analysis can conducted segmented regression method. However, case traditional regression models applied time series data, approach take account autocorrelated structure time series. methods include complex computational approaches.","code":""},{"path":"intervention-analysis.html","id":"types-of-intervention","chapter":"9 Intervention Analysis","heading":"9.1 Types of intervention","text":"different types interventions. instance, intervention can abrupt impact determining permanent temporary change, sudden short-lived change due event, gradual yet permanent change.","code":""},{"path":"intervention-analysis.html","id":"intervention-analysis-with-arima","chapter":"9 Intervention Analysis","heading":"9.2 Intervention analysis with ARIMA","text":"exemplify intervention analysis going reproduce example paper Interrupted time series analysis using autoregressive integrated moving average (ARIMA) models: guide evaluating large-scale health interventions.data run analysis can downloaded .example evaulates impact health policy intervention (Australian health policy intervention restricted conditions particular medicine (quetiapine) subsidised). methodological process can applied evaluate intervention context.case study described follows:(…) due growing concerns inappropriate prescribing, January 1, 2014 new prescriptions tablet strength include refills. primary outcome number monthly dispensings 25 mg quetiapine, 48 months observations (January 2011 December 2014).Thus, data comprises 48 months observations, date intervention January 1, 2014.also seasonality process:Australia, medicine dispensing claims significant yearly seasonality. Medicines subsidised citizens eligible residents Pharmaceutical Benefits Scheme (PBS), people paying --pocket co-payment towards cost medicines, remainder subsidised. person’s (family’s) total --pocket costs reach “Safety Net threshold” calendar year, eligible reduced co-payment remainder year. Thus, incentive people reaching Safety Net refill medicines frequently towards end year. Hence, see increase prescriptions end year, followed decrease January.researchers hypothesize nature intervention follows (see picture ):(…) due nature intervention postulated immediate drop dispensings post-intervention (step change), well change slope (ramp). Thus, included variables representing types impacts model. impacts, h = 0 r = 0.sentence , h describes effect happens r represents decay pattern (see picture ).First load data, converting time series format, visualize time series along vertical lines representing date intervention.Next, create dummy variables representing intervention.\ncan tricky R. case, authors convert time ts object human-readable format .yearmon function (zoo function can use loading xts library).vectors dummy variable intervention. value equal zero date intervention, 1 .Next, specific case, also want create variable representing constant increasing change, capturing increasing effect intervention time.\nAlso case creation variable can little tricky. create two vectors using rep seq function, concatenate using c function.argument rep function two integers x times (rep(x, times)), function creates vectors repeat (“rep”) x values number times specified times. 36 months intervention, assign value zero.Instead, use seq function create vectors increasing values. part variable represent gradual increase intervention (12 months data intervention). function seq takes three arguments , , : starting end values sequence, increment sequence. case create sequence values increases 1 12 1.create variable need, concatenate function c function, follows:search appropriate ARIMA model data using auto.arima function (forecast package). include variables created external regressors.resulting model ARIMA(2,1,0)(0,1,1)[12].use information retrieved auto.arima function fit ARIMA model data, without including intervention (variables created), using just data date intervention (January 2014). , use window function order restrict set data consider, indicating December 2013 end series.Next, forecast 12 months didn’t include (starting January 2014 end period observation, December 2014) using forecast function (library forecast). logic behind operation see happened series absence intervention. words, use prediction counterfactual order describe possible effect intervention series, determining observed values diverge forecast.plotting data, can visualize predicted values absence intervention (red dashed line) well observed values (blue line). seems health policy considerably impacted analyzed prescriptions.Coming back initial ARIMA model including intervention variables, calculating also confidence intervals significance coefficients using coeftest confint function lmtest library, can quantify impact policy.estimated step change − 3285 dispensings (95% CI − 4465 − 2104) estimated change slope − 1397 dispensings per month (95% CI − 1606 − 1188). (figure, ndr) shows values predicted ARIMA model absence intervention (counterfactual) compared observed values. means change subsidy 25 mg quetiapine January 2014 associated immediate, sustained decrease 3285 dispensings, decrease 1397 dispensings every month. words, 4682 (3285 + 1397) fewer dispensings January 2014 predicted subsidy changes implemented. February 2014, 6079 fewer dispensings (3285 + 2*1397). Importantly, findings considered valid duration study period (.e. December 2014).","code":"\n# Load data\nquet <- read.csv(file = \"./data/12874_2021_1235_MOESM1_ESM.csv\")\n\n# Convert data to time series object\nquet.ts <- ts(quet[,2], frequency=12, start=c(2011, 1))\n\n# Plot data to visualize time series\nplot.ts(quet.ts, ylim=c(0, 40000), col = \"blue\", xlab = \"Month\", ylab = \"Dispensings\")\n# Add vertical line indicating date of intervention (January 1, 2014)\nabline(v=2014, col = \"gray\", lty = \"dashed\", lwd=2)\nlibrary(xts)\n# Create variable representing step change and view\nstep <- as.numeric(as.yearmon(time(quet.ts)) >= \"Jan 2014\")\nstep##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n## [42] 1 1 1 1 1 1 1\nrep(0, 36)##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nseq(from = 1, to = 12, by = 1)##  [1]  1  2  3  4  5  6  7  8  9 10 11 12\n# Create variable representing ramp (change in slope) and view\nramp <- c(rep(0, 36), seq(1, 12, 1))\nramp ##  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n## [28]  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12\nlibrary(forecast)\n\n# Use automated algorithm to identify parameters\nmodel1 <- auto.arima(quet.ts, xreg = cbind(step, ramp), stepwise=FALSE)\n\n# Check residuals\ncheckresiduals(model1)## \n##  Ljung-Box test\n## \n## data:  Residuals from Regression with ARIMA(2,1,0)(0,1,1)[12] errors\n## Q* = 9.5692, df = 7, p-value = 0.2143\n## \n## Model df: 3.   Total lags used: 10\nmodel1## Series: quet.ts \n## Regression with ARIMA(2,1,0)(0,1,1)[12] errors \n## \n## Coefficients:\n##          ar1      ar2     sma1        step        ramp\n##       -0.873  -0.6731  -0.6069  -3284.7792  -1396.6523\n## s.e.   0.124   0.1259   0.3872    602.3362    106.6329\n## \n## sigma^2 = 648828:  log likelihood = -284.45\n## AIC=580.89   AICc=583.89   BIC=590.23\n# To forecast the counterfactual, model data excluding post-intervention time period\nmodel2 <- Arima(window(quet.ts, end = c(2013, 12)), order = c(2, 1, 0), \n                seasonal = list(order = c(0, 1, 1), period = 12))\n# Forecast 12 months post-intervention and convert to time series object\nfc <- forecast(model2, h = 12)\n\n# covert the average forecast (fc$mean) in a time series object\nfc.ts <- ts(as.numeric(fc$mean), start=c(2014, 1), frequency = 12)\n\n# Combine the observed and the forecast data\nquet.ts.2 <- ts.union(quet.ts, fc.ts)\nquet.ts.2##          quet.ts    fc.ts\n## Jan 2011   16831       NA\n## Feb 2011   17234       NA\n## Mar 2011   20546       NA\n## Apr 2011   19226       NA\n## May 2011   21136       NA\n## Jun 2011   20939       NA\n## Jul 2011   21103       NA\n## Aug 2011   22897       NA\n## Sep 2011   22162       NA\n## Oct 2011   22184       NA\n## Nov 2011   23108       NA\n## Dec 2011   25967       NA\n## Jan 2012   20123       NA\n## Feb 2012   21715       NA\n## Mar 2012   24497       NA\n## Apr 2012   21720       NA\n## May 2012   25053       NA\n## Jun 2012   23915       NA\n## Jul 2012   24972       NA\n## Aug 2012   26183       NA\n## Sep 2012   24163       NA\n## Oct 2012   26172       NA\n## Nov 2012   26642       NA\n## Dec 2012   29086       NA\n## Jan 2013   24002       NA\n## Feb 2013   24190       NA\n## Mar 2013   26052       NA\n## Apr 2013   26707       NA\n## May 2013   29077       NA\n## Jun 2013   26927       NA\n## Jul 2013   30300       NA\n## Aug 2013   29854       NA\n## Sep 2013   28824       NA\n## Oct 2013   31519       NA\n## Nov 2013   32084       NA\n## Dec 2013   33160       NA\n## Jan 2014   24827 29127.50\n## Feb 2014   23285 29671.28\n## Mar 2014   23884 31156.37\n## Apr 2014   21921 31339.65\n## May 2014   22715 33843.48\n## Jun 2014   19919 31809.61\n## Jul 2014   20560 34498.50\n## Aug 2014   18961 34774.18\n## Sep 2014   18780 33302.09\n## Oct 2014   17998 35641.85\n## Nov 2014   16624 36184.57\n## Dec 2014   18450 37792.03\n# Plot\nplot.ts(quet.ts.2, plot.type = \"single\", \n     col=c('blue','red'), xlab=\"Month\", ylab=\"Dispensings\", \n     lty=c(\"solid\", \"dashed\"), ylim=c(0,40000))\n\nabline(v=2014, lty=\"dashed\", col=\"gray\")\nlibrary(lmtest)\n\nmodel1## Series: quet.ts \n## Regression with ARIMA(2,1,0)(0,1,1)[12] errors \n## \n## Coefficients:\n##          ar1      ar2     sma1        step        ramp\n##       -0.873  -0.6731  -0.6069  -3284.7792  -1396.6523\n## s.e.   0.124   0.1259   0.3872    602.3362    106.6329\n## \n## sigma^2 = 648828:  log likelihood = -284.45\n## AIC=580.89   AICc=583.89   BIC=590.23\ncoeftest(model1)## \n## z test of coefficients:\n## \n##         Estimate  Std. Error  z value  Pr(>|z|)    \n## ar1     -0.87301     0.12396  -7.0427 1.885e-12 ***\n## ar2     -0.67314     0.12587  -5.3480 8.893e-08 ***\n## sma1    -0.60694     0.38722  -1.5674     0.117    \n## step -3284.77920   602.33616  -5.4534 4.942e-08 ***\n## ramp -1396.65226   106.63287 -13.0978 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nconfint(model1)##              2.5 %        97.5 %\n## ar1     -1.1159671    -0.6300565\n## ar2     -0.9198339    -0.4264433\n## sma1    -1.3658815     0.1520046\n## step -4465.3363731 -2104.2220250\n## ramp -1605.6488440 -1187.6556789"},{"path":"interrupted-time-series-analysis-using-segmented-regression.html","id":"interrupted-time-series-analysis-using-segmented-regression","chapter":"10 Interrupted time series analysis using segmented regression","heading":"10 Interrupted time series analysis using segmented regression","text":"Segmented regression another common way analyzing impact intervention. Two good papers explaining methods segmented regression , example:Bernal, J. L., Cummins, S., & Gasparrini, . (2017). Interrupted time series regression evaluation public health interventions: tutorial. International journal epidemiology, 46(1), 348-355.Wagner, . K., Soumerai, S. B., Zhang, F., & Ross‐Degnan, D. (2002). Segmented regression analysis interrupted time series studies medication use research. Journal clinical pharmacy therapeutics, 27(4), 299-309.explained latter, “Segmented regression analysis uses statistical models estimate level trend pre-intervention segment changes level trend intervention (interventions).”.exactly, segmented regression model structured follow:\\[Y = b_0 + b_1Time + b_2Intervention + b_3TimeSinceIntervention + e\\]includes least:outcome variable (Y);variable indicates time 1,2,…,t passed start series;dummy variable (0/1) observation collected (0) (1) intervention;variable measuring time 1,2,…,t passed since intervention occured, equal zero intervention.interpretation coefficients follows:\\(b_0\\) baseline level Time 0;Time (\\(b_1\\)) coefficient indicates trend (slope) intervention ( change outcome associated time unit increase).Intervention (\\(b_2\\)) coefficient indicates immediate effect (level change) induced intervention (last observation intervention first one ).Time Since Intervention (\\(b_3\\)) coefficient indicates “sustained effect”, .e., change trend intervention (effect time point passes intervention). measures difference slope line intervention. also possible calculate slope line intervention summing coefficients Time Time Since Treatment (\\(b_1 + b_3\\))good tutorial tecnique can found following link: https://ds4ps.org/pe4ps-textbook/docs/p-020-time-series.html.","code":""},{"path":"var.html","id":"var","chapter":"11 VAR","heading":"11 VAR","text":"VAR acronym stands Vector Autoregressive Model. common method analysis multivariate time series.can conceived way model system time series. VAR model, rigid distinction independent dependent variables, variable dependent independent. Besides endogenous variables dynamically interact, VAR model can include exogenous variables. Exogenous variables can impact endogenous variables, opposite true. make simple example, time series fans sold month may influenced quantity fans produced distributed industry, monthly temperature. purchase production fans can interact (endogenous variables), weather impacted processes, just impact external force (exogenous variable).make another example (paper Impact Politicization Health Online Misinformation Quality Information Vaccines), political debate certain topic - instance political debate led promulgation law mandatory vaccinations Italy - considered exogenous variable impacts news coverage topic spread problematic information Twitter. news media coverage Twitter discussions can considered part communication system dependent (news media set discussion agenda Twitter, also social media can stimulate news media coverage) assumed political debate led promulgation law mandatory vaccinations independent Twitter discussions topic.Stationary tests usually applied ascertain variables integrated (“” ARIMA model). case, variables differenced starting VAR analysis. preliminary analysis (instance testing cointegration) pre-processing can performed analysis. Next, number lags used selected. number can automatically identify automated methods (lag-length selection criteria methods). model evaluated. Results VAR model usually complicated, researchers relies statistical methods Granger causality test. Granger causality test, test developed nobel prize winner Clive Granger, applied study agenda setting processes. variable \\(X\\) said “Granger cause” another variable \\(Y\\) \\(Y\\) can better predicted past values \\(X\\) \\(Y\\) together past \\(Y\\) alone.Granger causality test statistical hypothesis test determining whether one time series useful forecasting another, first proposed 1969. Ordinarily, regressions reflect “mere” correlations, Clive Granger argued causality (…) tested measuring ability predict future values time series using prior values another time series. Since question “true causality” deeply philosophical, post hoc ergo propter hoc fallacy assuming one thing preceding another can used proof causation, (…) Granger test finds “predictive causality”.R package perform VAR modeling vars.Practical applications VAR modeling, including Granger causality, can found, instance, paper Assembling Networks Audiences Disinformation: Successful Russian IRA Twitter Accounts Built Followings,2015–2017 \nCoordinating Multi-Platform Disinformation Campaign: Internet Research Agency Activity Three U.S. Social Media Platforms, 2015 2017 link.","code":""},{"path":"var.html","id":"var-modeling-hands-on-tutorial","chapter":"11 VAR","heading":"11.1 VAR modeling hands-on tutorial","text":"","code":""},{"path":"var.html","id":"assumption-of-stationarity","chapter":"11 VAR","heading":"11.1.1 Assumption of stationarity","text":"Like time series techniques, VAR assumes series stationary. recap:stationary time series one whose properties depend time series observed. Thus, time series trends, seasonality, stationary — trend seasonality affect value time series different times. hand, white noise series stationary — matter observe , look much point time. […] general, stationary time series predictable patterns long-term. Time plots show series roughly horizontal (although cyclic behaviour possible), constant variance. Rob J. Hyndman George Athanasopoulos, “Forecasting: Principles Practice”.examples stationary non-stationary series.already learnt, pre-processing steps usually carried make stationary series stationary. instance, series linear trend can made stationary removing trend. seasonal series can seasonally adjusted. random-walk-like series can adjusted using differencing. Visual inspection statistical tests can performed support findings characteristics time series.pre-processing steps may unnecessary using vars library, function VAR (used fit VAR models) allows include trend seasonality components.","code":"\npar(mar=c(0,0,1,0))\nlayout(matrix(c(1,1,2,3,4,5), ncol = 2, nrow = 3, byrow = T))\n\nwn <- rnorm(n=1000)\n\nplot(ts(wn), main = \"Stationary Time Series (White Noise)\")\nabline(h = 0, col = \"red\", lwd = 2)\n\nnonvariance_stat <- rnorm(n=1000) * 1:1000\n\nplot(ts(nonvariance_stat), \n     main = \"Non stationary in variance\")\n\nlinear_trend <- rnorm(n=1000) + 0.05*1:1000\n\nplot(ts(linear_trend), \n     main = \"Non stationary in mean (Linear Trend)\")\n\nseasonality <- rnorm(n=24*30) + 0.2*rep(1:24, 30)\n\nplot(ts(seasonality), \n     main = \"Non stationary in mean (Seasonality)\")\n\nrandom_walk <- cumsum(sample(c(-100, 100), 1000, TRUE)) + rnorm(1000, sd = 44)\n\nplot(ts(random_walk), \n     main = \"Non stationary in mean (Random Walk)\")\npar(mar=c(0,0,1,0))\nlayout(matrix(c(1,1,2,3,4,5,6,7), ncol = 2, nrow = 4, byrow = T))\n\nplot(ts(wn), main = \"Stationary Time Series (White Noise)\")\nabline(h = 0, col = \"red\", lwd = 2)\n\nplot(ts(random_walk), \n     main = \"Non stationary in mean (Random Walk)\")\n\nplot(diff(ts(random_walk)), \n     main = \"Differenced\")\n\nplot(ts(linear_trend), \n     main = \"Non stationary in mean (Linear Trend)\")\n\nplot(ts(residuals(lm(linear_trend ~ seq(1, length(linear_trend), 1)))), \n     main = \"Detrended\")\n\nplot(ts(seasonality), \n     main = \"Non stationary in mean (Seasonality)\")\n\ndecomposed <- decompose(ts(seasonality, frequency = 24))\ndeseasonalized <- seasonality - decomposed$seasonal\n\nplot(ts(deseasonalized), \n     main = \"Deseasonalized\")"},{"path":"var.html","id":"other-assumptions-of-var-models-distribution-of-residuals","chapter":"11 VAR","heading":"11.1.2 Other assumptions of VAR models: distribution of residuals","text":"Besides stationarity linearity variables lagged values, VAR performed presence structural breaks. Structural breaks abrupt changes series’ mean overall process. can deal structural breaks different ways. example, might want split series different phases, using structural breaks breakpoints.crucial assumptions VAR residuals, required :Non serially correlatedNormal distributedHomoskedasticStatistical tests included vars package can used check assumptions.Another fundamental assumption VAR models absence cointegration series. Two series may cointegrated integrated order. Integrated series achieve stationarity differentiated. number differentiation needed achieve stationarity order integration (usually, one: (1) notation).Integrated series non-stationary, can happen linear combination integrated series stationary. case, said cointegrated. Although series follows seemingly random path, time, characterized equilibrium, driven common underlying process. Cointegration explained help story drunk dog:fact, drunk creature whose behavior follow random walk. Puppies, , wander aimlessly unleashed. new scent crosses puppy’s nose dictates direction pup’s next step. […] dog belongs drunk? drunk sets bar, wander aimlessly random-walk fashion. periodically intones “Oliver, ?”, Oliver interrupts aimless wandering bark. hears ; hears . thinks: “Oh can’t let get far ; ’ll lock ”. thinks, “Oh, can’t let get far ; ’ll wake middle night barking”. assessed far away moves partially close gap. Now neither drunk dog follows random walk; added formally call error-correction-mechanism steps. […] paths drunk dog still non-stationary. Significantly, despite nonstationarity paths, one might still say, “find , dog unlikely far away”. right, distance two paths stationary, walks woman dog said cointegrated (…). […] Notice cointegration probabilistic concept. dog leash, enforce fixed distance drunk dog. distance drunk dog instead random variable. stationary one, despite nonstationarity two paths. Murray, M. P. (1994). drunk dog: illustration cointegration error correction. American Statistician, 48(1), 37-39.Statistical tests used determine whether cointegration present. performed series integrated order integration (, cointegration can exists least two series integrated order). , cointegration tests performed. presence cointegration, Vector Error Correction Models (VECM) used instead VAR. VECM basically VAR model “error correction term”. topic (yet) covered tutorial.","code":""},{"path":"var.html","id":"var-fitting","chapter":"11 VAR","heading":"11.2 VAR fitting","text":"Let’s make simple example.\nUpload data:Let’s visually inspect series:variance look constant, can tentatively log-transform series work log-transformed data.check stationarity, can perform test Augmented Dickey-Fuller Test. Integration suggested “anti” series.series becomes stationary differencing.can recreate dataset including differentiated series. Using first difference transformation, lose first data point.Indeed, first point differentiated series obtained subtracting first point second one. second point differentiated series obtained subtracting second point third one, . Hence, differentiated dataset start second point time, first one original series. Therefore, need drop first data point dataset.fit VAR model use library vars.VAR models require researcher specify number lags include model. Roughly speaking, lags auto-regressive predictors. example, considering VAR model two variables \\(X\\) \\(Y\\), using 1 lag means use value \\(X_{t-1}\\) \\(Y_{t-1}\\) predict \\(X_t\\) \\(Y_t\\), 2 lags use \\(X_{t-1}\\), \\(X_{t-2}\\), \\(Y_{t-1}\\), \\(Y_{t-2}\\). choice lags important. function VARselect implements four different statistical tests find “best” number lags.case, criteria agree one lag.function fit model VAR, number lags specified using p parameter.can also include trend /seasonality, case doesn’t seem useful. also possible include exogenous variables. Exogenous variables usually defined variables external system: may affect system, affected system.VARselect function, use columns containing data (first column dataset contains dates).model looks like . can see, system linear regression models includes lagged predictors. analyzing results need confirm assumptions models met.serial.test test serially correlated errors. test okay significant. test significant.normality.test performs tests normality residuals. tests okay significant, like case.Another fundamental test performed function stability. test okay series stay within red bars. okay.arch.test assesses null hypothesis series residuals exhibits heteroscedasticity. Also test significant, good.assumptions met, can take look overall fit. doesn’t look amazing, assumptions met, let’s assume good enough let’s go ahead analysis.--good fit probably related fact variables VAR system highly correlated. coefficients statistically significant overall R-square small.Generally, coefficient table interpreted. Instead, VAR models interpreted using tools, Granger causality test.","code":"##             date_time anti   pro\n## 1 2021-03-12 23:59:00   37 14085\n## 2 2021-03-13 23:59:00  157  5219\n## 3 2021-03-14 23:59:00   40  5106\n## 4 2021-03-15 23:59:00   25  6898\n## 5 2021-03-16 23:59:00   24  2974\n## 6 2021-03-17 23:59:00   61  3230\nplot.ts(ts_tweets[, c(\"pro\", \"anti\")], \n        plot.type = \"multiple\",\n        main = \"tweets\")\nts_tweets$pro <- log(ts_tweets$pro)\nts_tweets$anti <- log(ts_tweets$anti)\n\nplot.ts(ts_tweets[, c(\"pro\", \"anti\")], \n        plot.type = \"multiple\",\n        main = \"tweets\")\ntseries::adf.test(ts_tweets$pro)## \n##  Augmented Dickey-Fuller Test\n## \n## data:  ts_tweets$pro\n## Dickey-Fuller = -3.9314, Lag order = 3, p-value = 0.02193\n## alternative hypothesis: stationary\ntseries::adf.test(ts_tweets$anti)## \n##  Augmented Dickey-Fuller Test\n## \n## data:  ts_tweets$anti\n## Dickey-Fuller = -2.101, Lag order = 3, p-value = 0.5338\n## alternative hypothesis: stationary\nanti <- diff(ts_tweets$anti, 1)\ntseries::adf.test(anti)## Warning in tseries::adf.test(anti): p-value smaller than printed p-value## \n##  Augmented Dickey-Fuller Test\n## \n## data:  anti\n## Dickey-Fuller = -4.7232, Lag order = 3, p-value = 0.01\n## alternative hypothesis: stationary\nlength(anti)## [1] 41\nlength(ts_tweets$pro)## [1] 42\n# drop the first data point\nts_tweets <- ts_tweets[-1,]\n\n# replance the anti series with the differentiated anti series\nts_tweets$anti <- anti\n# install.packages(\"vars\")\nlibrary(vars)\nvars::VARselect(ts_tweets[,c(\"pro\", \"anti\")])## $selection\n## AIC(n)  HQ(n)  SC(n) FPE(n) \n##      1      1      1      1 \n## \n## $criteria\n##                  1           2           3           4          5           6\n## AIC(n) -2.92630038 -2.81403228 -2.64199556 -2.42923055 -2.5769659 -2.77801920\n## HQ(n)  -2.83582733 -2.66324387 -2.43089178 -2.15781141 -2.2452314 -2.38596933\n## SC(n)  -2.64875447 -2.35145577 -1.99438843 -1.59659281 -1.5592975 -1.57532025\n## FPE(n)  0.05366012  0.06030449  0.07235601  0.09118664  0.0810697  0.06940455\n##                  7           8          9         10\n## AIC(n) -2.68597480 -2.68018280 -2.5499629 -2.4424493\n## HQ(n)  -2.23360957 -2.16750221 -1.9769669 -1.8091380\n## SC(n)  -1.29824525 -1.10742264 -0.7921721 -0.4996280\n## FPE(n)  0.08132081  0.08986526  0.1167941  0.1564827\nvar_fit <- vars::VAR(ts_tweets[,c(\"pro\", \"anti\")], p = 1)\nsummary(var_fit)## \n## VAR Estimation Results:\n## ========================= \n## Endogenous variables: pro, anti \n## Deterministic variables: const \n## Sample size: 40 \n## Log Likelihood: -53.79 \n## Roots of the characteristic polynomial:\n## 0.4477 0.2503\n## Call:\n## vars::VAR(y = ts_tweets[, c(\"pro\", \"anti\")], p = 1)\n## \n## \n## Estimation results for equation pro: \n## ==================================== \n## pro = pro.l1 + anti.l1 + const \n## \n##         Estimate Std. Error t value Pr(>|t|)    \n## pro.l1   0.25131    0.16894   1.488    0.145    \n## anti.l1 -0.05992    0.06722  -0.891    0.379    \n## const    6.27985    1.41950   4.424 8.22e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Residual standard error: 0.3364 on 37 degrees of freedom\n## Multiple R-Squared: 0.06146, Adjusted R-squared: 0.01073 \n## F-statistic: 1.212 on 2 and 37 DF,  p-value: 0.3093 \n## \n## \n## Estimation results for equation anti: \n## ===================================== \n## anti = pro.l1 + anti.l1 + const \n## \n##         Estimate Std. Error t value Pr(>|t|)   \n## pro.l1   0.01161    0.38475   0.030  0.97610   \n## anti.l1 -0.44871    0.15309  -2.931  0.00576 **\n## const   -0.10750    3.23284  -0.033  0.97365   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Residual standard error: 0.7661 on 37 degrees of freedom\n## Multiple R-Squared: 0.2029,  Adjusted R-squared: 0.1598 \n## F-statistic: 4.709 on 2 and 37 DF,  p-value: 0.01507 \n## \n## \n## \n## Covariance matrix of residuals:\n##         pro   anti\n## pro  0.1132 0.0862\n## anti 0.0862 0.5870\n## \n## Correlation matrix of residuals:\n##         pro   anti\n## pro  1.0000 0.3345\n## anti 0.3345 1.0000\nvars::serial.test(var_fit)## \n##  Portmanteau Test (asymptotic)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 33.632, df = 60, p-value = 0.9977\nvars::normality.test(var_fit)## $JB\n## \n##  JB-Test (multivariate)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 3.9951, df = 4, p-value = 0.4067\n## \n## \n## $Skewness\n## \n##  Skewness only (multivariate)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 1.8704, df = 2, p-value = 0.3925\n## \n## \n## $Kurtosis\n## \n##  Kurtosis only (multivariate)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 2.1247, df = 2, p-value = 0.3456\nplot(vars::stability(var_fit))\nvars::arch.test(var_fit)## \n##  ARCH (multivariate)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 44.188, df = 45, p-value = 0.5062\npar(mar=c(0,0,0,0))\nplot(var_fit)\nsummary(var_fit)## \n## VAR Estimation Results:\n## ========================= \n## Endogenous variables: pro, anti \n## Deterministic variables: const \n## Sample size: 40 \n## Log Likelihood: -53.79 \n## Roots of the characteristic polynomial:\n## 0.4477 0.2503\n## Call:\n## vars::VAR(y = ts_tweets[, c(\"pro\", \"anti\")], p = 1)\n## \n## \n## Estimation results for equation pro: \n## ==================================== \n## pro = pro.l1 + anti.l1 + const \n## \n##         Estimate Std. Error t value Pr(>|t|)    \n## pro.l1   0.25131    0.16894   1.488    0.145    \n## anti.l1 -0.05992    0.06722  -0.891    0.379    \n## const    6.27985    1.41950   4.424 8.22e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Residual standard error: 0.3364 on 37 degrees of freedom\n## Multiple R-Squared: 0.06146, Adjusted R-squared: 0.01073 \n## F-statistic: 1.212 on 2 and 37 DF,  p-value: 0.3093 \n## \n## \n## Estimation results for equation anti: \n## ===================================== \n## anti = pro.l1 + anti.l1 + const \n## \n##         Estimate Std. Error t value Pr(>|t|)   \n## pro.l1   0.01161    0.38475   0.030  0.97610   \n## anti.l1 -0.44871    0.15309  -2.931  0.00576 **\n## const   -0.10750    3.23284  -0.033  0.97365   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Residual standard error: 0.7661 on 37 degrees of freedom\n## Multiple R-Squared: 0.2029,  Adjusted R-squared: 0.1598 \n## F-statistic: 4.709 on 2 and 37 DF,  p-value: 0.01507 \n## \n## \n## \n## Covariance matrix of residuals:\n##         pro   anti\n## pro  0.1132 0.0862\n## anti 0.0862 0.5870\n## \n## Correlation matrix of residuals:\n##         pro   anti\n## pro  1.0000 0.3345\n## anti 0.3345 1.0000"},{"path":"var.html","id":"granger-causality-test","chapter":"11 VAR","heading":"11.3 Granger causality test","text":"use causality function perform Granger causality test. Granger causality test basic inferential tool available VAR analysis. time series X considered Granger cause another time series Y past values X Y predicts Y significantly better past values Y alone. analysis frequently performed communication studies focused agenda setting phenomena.Let’s see ’s Granger causality series. researcher needs specify “cause” variable (sometimes, researcher may hypothesis ). case, try . granger causality effect contemporaneous relationship (instant causation).","code":"\nvars::causality(var_fit, cause = \"pro\")## $Granger\n## \n##  Granger causality H0: pro do not Granger-cause anti\n## \n## data:  VAR object var_fit\n## F-Test = 0.00090982, df1 = 1, df2 = 74, p-value = 0.976\n## \n## \n## $Instant\n## \n##  H0: No instantaneous causality between: pro and anti\n## \n## data:  VAR object var_fit\n## Chi-squared = 4.0242, df = 1, p-value = 0.04485\nvars::causality(var_fit, cause = \"anti\")## $Granger\n## \n##  Granger causality H0: anti do not Granger-cause pro\n## \n## data:  VAR object var_fit\n## F-Test = 0.79448, df1 = 1, df2 = 74, p-value = 0.3756\n## \n## \n## $Instant\n## \n##  H0: No instantaneous causality between: anti and pro\n## \n## data:  VAR object var_fit\n## Chi-squared = 4.0242, df = 1, p-value = 0.04485"},{"path":"readings-and-bibliographical-references.html","id":"readings-and-bibliographical-references","chapter":"12 Readings and Bibliographical References","heading":"12 Readings and Bibliographical References","text":"","code":""},{"path":"readings-and-bibliographical-references.html","id":"mandatory","chapter":"12 Readings and Bibliographical References","heading":"12.1 Mandatory","text":"Shin, Y. (2017). Time series analysis social sciences: fundamentals. Univ California Press.","code":""},{"path":"readings-and-bibliographical-references.html","id":"other-readings","chapter":"12 Readings and Bibliographical References","heading":"12.2 Other readings","text":"Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, ., & Yang, J. (2019). Temporal Turn Communication Research: Time Series Analyses Using Computational Approaches. International Journal Communication (19328036), 13.Brodersen KH, Gallusser F, Koehler J, Remy N, Scott SL. Inferring causal impact using Bayesian structural time-series models. Annals Applied Statistics, 2015, Vol. 9, . 1, 247-274.Gaubatz, K. T. (2014). Survivor’s Guide R: Introduction Uninitiated Unnerved. SAGE Publications.Liboschik, T., Fokianos, K., & Fried, R. (2017). tscount: R package analysis count time series following generalized linear models. Journal Statistical Software, 82(1), 1-51.Schaffer, . L., Dobbins, T. ., & Pearson, S. . (2021). Interrupted time series analysis using autoregressive integrated moving average (ARIMA) models: guide evaluating large-scale health interventions. BMC medical research methodology, 21(1), 1-12.Zivot E., Wang J. (2003) Unit Root Tests. : Modeling Financial Time Series S-Plus®. Springer, New York, NY. https://doi.org/10.1007/978-0-387-21763-5_4","code":""},{"path":"readings-and-bibliographical-references.html","id":"useful-resources","chapter":"12 Readings and Bibliographical References","heading":"12.3 Useful resources","text":"Cross Validated statistics-related questionsStackoverflow coding-related questions","code":""}]
