<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>9 Regression | Time Series Analysis With R</title>
<meta name="author" content="Nicola Righetti">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="9 Regression | Time Series Analysis With R">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="9 Regression | Time Series Analysis With R">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.8.0/transition.js"></script><script src="libs/bs3compat-0.8.0/tabs.js"></script><script src="libs/bs3compat-0.8.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style.css">
<meta name="description" content="In this chapter we are going to see how to conduct a regression analysis with time series data. Regression analysis is a used for estimating the relationships between a dependent variable (DV)...">
<meta property="og:description" content="In this chapter we are going to see how to conduct a regression analysis with time series data. Regression analysis is a used for estimating the relationships between a dependent variable (DV)...">
<meta name="twitter:description" content="In this chapter we are going to see how to conduct a regression analysis with time series data. Regression analysis is a used for estimating the relationships between a dependent variable (DV)...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Time Series Analysis With R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Time Series Analysis With R</a></li>
<li><a class="" href="getting-started-with-r.html"><span class="header-section-number">2</span> Getting started with R</a></li>
<li><a class="" href="basic-data-wrangling-with-tidyverse.html"><span class="header-section-number">3</span> Basic Data Wrangling with Tidyverse</a></li>
<li><a class="" href="basic-concepts.html"><span class="header-section-number">4</span> Basic Concepts</a></li>
<li><a class="" href="time-series-objects.html"><span class="header-section-number">5</span> Time Series Objects</a></li>
<li><a class="" href="plot-time-series.html"><span class="header-section-number">6</span> Plot Time Series</a></li>
<li><a class="" href="structural-decomposition.html"><span class="header-section-number">7</span> Structural Decomposition</a></li>
<li><a class="" href="correlations-and-arima.html"><span class="header-section-number">8</span> Correlations and ARIMA</a></li>
<li><a class="active" href="regression.html"><span class="header-section-number">9</span> Regression</a></li>
<li><a class="" href="intervention-analysis.html"><span class="header-section-number">10</span> Intervention Analysis</a></li>
<li><a class="" href="interrupted-time-series-analysis-using-segmented-regression.html"><span class="header-section-number">11</span> Interrupted time series analysis using segmented regression</a></li>
<li><a class="" href="var.html"><span class="header-section-number">12</span> VAR</a></li>
<li><a class="" href="readings-and-bibliographical-references.html"><span class="header-section-number">13</span> Readings and Bibliographical References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="regression" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Regression<a class="anchor" aria-label="anchor" href="#regression"><i class="fas fa-link"></i></a>
</h1>
<p>In this chapter we are going to see how to conduct a regression analysis with time series data.</p>
<p><em>Regression analysis</em> is a used for estimating the relationships between a <em>dependent variable (DV)</em> (also called <em>outcome</em> or <em>response</em>) and one or more <em>independent variables (IV)</em> (also called <em>predictors</em> or <em>explanatory variables</em>).</p>
<p>A standard regression model <span class="math inline">\(Y\)</span> = <span class="math inline">\(\beta\)</span> + <span class="math inline">\(\beta x\)</span> + <span class="math inline">\(\epsilon\)</span> has no time component. Differently, a time series regression model includes a time dimension and can be written, in a simple and general formulation, using just one explanatory variable, as follows:</p>
<p><span class="math display">\[
y_t = \beta_0 + \beta_1x_t + \epsilon_t
\]</span></p>
<p>In this equation, <span class="math inline">\(y_t\)</span> is the time series we try to understand/predict (the <em>dependent variable (DV)</em>), <span class="math inline">\(\beta_0\)</span> is the <em>intercept</em> (a constant value that represents the expected mean value of <span class="math inline">\(y_t\)</span> when <span class="math inline">\(x_t = 0\)</span>), the coefficient <span class="math inline">\(\beta_1\)</span> is the <em>slope</em>, representing the average change in <span class="math inline">\(y\)</span> at one unit increase in <span class="math inline">\(x\)</span> (the <em>independent variable (IV) or explanatory variable</em>), and <span class="math inline">\(\epsilon_t\)</span> is the time series of residuals (the error term).</p>
<p>A multiple regression, with more than one explanatory variable, can be written as follows:</p>
<p><span class="math display">\[
y_t = \beta_0 + \beta_1x_{1,t} + \beta_2x_{2,t} + ... + \beta_kx_{k,t} + \epsilon_t
\]</span></p>
<div id="static-and-dynamic-models" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Static and Dynamic Models<a class="anchor" aria-label="anchor" href="#static-and-dynamic-models"><i class="fas fa-link"></i></a>
</h2>
<p>From a time series analysis perspective, a general distinction can be made between “static” and “dynamic” regression models:</p>
<ul>
<li>A <strong>static regression model</strong> includes just contemporary relations between the explanatory variables (independent variables) and the response (dependent variable). This model could be appropriate when the expected value of the response changes <em>immediately</em> when the value of the explanatory variable changes. Considering a model with <span class="math inline">\(k\)</span> independent variables {<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_k\)</span>}, a static (multiple) regression model, has the form just seen above:</li>
</ul>
<p><span class="math display">\[
y_t = \beta_0 + \beta_1x_{1,t} + \beta_2x_{2,t} + ... + \beta_kx_{k,t} + \epsilon_t
\]</span></p>
<p>Each <span class="math inline">\(\beta\)</span> coefficient models the <em>instant change</em> in the conditional expected value of the response variable <span class="math inline">\(y_t\)</span> as the value of <span class="math inline">\(x_{k,t}\)</span> changes by one unit, keeping constant all the other predictors (i.e.: the other <span class="math inline">\(x_{k,t}\)</span>):</p>
<ul>
<li>A <strong>dynamic regression model</strong> includes relations between <em>both the current and the lagged (past) values of the explanatory (independent) variables</em>, that is, the expected value of the response variable may change <em>after</em> a change in the values of the explanatory variables.</li>
</ul>
<p><span class="math display">\[
\begin{split}
y_t &amp;= \beta_0
     + \beta_{10} x_{1,t} + \beta_{11} x_{1,t-1} + \dots + \beta_{1m} x_{1,t-m} \\
    &amp;\quad + \beta_{20} x_{2,t} + \beta_{21} x_{2,t-1} + \dots + \beta_{2m} x_{2,t-m} \\
    &amp;\quad + \dots \\
    &amp;\quad + \beta_{k0} x_{k,t} + \beta_{k1} x_{k,t-1} + \dots + \beta_{km} x_{k,t-m} \\
    &amp;\quad + \epsilon_t
\end{split}
\]</span></p>
<p>Despite the differences between these two analytic perspectives, the term <em>dynamic regression</em> is also used, in the literature, in a more general way to refer to regression models with autocorrelated errors (also when they are used to analyze only contemporary relations between variables).</p>
</div>
<div id="regression-models" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Regression models<a class="anchor" aria-label="anchor" href="#regression-models"><i class="fas fa-link"></i></a>
</h2>
<p>Except for the possible use of lagged regressors, which are typical of time series, the above described statistical models are standard regression models, commonly used with cross-sectional data.</p>
<p>Standard linear regression models can sometimes work well enough with time series data, <strong>if specific conditions are met</strong>. Besides standard assumptions of linear regression<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;1) Linearity: The relationship between X and Y must be linear; 2) Independence of errors: There is not a relationship between the residuals and the Y variable; 3) Normality of errors: The residuals must be approximately normally distributed; 4) Equal variances: The variance of the residuals is the same for all values of X&lt;/p&gt;"><sup>1</sup></a>, a careful analysis should be done in order to ascertain that <strong>residuals are not autocorrelated</strong>, since this can cause problems in the estimated model.</p>
<p>In this chapter we’ll see how to deal with autocorrelated residuals. However, even before that, it is important that the series are <strong>stationary</strong>, in order to avoid possible <em>spurious correlations</em>.</p>
<div id="stationarity" class="section level3" number="9.2.1">
<h3>
<span class="header-section-number">9.2.1</span> Stationarity<a class="anchor" aria-label="anchor" href="#stationarity"><i class="fas fa-link"></i></a>
</h3>
<p>We already discussed stationarity in the previous chapters. Here we can observe that time series can be nonstationary due to different reasons, thus different strategies can be employed to <em>stationarize</em> the data.</p>
<p>For instance, a nonstationary series can be a series with <strong>unequal variance</strong> over time. A common way to try to fix the problem is by applying a log-transformation.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-162-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>Another reason for nonstationarity is the periodic variation due to <strong>seasonality</strong> (regular fluctuations in a time series that follow a specific time pattern, e.g.: social media activity during week-ends, Christmas effect in consumption, etc.).</p>
<p>To remove the seasonal pattern, you might want to use a <em>seasonally-adjusted</em> time series. Otherwise, you could create a dummy variable for the seasonal period (that is, a variable that follows the seasonal pattern in the data in order to account, in the model, for these fluctuations).</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-163-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>An important reason for nonstationarity is also the presence of a trend in the data. There are <strong>stochastic trends</strong> and <strong>deterministic trends</strong>. Deterministic trends are a fixed function of time, while stochastic trends change in an unpredictable way.</p>
<p>Series with a deterministic trend are also called <em>trend stationary</em> because they can be stationary around a deterministic trend, and it could be possible to achieve stationarity by removing the time trend. In trend stationary processes, the shocks to the process are transitory and the process is <em>mean reverting</em>.</p>
<p>Processes with a <em>stochastic trend</em> are also called <em>difference stationary</em> because they can become stationary through <em>differencing</em>. In series with stochastic trends we could see that shocks have permanent effects.</p>
<p>When dealing with <em>deterministic trend</em>, we might want to work with detrended series.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-164-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>Otherwise, in regression analysis, it is more common to add a dummy variable consisting of a value that increases with time, to account for a linear deterministic time trend. This time-count variable will remove the deterministic trend from the dependent variable, allowing the other predictors to explain the remaining variance.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-165-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>When we have a series with a stochastic trend, we can achieve stationarity through differencing.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-166-1.png" width="100%" style="display: block; margin: auto;"></div>
<div id="tests-for-stochastic-and-deterministic-trend" class="section level4" number="9.2.1.1">
<h4>
<span class="header-section-number">9.2.1.1</span> Tests for Stochastic and Deterministic Trend<a class="anchor" aria-label="anchor" href="#tests-for-stochastic-and-deterministic-trend"><i class="fas fa-link"></i></a>
</h4>
<p>The correct detrending method depends on the type of trend. First differencing is appropriate for intergrated <em>I(1)</em> time series and time-trend regression is appropriate for trend stationary <em>I(0)</em> time series.</p>
<p>In case of deterministic trend, differencing is the incorrect solution, while detrending the series in function of time (regressing the series on a variable such as time and saving the residuals) is the correct solution. Differencing when none is required (<em>over-differencing</em>) may induce dynamics into the series that are not part of the data-generating process (for instance, it could create a first-order moving average process).</p>
<p>Specific statistical tests have been developed to distinguish between the two types of trends. In particular, <em>unit root tests</em> and <em>stationary test</em> can be used to determine if trending data should be first differenced or regressed on deterministic functions of time to render the data stationary.</p>
<p>Considering a simple model like the following, where <span class="math inline">\(Td\)</span> is a deterministic linear trend and <span class="math inline">\(z_t\)</span> is an autoregressive process of order 1 <em>AR(1)</em>. The difference between a process with stochastic and deterministic trend can be traced back to the parameter <span class="math inline">\(|\phi|\)</span>: When <span class="math inline">\(|\phi| = 1\)</span>, then <span class="math inline">\(z_t\)</span> is a <em>stochastic trend</em> and <span class="math inline">\(y_t\)</span> is an integrated process <em>I(1)</em> with <em>drift</em> (the so-called “drift” refers to the presence of a constant term, in this case <span class="math inline">\(\kappa\)</span>). When <span class="math inline">\(\phi &lt; 1\)</span>, the process is not integrated (<em>I(0)</em>) and <span class="math inline">\(y_t\)</span> exhibits a <em>deterministic trend</em><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Reference of this part is Zivot E., Wang J. (2003), Unit Root Tests, in &lt;em&gt;Modeling Financial Time Series with S-Plus®&lt;/em&gt;. Springer, New York&lt;/p&gt;"><sup>2</sup></a>:</p>
<p><span class="math display">\[
\begin{split}
y_t &amp;= Td_t + z_t \\
Td_t &amp;= \kappa + \delta_t \\
z_t &amp;= \phi z_{t-1} + \epsilon_t, \quad \epsilon_t \sim N(0, \sigma^2)
\end{split}
\]</span></p>
<p>Let’s simulate and visualize the above equation (<span class="math inline">\(y_t = \kappa + \delta_t + \phi z_{t-1} + \epsilon_t\)</span>):</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-167-1.png" width="100%" style="display: block; margin: auto;"></div>
<p><strong>Unit root tests</strong> are aimed at testing the null hypothesis that <span class="math inline">\(|\phi| = 1\)</span> (<em>difference stationary</em>), against the alternative hypothesis that <span class="math inline">\(|\phi| &lt; 1\)</span> (<em>trend stationary</em>).</p>
<p><strong>Stationarity tests</strong> take the null hypothesis that <span class="math inline">\(y_t\)</span> is trend stationary, and are based on testing for a moving average element in <span class="math inline">\(\Delta z_t\)</span> (<span class="math inline">\(\Delta\)</span> represents the operation of differencing).</p>
<p><span class="math display">\[
\begin{split}
\text{Original} \\
y_t &amp;= Td_t + z_t \\
Td_t &amp;= \kappa + \delta_t \\
z_t &amp;= \phi z_{t-1} + \epsilon_t, \quad \epsilon_t \sim N(0, \sigma^2)
\end{split}
\]</span></p>
<p><span class="math display">\[
\begin{split}
\text{First difference} \\
\Delta y_t &amp;= \Delta Td_t + \Delta z_t \\
\Delta Td_t &amp;= \Delta \kappa + \Delta \delta_t = \delta \\
\Delta z_t &amp;= \phi \Delta z_{t-1} + \Delta \epsilon_t = \phi \Delta z_{t-1} + \epsilon_t - \epsilon_{t-1}
\end{split}
\]</span></p>
<p><span class="math inline">\(\Delta z_t\)</span> can be also written as:</p>
<p><span class="math display">\[
\Delta \epsilon_t = \phi \Delta z_{t-1} + \epsilon_t + \theta \epsilon_{t-1}
\]</span></p>
<p>with <span class="math inline">\(\theta = -1\)</span>. That is, when the series is trend stationary, taking the first difference results in overdifferencing and in the creation of a moving average (MA) term <span class="math inline">\(\theta \epsilon_{t-1}\)</span>. The creation of a moving average element, which is missing in the original series, is also why differencing a trend-stationary process is problematic.</p>
<div id="kpss-test" class="section level5" number="9.2.1.1.1">
<h5>
<span class="header-section-number">9.2.1.1.1</span> KPSS Test<a class="anchor" aria-label="anchor" href="#kpss-test"><i class="fas fa-link"></i></a>
</h5>
<p>A test to verify if the series is <em>trend stationary</em> is the <strong>Kwiatkowski-Phillips-Schmidt-Shin (KPSS)</strong> test. It is one of the most commonly used stationarity test, and is implemented in the library <em>tseries</em> (function <em>kpss.test</em>). KPSS test the <em>null hypothesis</em> that the series is <em>trend stationary</em>.</p>
<p>In this case, the <em>p-value</em> of the test is higher than 0.05, so the test cannot reject the null hypothesis of trend stationarity. That is to say, there are some evidence of trend-stationary process.</p>
<pre><code>## 
##  KPSS Test for Trend Stationarity
## 
## data:  y_I0
## KPSS Trend = 0.10404, Truncation lag parameter = 5, p-value
## = 0.1</code></pre>
<p>By changing the null from “Trend” to “Level”, the KPSS test can also test the <em>null hypothesis</em> of <strong>level stationarity</strong>. A level stationary time series is a <em>time series with a non-zero but constant mean</em>, that is to say, without trend.</p>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  y_I0
## KPSS Level = 8.4092, Truncation lag parameter = 5, p-value
## = 0.01</code></pre>
<p>In this case, the KPSS test for level stationarity reject the null hypothesis, that is to say, the process seems not to be level stationary. Considered together, the KPSS tests suggest that the series has a deterministic trend.</p>
<p>If we use the KPSS test to test if the <em>stochastic trend</em> series we created above is trend or level stationary, the test <em>rejects</em> the null hypothesis (i.e.: reject the hypothesis of both a trend and level stationary process).</p>
<pre><code>## 
##  KPSS Test for Trend Stationarity
## 
## data:  y_I1
## KPSS Trend = 1.5, Truncation lag parameter = 5, p-value =
## 0.01</code></pre>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  y_I1
## KPSS Level = 7.7377, Truncation lag parameter = 5, p-value
## = 0.01</code></pre>
<p>When a series has a stochastic trend, we can achieve stationarity through differencing. Indeed, the KPSS test does not reject the null hypothesis of level stationarity when applied to the the stochastic-trend series, once differenced.</p>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  diff(y_I1)
## KPSS Level = 0.18432, Truncation lag parameter = 5, p-value
## = 0.1</code></pre>
<p>In the above cases the KPSS results are correct, since we have simulated and tested a time series with a deterministic and stochastic trend. However, these kind of tests can also be wrong. For instance, it is possible they reject the null hypothesis when it is actually true (<a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors">“Type I error”</a>). For this reason, it can be useful to use more than one test. For instance, the KPSS can be used along with the Augmented Dickey-Fuller Test (ADF), a popular <em>unit root test</em>.</p>
</div>
<div id="augmented-dickey-fuller-adf-test" class="section level5" number="9.2.1.1.2">
<h5>
<span class="header-section-number">9.2.1.1.2</span> Augmented Dickey-Fuller (ADF) Test<a class="anchor" aria-label="anchor" href="#augmented-dickey-fuller-adf-test"><i class="fas fa-link"></i></a>
</h5>
<p>The Augmented Dickey-Fuller Test (ADF) is a popular <em>unit root test</em>. An R implementation of the test can be found in the library <em>tseries</em> (function <em>adf.test</em>). The null hypothesis is that the series has a unit root, and the alternative hypothesis is that the series is stationary or trend stationary.</p>
<p>If we use the ADF test on the integrated series (which has a unit root), the test fails to reject the null hypothesis of unit root, which is correct.</p>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  y_I1
## Dickey-Fuller = -1.7632, Lag order = 7, p-value = 0.6785
## alternative hypothesis: stationary</code></pre>
<p>If we use the ADF test on the trend-stationary series (without unit root), the test rejects the null hypothesis of unit root, which is correct.</p>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  y_I0
## Dickey-Fuller = -5.8397, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<p>If we use the ADF test on the integrated series, after having transformed it through differencing, the test rejects the null hypothesis of unit root, which is correct.</p>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  diff(y_I1)
## Dickey-Fuller = -7.8913, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
</div>
<div id="phillips-perron-test" class="section level5" number="9.2.1.1.3">
<h5>
<span class="header-section-number">9.2.1.1.3</span> Phillips-Perron Test<a class="anchor" aria-label="anchor" href="#phillips-perron-test"><i class="fas fa-link"></i></a>
</h5>
<p>Another <em>unit root test</em> is the <strong>Phillips-Perron</strong> test. It differs from the ADF test in some aspects (how it deals with serial correlation and heteroskedasticity in the errors). Also this test is implemented in the library <em>tseries</em> (funtion <em>pp.test</em>). Results of the test are similar to those of the ADF test:</p>
<pre><code>## 
##  Phillips-Perron Unit Root Test
## 
## data:  y_I0
## Dickey-Fuller Z(alpha) = -144.04, Truncation lag parameter
## = 5, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<pre><code>## 
##  Phillips-Perron Unit Root Test
## 
## data:  y_I1
## Dickey-Fuller Z(alpha) = -5.4817, Truncation lag parameter
## = 5, p-value = 0.8039
## alternative hypothesis: stationary</code></pre>
<pre><code>## 
##  Phillips-Perron Unit Root Test
## 
## data:  diff(y_I1)
## Dickey-Fuller Z(alpha) = -489.82, Truncation lag parameter
## = 5, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<p>In case of uncertainty, more than one test can be used.</p>
</div>
</div>
</div>
<div id="non-autocorrelated-residuals" class="section level3" number="9.2.2">
<h3>
<span class="header-section-number">9.2.2</span> Non-autocorrelated residuals<a class="anchor" aria-label="anchor" href="#non-autocorrelated-residuals"><i class="fas fa-link"></i></a>
</h3>
<p>We try to fit a linear regression model. First, we create two series <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, with <span class="math inline">\(x\)</span> correlated with <span class="math inline">\(y\)</span> at lags <span class="math inline">\(x_{t-3}\)</span> and <span class="math inline">\(x_{t-4}\)</span>.</p>
<p>The <em>real</em> model (in this case we know it because we created it through the above simulation), is as follows:</p>
<p><span class="math display">\[
y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1)
\]</span>
#### lm</p>
<p>To fit a linear regression, we can use the function <strong>lm</strong> (the standard funtion to perform linear regression analysis in base R, no additional packages are necessary).</p>
<p>The function <strong>summary</strong> prints the summary of the model, which includes the estimates (the “coefficients” of the variables), the standard errors, the statistical significance of the variables, and other information.</p>
<pre><code>## 
## Call:
## lm(formula = xy_series[, 1] ~ xy_series[, 2] + xy_series[, 3])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.57051 -0.73392  0.06238  0.74187  2.14794 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    14.96869    0.07238  206.80   &lt;2e-16 ***
## xy_series[, 2]  0.85549    0.07680   11.14   &lt;2e-16 ***
## xy_series[, 3]  1.42126    0.07678   18.51   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.002 on 196 degrees of freedom
## Multiple R-squared:  0.873,  Adjusted R-squared:  0.8717 
## F-statistic: 673.5 on 2 and 196 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We said that regression models sometimes work well enough with time series data, if specific conditions are met. Regards the conditions (or <strong>assumptions</strong>), in particular, the <strong>residuals</strong> of the models should have zero mean, they shouldn’t show any significant autocorrelation, and they should be normally distributed.</p>
<p>To check whether these assumptions are met, we can visualize the <em>plot of residuals, its ACF/PACF and histogram</em>, and also test the residuals for possible autocorrelation using a statistical test like the <a href="https://en.wikipedia.org/wiki/Breusch%E2%80%93Godfrey_test">Breusch-Godfrey test</a> (this test is the default in the forecast library when a linear regression object <em>lm</em> is tested).</p>
<p>To create the plots we can use the base R functions, or we can use the convenient <em>checkresiduals</em> function in the <em>forecast</em> package.</p>
<p>In this case everything seems fine.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-180-1.png" width="100%" style="display: block; margin: auto;"></div>
<pre><code>## 
##  Breusch-Godfrey test for serial correlation of order up to
##  10
## 
## data:  Residuals
## LM test = 8.689, df = 10, p-value = 0.5618</code></pre>
<p>If we look at the model summary printed above, we can see that the estimated model is the following (the standard deviation of residuals is <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sigma.html">misnamed as “residual standard error” in the summary of <em>lm</em></a>):</p>
<p><span class="math display">\[
y_t = 14.96869 + 0.85549x_{t-3} + 1.42126x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1.002^2)
\]</span>
The estimated model is also close to the “true” model:</p>
<p><span class="math display">\[
y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1)
\]</span>
#### dynml</p>
<p>Instead of <em>lm</em>, the package <strong>dynml</strong> and the function with the same name (<em>dynml</em>) can be used to fit a dynamic regression models in R. One of the main advantages of this package is that it allows users to fit time series linear regression models without calculating the lagged values by hand. To add a lagged variable, it can simply be used the <em>L</em> (<em>Lag</em>) function. The <em>L</em> function takes as arguments the name of the variable and the lag length. For instance <em>L(x, 4)</em> corresponds to <span class="math inline">\(x_{t-4}\)</span>.</p>
<pre><code>## 
## Time series regression with "ts" data:
## Start = 5, End = 203
## 
## Call:
## dynlm(formula = y_series ~ L(x_series, 3) + L(x_series, 4))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.57051 -0.73392  0.06238  0.74187  2.14794 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    14.96869    0.07238  206.80   &lt;2e-16 ***
## L(x_series, 3)  0.85549    0.07680   11.14   &lt;2e-16 ***
## L(x_series, 4)  1.42126    0.07678   18.51   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.002 on 196 degrees of freedom
## Multiple R-squared:  0.873,  Adjusted R-squared:  0.8717 
## F-statistic: 673.5 on 2 and 196 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The <em>dynlm</em> function also permits to include trend (function <em>trend</em>) and seasonal (function <em>season</em>) components in the model (it is also possible to change the reference value for the seasonal period, see <em>?dynlm</em>). Just to make an example of the code to perform a dynamic regression with <em>dynlm</em>:</p>
<pre><code>## 
## Time series regression with "ts" data:
## Start = 1949(7), End = 1960(12)
## 
## Call:
## dynlm(formula = ap ~ trend(ap) + season(ap) + L(ap_x, 3))
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.156306 -0.035098  0.007821  0.041535  0.141713 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    4.116392   0.224848  18.307  &lt; 2e-16 ***
## trend(ap)      0.105758   0.005596  18.899  &lt; 2e-16 ***
## season(ap)Feb -0.026936   0.024867  -1.083 0.280811    
## season(ap)Mar  0.127650   0.026209   4.870 3.32e-06 ***
## season(ap)Apr  0.111029   0.028324   3.920 0.000146 ***
## season(ap)May  0.132132   0.031750   4.162 5.86e-05 ***
## season(ap)Jun  0.248250   0.030001   8.275 1.71e-13 ***
## season(ap)Jul  0.334578   0.027595  12.125  &lt; 2e-16 ***
## season(ap)Aug  0.333578   0.029138  11.448  &lt; 2e-16 ***
## season(ap)Sep  0.172679   0.026344   6.555 1.35e-09 ***
## season(ap)Oct  0.034292   0.026311   1.303 0.194866    
## season(ap)Nov -0.101556   0.027526  -3.689 0.000335 ***
## season(ap)Dec -0.010445   0.024767  -0.422 0.673966    
## L(ap_x, 3)     0.061495   0.022439   2.741 0.007039 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.05831 on 124 degrees of freedom
## Multiple R-squared:  0.9829, Adjusted R-squared:  0.9811 
## F-statistic: 546.9 on 13 and 124 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="regression-with-arma-errors" class="section level3" number="9.2.3">
<h3>
<span class="header-section-number">9.2.3</span> Regression with ARMA errors<a class="anchor" aria-label="anchor" href="#regression-with-arma-errors"><i class="fas fa-link"></i></a>
</h3>
<p>While in the previous case a standard linear model works well, it is often the case that <em>residuals of times series regressions are autocorrelated</em>, and a linear regression model can be suboptimal or even wrong. For instance, let’s create other two time series that are, as the previous ones, cross-correlated at lag 3 and 4, but with a bit more complicated structure.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-183-1.png" width="100%" style="display: block; margin: auto;"></div>
<pre><code>## $ccf
## 
## Autocorrelations of series 'X', by lag
## 
##    -19    -18    -17    -16    -15    -14    -13    -12    -11 
## -0.021 -0.029  0.012  0.065  0.024  0.008  0.076  0.107  0.001 
##    -10     -9     -8     -7     -6     -5     -4     -3     -2 
## -0.031 -0.024 -0.039  0.048  0.070 -0.021  0.620  0.425  0.036 
##     -1      0      1      2      3      4      5      6      7 
## -0.001  0.052  0.080  0.068 -0.008 -0.011  0.096  0.124  0.062 
##      8      9     10     11     12     13     14     15     16 
## -0.022  0.030  0.023 -0.035 -0.039 -0.008  0.016 -0.059 -0.149 
##     17     18     19 
## -0.120 -0.027 -0.020 
## 
## $model
## 
## Call:
## ar.ols(x = x)
## 
## Coefficients:
##       1        2        3        4        5        6        7  
##  0.6362   0.1109   0.0496  -0.1533  -0.0240  -0.0499   0.2041  
##       8        9       10       11       12       13       14  
##  0.0221  -0.0990  -0.0981   0.1703  -0.0558  -0.0812   0.1179  
## 
## Intercept: 0.006091 (0.06307) 
## 
## Order selected 14  sigma^2 estimated as  0.7336</code></pre>
<p>Considering the autocorrelated structure of the series, the true model can be written as follows:</p>
<p><span class="math display">\[
\begin{split}
y_t &amp;= 15 + 0.8 x_{t-3} + 1.5 x_{t-4} + \eta_t \\
\eta_t &amp;= 0.7 \eta_{t-1} + \epsilon_t + 0.6 \epsilon_{t-1} \\
\epsilon &amp;\sim N(0, 1)
\end{split}
\]</span></p>
<p>It is possible to calculate the regression using the <em>lm</em> function, calculating the lagged variables by hand, or to use the <em>dynml</em> library and function.</p>
<pre><code>## 
## Call:
## lm(formula = xy2_series[, 1] ~ xy2_series[, 3:4])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.9200 -1.4508  0.0667  1.5395  5.1083 
## 
## Coefficients:
##                                 Estimate Std. Error t value
## (Intercept)                      14.9005     0.1486 100.273
## xy2_series[, 3:4]x2Lagged.xLag3   1.0407     0.1595   6.523
## xy2_series[, 3:4]x2Lagged.xLag4   1.5171     0.1599   9.488
##                                 Pr(&gt;|t|)    
## (Intercept)                      &lt; 2e-16 ***
## xy2_series[, 3:4]x2Lagged.xLag3 6.14e-10 ***
## xy2_series[, 3:4]x2Lagged.xLag4  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.028 on 189 degrees of freedom
##   (12 observations deleted due to missingness)
## Multiple R-squared:  0.6735, Adjusted R-squared:   0.67 
## F-statistic: 194.9 on 2 and 189 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## 
## Time series regression with "ts" data:
## Start = 5, End = 196
## 
## Call:
## dynlm(formula = y2_series ~ L(x2_series, 3) + L(x2_series, 4))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.9200 -1.4508  0.0667  1.5395  5.1083 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      14.9005     0.1486 100.273  &lt; 2e-16 ***
## L(x2_series, 3)   1.0407     0.1595   6.523 6.14e-10 ***
## L(x2_series, 4)   1.5171     0.1599   9.488  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.028 on 189 degrees of freedom
## Multiple R-squared:  0.6735, Adjusted R-squared:   0.67 
## F-statistic: 194.9 on 2 and 189 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The estimated model is the following:</p>
<p><span class="math display">\[
\begin{split}
y_t &amp;= 14.9005 + 1.0407 x_{t-3} + 1.5171 x_{t-4} + \epsilon_t \\
\epsilon &amp;\sim N(0, 2.028^2)
\end{split}
\]</span></p>
<p>The original series can also be visualized with the fitted values (the values resulting from the model), to visually inspect how well the model represents the original series. The differences between the original and the fitted series are the <em>residuals</em>.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-185-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>The diagnostic plots of the residuals show the presence of autocorrelation, and the Breusch-Godfrey test is highly significant (its value is far lower than the critical value <span class="math inline">\(\alpha = 0.05\)</span>)</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-186-1.png" width="100%" style="display: block; margin: auto;"></div>
<pre><code>## 
##  Breusch-Godfrey test for serial correlation of order up to
##  10
## 
## data:  Residuals
## LM test = 149.45, df = 10, p-value &lt; 2.2e-16</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-186-2.png" width="100%" style="display: block; margin: auto;"></div>
<p>In this case, it’s better to take into account the residuals’ autocorrelation by using a regression model capable to handle autocorrelated time series structures.</p>
<p>In the previous chapter we said that ARIMA models are a special type of regression model, in which the dependent variable is the time series itself, and the independent variables are all lags of the time series. This model is capable to take into account the <em>autocorrelated</em> structure of time series.</p>
<p>ARIMA is a modeling technique that can be applied to a single time series, but it can be extended to include additional, <strong>exogenous variables</strong>. The ARIMA model including exogenous regressors (i.e.: other time series besides the lagged dependent variable) is like a multiple regression models for time series. In particular, it can be considered a regression model capable to control for autocorrelation in residuals.</p>
<p>It is possible to use more than one option to fit an ARIMA model with external regressors. A convenient option is provided by the function <strong>auto.arima</strong>, in the package <em>forecast</em>. This library has an argument <strong>xreg</strong> which can be use with <em>a numerical vector or matrix of external regressors, which must have the same number of rows as y</em> (see ?auto.arima).</p>
<pre><code>## Series: xy2_series[, 1] 
## Regression with ARIMA(1,0,1) errors 
## 
## Coefficients:
##          ar1     ma1  intercept  x2Lagged.xLag3  x2Lagged.xLag4
##       0.6863  0.6491    14.8532          0.9506          1.5732
## s.e.  0.0555  0.0528     0.3607          0.0757          0.0762
## 
## sigma^2 = 0.9482:  log likelihood = -265.76
## AIC=543.52   AICc=543.97   BIC=563.06</code></pre>
<p>The resulting model seems to be more appropriate than the previous one, fitted by using just a “classic” linear regression. This is clear also by comparing the two models through the <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion"><strong>AIC criterion (Akaike information criterion)</strong></a>. The AIC value is used to compare the <em>goodness-of-fit</em> of different models fitted to the same dataset. The lower the AIC value, the better the fit (see also the next paragraph).</p>
<p>The auto.arima function prints the AIC value by default, while this value is not given with the <em>lm</em> function. To get it, we need to use the <strong>AIC</strong> function.</p>
<pre><code>## [1] 821.4495</code></pre>
<p>In this case, the ARIMA regression model results a far better model (<em>AIC=543.52</em>) compared with the classic linear model (<em>AIC=821.45</em>).</p>
<p><span class="math display">\[
\begin{split}
y_t &amp;= 14.8532 + 0.9506 x_{t-3} + 1.5732 x_{t-4} + \eta_t \\
\eta_t &amp;= 0.6863 \eta_{t-1} + \epsilon_t + 0.6491 \epsilon_{t-1} \\
\epsilon &amp;\sim N(0, 0.9482)
\end{split}
\]</span></p>
<p>Diagnostic analysis of the residuals, shows that there is no concerning sign of autocorrelation in the residuals, which looks like white noise. Also the test for autocorrelated errors is not significant (the default test for autocorrelation when testing an ARIMA models with external regressors in the <em>forecast</em> package is the <strong>Ljung-Box test</strong>)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;There are many tests for detecting autocorrelation. Besides the already mentioned &lt;em&gt;Breusch-Godfrey test&lt;/em&gt; and &lt;em&gt;Ljung-Box test&lt;/em&gt;, other popular tests are the &lt;em&gt;Durbin Watson test&lt;/em&gt;, and the &lt;em&gt;Box–Pierce test&lt;/em&gt;. Each test has its own characteristics. For instance, the Durbin-Watson test is a popular way to test for autocorrelation, but it &lt;a href="https://www.jstor.org/stable/pdf/1909870.pdf?refreqid=excelsior%3A9526730d9debe4fa8f1a4d5fa601d523"&gt;shouldn’t be used with lagged dependent variables&lt;/a&gt;. In this case it can be used the Breusch-Godfrey test&lt;/p&gt;'><sup>3</sup></a>).</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-189-1.png" width="100%" style="display: block; margin: auto;"></div>
<pre><code>## 
##  Ljung-Box test
## 
## data:  Residuals from Regression with ARIMA(1,0,1) errors
## Q* = 6.3861, df = 8, p-value = 0.6041
## 
## Model df: 2.   Total lags used: 10</code></pre>
<p>Also by visually inspect the original series along with the fitted series (the values resulting from the model), it can be seen that the model is better than the previous one.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-190-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>We can also compare the fitted versus original values by using a scatterplot. A better model produces a thinner diagonal line.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-191-1.png" width="100%" style="display: block; margin: auto;">
The <em>auto.arima</em> function does not give the statistical significance of the coefficients (the approach adopted by the <em>forecast</em> library is different, based on the choice of the best model to do forecasting), but it is possible to get that by using the function <em>coeftest</em> in the library <em>lmtest</em>.</p>
<pre><code>## 
## z test of coefficients:
## 
##                 Estimate Std. Error z value  Pr(&gt;|z|)    
## ar1             0.686330   0.055525  12.361 &lt; 2.2e-16 ***
## ma1             0.649134   0.052850  12.283 &lt; 2.2e-16 ***
## intercept      14.853152   0.360689  41.180 &lt; 2.2e-16 ***
## x2Lagged.xLag3  0.950588   0.075705  12.557 &lt; 2.2e-16 ***
## x2Lagged.xLag4  1.573242   0.076224  20.640 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div id="count-regression-models" class="section level3" number="9.2.4">
<h3>
<span class="header-section-number">9.2.4</span> Count regression models<a class="anchor" aria-label="anchor" href="#count-regression-models"><i class="fas fa-link"></i></a>
</h3>
<p>The models described above are mostly used with <strong>continuous</strong> variables (expressed as <em>numeric</em> or <em>double</em> in the R data format). However, it is often the case that time series are composed of integer values, or <strong>count data</strong> (expressed as <em>integer</em>).</p>
<p>Sometimes, the above mentioned methods work well also with this type of data (for instance, when the counts are large). Other times, time series model developed for count data can be a better choice (for instance, when the series include mostly small integer values).</p>
<p>Two of the most common statistical models to deal with count data are based on the <a href="https://en.wikipedia.org/wiki/Poisson_distribution"><strong>Poisson</strong></a> and the <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution"><strong>Negative Binomial</strong></a> distributions. These probability distributions are the ones that are usually employed to model count data.</p>
<p>There are a few libraries to fit count time series regression models in R. We take into consideration <strong>tscount</strong>, and its function <em>tsglm</em>.</p>
<p>We consider, as an example of the <em>tscount</em> function to fit count time series regression models, the dataset “Seatbelts” (monthly number of killed drivers of light goods vehicles in Great Britain between January 1969 and December 1984), following the <a href="https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf">paper</a> that describes the <em>tscount</em> library.</p>
<p>The authors of the package write in the <a href="https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf">paper (par. 7.2)</a> describing the library:</p>
<blockquote>
<p>This time series is part of a dataset which was first considered by Harvey and Durbin (1986) for studying the effect of compulsory wearing of seatbelts introduced on 31 January 1983. The dataset, including additional covariates, is available in R in the object Seatbelts. In their paper Harvey and Durbin (1986) analyze the numbers of casualties for drivers and passengers of cars, which are <strong>so large</strong> that they can be treated with <strong>methods for continuous-valued data</strong>. The monthly number of killed drivers of vans analyzed here is <strong>much smaller</strong> (its <em>minimum is 2 and its maximum 17</em>) and <strong>therefore methods for count data are to be preferred</strong>.</p>
</blockquote>
<p>We are going to fit a model aimed at capturing a first order autoregressive <em>AR(1)</em> term and a yearly <em>seasonality</em> by a 12th order autoregressive term.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-195-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>The function <em>tsglm</em> allows users to declare the autoregressive and seasonal autoregressive terms in a convenient way (in the following part of the function: <em>model = list(past_obs = c(1, 12))</em>).</p>
<p>It is possible to check the residuals with the usual plots.</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-197-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>The function <em>summary</em> can be used to get the parameter estimates for the model (in this case the function can also employ a parametric bootstrap procedure (<em>B</em>) to obtain standard errors and confidence intervals of the regression parameters. The authors use <em>B=500</em> in the original paper, since in their experience this value yields stable results. Higher B values can be more precise but require time to be calculated).</p>
<pre><code>## 
## Call:
## tsglm(ts = timeseries_until1981, model = list(past_obs = c(1, 
##     12)), xreg = regressors_until1981, link = "log", distr = "poisson")
## 
## Coefficients:
##              Estimate  Std.Error  CI(lower)  CI(upper)
## (Intercept)   1.83284   0.367830    1.11191    2.55377
## beta_1        0.08672   0.080913   -0.07187    0.24530
## beta_12       0.15288   0.084479   -0.01269    0.31846
## PetrolPrice   0.83069   2.303555   -3.68420    5.34557
## linearTrend  -0.00255   0.000653   -0.00383   -0.00127
## Standard errors and confidence intervals (level =  95 %) obtained
## by normal approximation.
## 
## Link function: log 
## Distribution family: poisson 
## Number of coefficients: 5 
## Log-likelihood: -396.1765 
## AIC: 802.3529 
## BIC: 817.6022 
## QIC: 802.3529</code></pre>
<p>The model is as follows:</p>
<p><span class="math display">\[
log(\lambda_t) = 1.83 + 0.09Y_{t-1} + 0.15Y_{t-12} + 0.83X_t - 0.003t
\]</span>
In the above equation notice that, the Poisson regression, models the logarithm of the <em>Y</em> values at times <em>t</em> (expressed as <span class="math inline">\(log(\lambda_t)\)</span>).</p>
<p>Another example (using the dataset you can download <a href="https://drive.google.com/file/d/1eIOERBLCUCaap3WCoM5QT6I0iTW-Hqwa/view?usp=sharing">here</a>):</p>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-200-1.png" width="100%" style="display: block; margin: auto;"></div>
<pre><code>## 
## Autocorrelations of series 'X', by lag
## 
## -1.2500 -1.1667 -1.0833 -1.0000 -0.9167 -0.8333 -0.7500 -0.6667 
##  -0.127   0.064  -0.013   0.008   0.033   0.022  -0.041  -0.278 
## -0.5833 -0.5000 -0.4167 -0.3333 -0.2500 -0.1667 -0.0833  0.0000 
##   0.120  -0.313   0.417   0.282  -0.061  -0.025  -0.075  -0.012 
##  0.0833  0.1667  0.2500  0.3333  0.4167  0.5000  0.5833  0.6667 
##  -0.180   0.063   0.039  -0.154  -0.070  -0.040   0.029   0.068 
##  0.7500  0.8333  0.9167  1.0000  1.0833  1.1667  1.2500 
##  -0.008  -0.045  -0.221  -0.068  -0.039   0.112   0.177</code></pre>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-203-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>Besides checking the residuals, it is possible to plot the <strong>PIT histogram</strong>, provided by the function <strong>pit</strong> in <em>tscount</em>:</p>
<blockquote>
<p>A PIT histogram is a tool for evaluating the statistical consistency between the probabilistic forecast and the observation. The predictive distributions of the observations are compared with the actual observations. If the predictive distribution is ideal the result should be a flat PIT histogram with no bin having an extraordinary high or low level. For more information about PIT histograms see the references listed below.</p>
</blockquote>
<div class="inline-figure"><img src="_main_files/figure-html/unnamed-chunk-204-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>In the library are included other diagnostic tools and metrics that can help choosing between poisson and negative binomial models (see the <a href="https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf">paper</a> for further information).</p>
<p>The function <em>summary</em> prints the coefficients of the model and their confidence interval.</p>
<pre><code>## 
## Call:
## tsglm(ts = qanon, model = list(past_obs = 1), xreg = reg, link = "log", 
##     distr = "poisson")
## 
## Coefficients:
##                 Estimate  Std.Error  CI(lower)  CI(upper)
## (Intercept)      0.58373    0.18036   0.230232     0.9372
## beta_1           0.83746    0.04834   0.742715     0.9322
## fake_news_lag4   0.00892    0.00274   0.003546     0.0143
## fake_news_lag5   0.00638    0.00354  -0.000554     0.0133
## fake_news_lag6  -0.01617    0.00272  -0.021506    -0.0108
## Standard errors and confidence intervals (level =  95 %) obtained
## by normal approximation.
## 
## Link function: log 
## Distribution family: poisson 
## Number of coefficients: 5 
## Log-likelihood: -239.6582 
## AIC: 489.3163 
## BIC: 500.2646 
## QIC: 489.3163</code></pre>
</div>
</div>
<div id="model-selection-aic-aicc-bic" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Model Selection (AIC, AICc, BIC)<a class="anchor" aria-label="anchor" href="#model-selection-aic-aicc-bic"><i class="fas fa-link"></i></a>
</h2>
<p>Statistical modeling is, usually, a recursive process that requires to fit several different models and, eventually, to select the most appropriate one. For instance, the Box and Jenkins approach employed to find an appropriate ARIMA model for a time series (see the previous chapter), requires the fitting of multiple models to find the most suitable one based on the data. Similarly, the “auto.arima” function in the library <em>forecast</em>, that automatizes the search for an appropriate ARIMA model, conducts a search over possible model.</p>
<p>To compare the models and select the most appropriate one, it is necessary to use some criteria. In the example above we have employed the AIC criterion. Other similar criteria are the AICc, and the BIC. They all can be used to find the most appropriate model, by comparing the <em>goodness-of-fit</em> of different models fitted to the same dataset. For instance, the documentation of the “auto.arima” function says that the function <em>“returns best ARIMA model according to either AIC, AICc or BIC value”</em>.</p>
<p>The <strong>AIC</strong> criterion is the acronym for <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion"><em>Akaike information criterion)</em></a>. The lower the AIC value, the better the fit (see also the next paragraph).</p>
<p>The <strong>AICc</strong> criterion, is the same, but with a <em>correction for small sample size</em>. When the sample is small it can be used in place of the AIC criterion. As the sample size increases, the AICc converges to the AIC.</p>
<p>The <strong>BIC</strong> criterion is the <em>Bayesian Information Criterion (or Schwartz’s Bayesian Criterion)</em> and has a stronger penalty than the AIC for overparametrized models (more complex models, with several predictors).</p>
<p>These criteria can also be used when searching for an appropriate regression model, to compare several different models including different lags of the variables.</p>
<p>When comparing models by using these criteria, it is important that the models are fitted to <strong>the same dataset</strong>, otherwise the results are not comparable. This is an important aspect to take into account when using lagged predictors. For instance, you may want to try a model including one lagged predictor <span class="math inline">\(x_{t-1}\)</span> and a model including two lagged predictors <span class="math inline">\(x_{t-1}\)</span> and <span class="math inline">\(x_{t-2}\)</span>, and to compare them in order to select the best one according to AIC, AICc or the BIC criterion. However, when you add lagged predictor you loose data points.</p>
<pre><code>## Time Series:
## Start = 1 
## End = 42 
## Frequency = 1 
##               y       xLag0       xLag1       xLag2
##  1  1.075501365  0.48505236          NA          NA
##  2 -0.276740732 -1.20993027  0.48505236          NA
##  3 -0.646751445  0.01724609 -1.20993027  0.48505236
##  4  0.102585633  0.70085715  0.01724609 -1.20993027
##  5 -0.072519610 -0.40170226  0.70085715  0.01724609
##  6  1.619712305 -0.48727576 -0.40170226  0.70085715
##  7  0.687751998 -0.76436248 -0.48727576 -0.40170226
##  8  0.234929596  2.97223380 -0.76436248 -0.48727576
##  9 -0.430528202 -0.43578545  2.97223380 -0.76436248
## 10  0.638039146 -1.35197954 -0.43578545  2.97223380
## 11 -0.217226469 -0.52868886 -1.35197954 -0.43578545
## 12  0.020494309 -0.06027948 -0.52868886 -1.35197954
## 13 -1.913357840 -0.03222441 -0.06027948 -0.52868886
## 14 -0.526191144  0.55290101 -0.03222441 -0.06027948
## 15  0.416282618 -0.25966064  0.55290101 -0.03222441
## 16  0.457627862 -1.01400343 -0.25966064  0.55290101
## 17 -0.275639625 -0.86452761 -1.01400343 -0.25966064
## 18 -0.353882346  0.11040086 -0.86452761 -1.01400343
## 19 -0.901762561 -0.02452574  0.11040086 -0.86452761
## 20 -1.052145243 -2.51548495 -0.02452574  0.11040086
## 21  0.832626641  0.44375488 -2.51548495 -0.02452574
## 22  0.056411159  1.56218440  0.44375488 -2.51548495
## 23  0.838708652  0.97180894  1.56218440  0.44375488
## 24  0.344611778  1.58747428  0.97180894  1.56218440
## 25 -1.088605832 -1.84530863  1.58747428  0.97180894
## 26 -0.527820628 -1.27107477 -1.84530863  1.58747428
## 27  3.061137014  2.03899229 -1.27107477 -1.84530863
## 28 -0.520213835 -0.72914933  2.03899229 -1.27107477
## 29  0.248244201  0.93447357 -0.72914933  2.03899229
## 30  0.046969519 -0.75324932  0.93447357 -0.72914933
## 31  0.249449596 -0.68448064 -0.75324932  0.93447357
## 32  1.041843001 -0.59473197 -0.68448064 -0.75324932
## 33 -0.338376495 -0.12412231 -0.59473197 -0.68448064
## 34  0.201287727 -1.39014027 -0.12412231 -0.59473197
## 35  0.602711731 -1.23556534 -1.39014027 -0.12412231
## 36 -0.225018148 -1.28500792 -1.23556534 -1.39014027
## 37 -0.010113590 -1.05225339 -1.28500792 -1.23556534
## 38 -0.193597244  1.36496602 -1.05225339 -1.28500792
## 39  0.009188137 -1.07721180  1.36496602 -1.05225339
## 40  1.441899269  0.67538151 -1.07721180  1.36496602
## 41           NA          NA  0.67538151 -1.07721180
## 42           NA          NA          NA  0.67538151</code></pre>
<p>Thus, when you fit models with different lags, you have to fit them on the same dataset. In this case, for instance, you have to skip the NA rows, and use just the rows from 3 to 40.</p>
<p>Then you can compare the model, for instance, using the AIC criterion, and choose the model with the smallest value.</p>
<pre><code>## [1] 95.11836</code></pre>
<pre><code>## [1] 94.31932</code></pre>
<pre><code>## [1] 96.07911</code></pre>
<p>Finally, you fit the model using all the available data.</p>
<pre><code>## Series: example_data[, 1] 
## Regression with ARIMA(0,0,0) errors 
## 
## Coefficients:
##         xreg
##       0.2447
## s.e.  0.1120
## 
## sigma^2 = 0.6511:  log likelihood = -47.67
## AIC=99.34   AICc=99.66   BIC=102.72</code></pre>
<p>Besides these criteria, there are also other strategies for <a href="https://en.wikipedia.org/wiki/Model_selection">model selection</a>.</p>
</div>
<div id="some-examples-in-the-literature" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Some examples in the literature<a class="anchor" aria-label="anchor" href="#some-examples-in-the-literature"><i class="fas fa-link"></i></a>
</h2>
<p>There are several examples of the use of time series regression models in the literature in the field of communication science.</p>
<p>For instance, in <a href="https://ijoc.org/index.php/ijoc/article/viewFile/14843/3344">The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates</a><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Wozniak, A., Wessler, H., Chan, C. H., &amp;amp; Lück, J. (2021). The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates. &lt;em&gt;International Journal of Communication&lt;/em&gt;, 15(27)&lt;/p&gt;"><sup>4</sup></a>, the authors <em>examined whether the UN climate change conferences are conducive to an emergence of a transnational public sphere by triggering issue convergence and increased transnational interconnectedness across national media debates</em>. They authors detail the method they follows in this way:</p>
<blockquote>
<p>[…] Given the autoregressive nature and other properties of time series, an ordinary least squares regression analysis would violate the normality of error and the independence of observations assumption (Wells et al., 2019). Instead, <strong>we applied the dynamic regression approach</strong> (Gujarati &amp; Porter, 2009; Hyndman &amp; Athanasopoulos, 2018), which assumes that the <strong>error term follows an autoregressive integrated moving average (ARIMA) model</strong> (…). we found the best ARIMA structure of the error term by using the <em>auto.arima function from the forecast R package</em> (Hyndman &amp; Khandakar, 2008). It searches for an ARIMA structure that can explain the most variance according to the <em>Akaike information criterion</em> (Akaike, 1973).</p>
</blockquote>
<p>In this case they use the term “dynamic regression” to refer to a time series regression with ARIMA errors, but they did not include lagged values of their variables, thus analyzing contemporary relationships between variables.</p>
<p>The found, for instance, that <em>events taking place on a supranational level of governance (…) consistently led to spikes in media attention across countries. In contrast, a bottom-up effort such as Fridays for Future showed an inconsistent relationship with media attention across the four countries.</em></p>
<div class="inline-figure"><img src="images/Event-Centered%20Nature%20of%20Global%20Public%20Spheres.png" width="100%" style="display: block; margin: auto;"></div>
<p>In <a href="https://ijoc.org/index.php/ijoc/article/viewFile/11666/2819">Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event</a><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Lee, F. L., Liang, H., &amp;amp; Tang, G. K. (2019). Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event. International Journal of Communication, 13, 20.&lt;/p&gt;"><sup>5</sup></a>, the authors used both standard regression and regression with ARIMA errors to show that <em>“online incivility — operationalized as the use of foul language — grew as volume of political discussions and levels of cyberbalkanization increased. Incivility led to higher levels of opinion polarization.”</em>. Also in this case the authors analyze a “static process”, that is, focus on contemporary relationships between variables.</p>
<div class="inline-figure"><img src="images/Online-Incivility.png" width="100%" style="display: block; margin: auto;"></div>
<p>In <a href="https://journals.sagepub.com/doi/abs/10.1177/1077699013493792">Beyond cognitions: A longitudinal study of online search salience and media coverage of the president</a><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Ragas, M. W., &amp;amp; Tran, H. (2013). Beyond cognitions: A longitudinal study of online search salience and media coverage of the president. Journalism &amp;amp; Mass Communication Quarterly, 90(3), 478-499.&lt;/p&gt;"><sup>6</sup></a>, the authors used regression models with ARIMA errors to examine <em>shifts in newswire coverage and search interest among Internet users in President Obama during the first two years of his administration (2009-2010)</em>.</p>
<div class="inline-figure"><img src="images/Beyond-Cognitions.png" width="100%" style="display: block; margin: auto;"></div>
<p>In this case, the authors analyze relationships between variables taking into account lagged values, thus adopting a “dynamic process” perspective. For instance, they write:</p>
<blockquote>
<p>RQ2 sought to determine the time span of linkages between coverage volume and search volume. (…) <strong>ARIMA</strong> models were run to gauge the <em>dynamics</em> of mutual influence between these two time series. The first model examined the effect of coverage volume on search volume over time (i.e., basic agenda setting) (…) presidential public relations, was included as an additional input series. The first model, with search volume being a single dependent variable, was <strong>identified</strong> through a <strong>close examination of autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs)</strong>. This analysis revealed a classic <strong>autoregressive model for the series (1, 0, 0)</strong>. […] According to the results, <em>shifts in aggregate search volume over this two-year period were significantly</em> <strong>predicted by coverage volume over the prior five weeks</strong> (p &lt; .010)* and by presidential public relations efforts in the preceding two, three (p &lt; .001), and five weeks (p &lt; .005). The ARIMA model with two predictors was correctly specified (<strong>Ljung–Box Q</strong> = 18.132, p = .381) and it explained roughly 35% of the observed variation in the series.</p>
</blockquote>
<p>In <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4126885/">AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993–2007</a><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Stevens, R., &amp;amp; Hornik, R. C. (2014). AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993–2007. Journal of health communication, 19(8), 893-906&lt;/p&gt;"><sup>7</sup></a>, the authors <em>examined the effect of newspaper coverage of HIV/AIDS on HIV testing behavior in a U.S. population.</em>, using a <em>lagged regression</em> to support <em>causal order claims by ensuring that newspaper coverage precedes the testing behavior with the inclusion of the 1-month lagged newspaper coverage variable in the model</em>. Counterintuitively, they found that the news media coverage had a negative effect on testing behavior: <em>For every additional 100 HIV/AIDS risk related newspaper stories published in this group of U.S. newspapers each month, there was a 1.7% decline in HIV testing levels in the following month</em>, with a higher negative effects on African Americans.</p>
<div class="inline-figure"><img src="images/AIDS%20in%20Black%20and%20White.png" width="100%" style="display: block; margin: auto;"></div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="correlations-and-arima.html"><span class="header-section-number">8</span> Correlations and ARIMA</a></div>
<div class="next"><a href="intervention-analysis.html"><span class="header-section-number">10</span> Intervention Analysis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#regression"><span class="header-section-number">9</span> Regression</a></li>
<li><a class="nav-link" href="#static-and-dynamic-models"><span class="header-section-number">9.1</span> Static and Dynamic Models</a></li>
<li>
<a class="nav-link" href="#regression-models"><span class="header-section-number">9.2</span> Regression models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#stationarity"><span class="header-section-number">9.2.1</span> Stationarity</a></li>
<li><a class="nav-link" href="#non-autocorrelated-residuals"><span class="header-section-number">9.2.2</span> Non-autocorrelated residuals</a></li>
<li><a class="nav-link" href="#regression-with-arma-errors"><span class="header-section-number">9.2.3</span> Regression with ARMA errors</a></li>
<li><a class="nav-link" href="#count-regression-models"><span class="header-section-number">9.2.4</span> Count regression models</a></li>
</ul>
</li>
<li><a class="nav-link" href="#model-selection-aic-aicc-bic"><span class="header-section-number">9.3</span> Model Selection (AIC, AICc, BIC)</a></li>
<li><a class="nav-link" href="#some-examples-in-the-literature"><span class="header-section-number">9.4</span> Some examples in the literature</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Time Series Analysis With R</strong>" was written by Nicola Righetti. It was last built on 2025-10-14.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
