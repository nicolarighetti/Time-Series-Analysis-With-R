[{"path":"index.html","id":"time-series-analysis-with-r","chapter":"1 Time Series Analysis With R","heading":"1 Time Series Analysis With R","text":"Welcome book  Time Series Analysis R.book provides practical introduction analyzing time series data using R. guides reader :characteristics specificity time series dataThe characteristics specificity time series dataUsing free statistical software R conduct time series analysisUsing free statistical software R conduct time series analysisKey univariate multivariate techniques analyzing time seriesKey univariate multivariate techniques analyzing time seriesBy end book, readers understand unique aspects time series data able perform simple analyses  R using methods presented .Note: book first created students International Master Communication Science University Vienna.","code":""},{"path":"index.html","id":"citation","chapter":"1 Time Series Analysis With R","heading":"1.1 Citation","text":"use book work, please cite :Nicola Righetti (2025). Time Series Analysis R. [Online Bookdown Book]. Available : https://nicolarighetti.github.io/Time-Series-Analysis--R/","code":""},{"path":"getting-started-with-r.html","id":"getting-started-with-r","chapter":"2 Getting started with R","heading":"2 Getting started with R","text":"","code":""},{"path":"getting-started-with-r.html","id":"rstudio-interface-and-data","chapter":"2 Getting started with R","heading":"2.1 RStudio Interface and Data","text":"","code":""},{"path":"getting-started-with-r.html","id":"download-and-install-rstudio","chapter":"2 Getting started with R","heading":"2.1.1 Download and Install RStudio","text":"course based statistical software R. R easier use development environment RStudio (works Windows, Apple, OS).possible download free version RStudio Desktop official websites.might also use free online version RStudio registering RStudio Cloud free plan. However, free plan gives just 15 hours per months. lessons take 4.5 hours per month, since also need practice, best choice install RStudio R computer.Now going see get started RStudio Desktop.First, download install free version RStudio Desktop open software.","code":""},{"path":"getting-started-with-r-1.html","id":"getting-started-with-r-1","chapter":"3 Getting started with R","heading":"3 Getting started with R","text":"","code":""},{"path":"getting-started-with-r-1.html","id":"rstudio-interface-and-data-1","chapter":"3 Getting started with R","heading":"3.1 RStudio Interface and Data","text":"","code":""},{"path":"getting-started-with-r-1.html","id":"download-and-install-rstudio-1","chapter":"3 Getting started with R","heading":"3.1.1 Download and Install RStudio","text":"course based statistical software R. R easier use within RStudio, works Windows, macOS, operating systems.possible download free version RStudio Desktop official websites. might also use free online version RStudio registering RStudio Cloud free plan. However, free plan gives just 15 hours per months.Now let’s see get started RStudio Desktop. First, download install free version RStudio Desktop open software.","code":""},{"path":"getting-started-with-r-1.html","id":"create-a-rstudio-project-and-import-data","chapter":"3 Getting started with R","heading":"3.1.2 Create a RStudio Project and Import data","text":"starting data analysis project RStudio, create new dedicated environment keep scripts (files containing code perform analysis), data sets, outputs analysis (plots tables). dedicated workspace simply called project.create new project RStudio, follow steps:click File (top left);, click New Project;select New Directory, New Project;Choose folder project give name project. can use name Time-Series-Analysis--R.create new folder project main folder specified previous step. folder, find file .Rproj, name project assigned. work project, just need open .Rproj file.","code":""},{"path":"getting-started-with-r-1.html","id":"create-a-script","chapter":"3 Getting started with R","heading":"3.1.3 Create a Script","text":"project created, can open new script save .script file containing code. can create first script named basic-r-syntax, test basic code going see. script saved extension .r.can open, change, save file every time work . Saving code important; otherwise, write code every time work project!","code":""},{"path":"getting-started-with-r-1.html","id":"the-rstudio-user-interface","chapter":"3 Getting started with R","heading":"3.1.4 The RStudio User Interface","text":"interface RStudio organized four main quadrants:","code":"- The top-left quadrant is the editor. Here you can create or open a script and compose the R commands.\n- The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. There is also the *Files* tab, where you can navigate files and folders and find, for instance, the data sets you want to upload.\n- On the bottom left is the R Console window, where the code gets executed and the output is produced. You can run the commands, sending the code from the editor to the console, by highlighting it and hitting the *Run* button, or the Ctrl-Enter key combination. It is also possible to type and run commands directly into the console window (in this case, nothing will be saved).\n- The bottom-right quadrant is a window for graphics output. Here you can visualize your plots. There are also tabs for R packages and the R Help facility."},{"path":"getting-started-with-r-1.html","id":"load-and-save-data","chapter":"3 Getting started with R","heading":"3.1.5 Load and Save Data","text":"load data R, can click Files window top-right quadrant, navigate files folders, found data set file, can just click follow semi-automated import procedure.Otherwise, can upload data set using function. instance, import csv file, one common formats data sets, can use function read.csv. main argument function path file want upload. specify file path, consider working within specific environment; , working directory folder project (can double-check working directory running command getwd()). Thus, indicate path data set want upload, can write dot followed slash ./, followed path data set inside working directory. instance, case , data set saved folder named data inside working directory. name data set tweets_vienna extension .csv. Therefore, code upload file follows:save data, options. Generally, want save data set, can opt .csv .rds format. .rds format readable R, .csv format “universal” (can read Excel, instance).save file .csv, can use function write.csv. main arguments function name object saved, path folder object saved, name want assign file.save .rds file, procedure similar, saveRDS function employed. read .rds file, appropriate function readRDS.code , can notice hash mark sign followed text. comment. Comments textual content used describe code order make easier understand reuse. Comments written hash mark sign (#) text written hash mark sign ignored R: can read comments, R consider code.","code":"\nfake_news <- read.csv(\"./data/fake-news-stories-over-time-20210111144200.csv\")\nwrite.csv(fake_news, file = \"./data/fake_news.csv\")\nsaveRDS(fake_news, file = \"./data/fake_news.rds\")\n\nfake_news <- readRDS(\"./data/fake_news.rds\")   # read a .rds file"},{"path":"getting-started-with-r-1.html","id":"create-new-folders","chapter":"3 Getting started with R","heading":"3.1.6 Create new Folders","text":"good practice create, main folder project, sub-folders dedicated different types files used project, folder data data sets.create new folder, can go Files window RStudio interface, click New Folder, give name.","code":""},{"path":"getting-started-with-r-1.html","id":"basic-r","chapter":"3 Getting started with R","heading":"3.2 Basic R","text":"","code":""},{"path":"getting-started-with-r-1.html","id":"objects","chapter":"3 Getting started with R","heading":"3.2.1 Objects","text":"object R entity composed name value.arrow (<-) sign used create objects assign value object (change “update” previous value).Example: create object name object_consisting_of_a_number value equal 2:Enter name object console run command: value assigned object displayed.object equal value. Therefore, instance, object numerical value can used perform arithmetical operations.value object can transformed:object can also represent function.Example: create object sum (addition) function:function can now applied two numerical values:Actually, don’t need function, since mathematical functions already implemented R.value object can number, function, vector. Vectors sequences values.vector numbers can argument mathematical operations.R objects matrix, list, data.frame.matrix table composed rows columns containing numerical values.list just list objects. instance, list includes numerical value, vector numbers, matrix.data.frame like matrix can contain numbers also types data, characters (textual type data) factors (unordered categorical variables, gender, ordered categories, low, medium, high).Data sets usually stored data.frames. instance, import csv Excel file R, corresponding R object data.frame.access specific column data.frame, can use name data.frame, dollar symbol $, name column.possible add columns data.frame writing:name data.framethe dollar signa name new columnthe arrow sign <-vector values stored new column (length equal vectors composing data.frame)possible visualize first rows data.frame using function head.","code":"\nobject_consisting_of_a_number <- 2\nobject_consisting_of_a_number## [1] 2\nobject_consisting_of_a_number * 10## [1] 20\nobject_consisting_of_a_number <- object_consisting_of_a_number * 10\n\nobject_consisting_of_a_number## [1] 20\nfunction_sum <- function(x, y){\n  result <- x + y\n  return(result)\n}\nfunction_sum(5, 2)## [1] 7\nsum(5, 2)## [1] 7\n5 + 7## [1] 12\n2 * 3## [1] 6\n3^2## [1] 9\nsqrt(9)## [1] 3\nvector_of_numbers <- c(1,2,3,4,5,6,7,8,9,10) \nvector_of_numbers##  [1]  1  2  3  4  5  6  7  8  9 10\nvector_of_numbers * 2##  [1]  2  4  6  8 10 12 14 16 18 20\nvector_of_numbers + 3##  [1]  4  5  6  7  8  9 10 11 12 13\na_matrix <- matrix(data = 1:50, nrow = 10, ncol = 5)\n\na_matrix##       [,1] [,2] [,3] [,4] [,5]\n##  [1,]    1   11   21   31   41\n##  [2,]    2   12   22   32   42\n##  [3,]    3   13   23   33   43\n##  [4,]    4   14   24   34   44\n##  [5,]    5   15   25   35   45\n##  [6,]    6   16   26   36   46\n##  [7,]    7   17   27   37   47\n##  [8,]    8   18   28   38   48\n##  [9,]    9   19   29   39   49\n## [10,]   10   20   30   40   50\na_list <- list(object_consisting_of_a_number, vector_of_numbers, a_matrix)\n\na_list## [[1]]\n## [1] 20\n## \n## [[2]]\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## \n## [[3]]\n##       [,1] [,2] [,3] [,4] [,5]\n##  [1,]    1   11   21   31   41\n##  [2,]    2   12   22   32   42\n##  [3,]    3   13   23   33   43\n##  [4,]    4   14   24   34   44\n##  [5,]    5   15   25   35   45\n##  [6,]    6   16   26   36   46\n##  [7,]    7   17   27   37   47\n##  [8,]    8   18   28   38   48\n##  [9,]    9   19   29   39   49\n## [10,]   10   20   30   40   50\n# this is an object (vector) consisting of a series of numerical values\nnumerical_vector <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)\nnumerical_vector##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14\n# this is another object (vector) consisting of a series of categorical values\ncategorical_vector <- c(\"Monday\", \"Tuesday\", \"Monday\", \"Tuesday\", \"Monday\", \"Wednesday\",\"Thursday\", \"Wednesday\", \"Thursday\", \"Saturday\", \"Sunday\", \"Friday\", \"Saturday\", \"Sunday\")\ncategorical_vector##  [1] \"Monday\"    \"Tuesday\"   \"Monday\"    \"Tuesday\"   \"Monday\"    \"Wednesday\" \"Thursday\"  \"Wednesday\"\n##  [9] \"Thursday\"  \"Saturday\"  \"Sunday\"    \"Friday\"    \"Saturday\"  \"Sunday\"\n# this is an object consisting of a data.frame, created combining vectors through the function \"data.frame\"\na_dataframe <- data.frame(\"first_variable\" = numerical_vector,\n                          \"second_variable\" = categorical_vector)\na_dataframe##    first_variable second_variable\n## 1               1          Monday\n## 2               2         Tuesday\n## 3               3          Monday\n## 4               4         Tuesday\n## 5               5          Monday\n## 6               6       Wednesday\n## 7               7        Thursday\n## 8               8       Wednesday\n## 9               9        Thursday\n## 10             10        Saturday\n## 11             11          Sunday\n## 12             12          Friday\n## 13             13        Saturday\n## 14             14          Sunday\na_dataframe$first_variable##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14\na_dataframe$second_variable##  [1] \"Monday\"    \"Tuesday\"   \"Monday\"    \"Tuesday\"   \"Monday\"    \"Wednesday\" \"Thursday\"  \"Wednesday\"\n##  [9] \"Thursday\"  \"Saturday\"  \"Sunday\"    \"Friday\"    \"Saturday\"  \"Sunday\"\na_dataframe$a_new_variable <- c(12, 261, 45, 29, 54, 234, 45, 42, 6, 267, 87, 3, 12, 9)\na_dataframe##    first_variable second_variable a_new_variable\n## 1               1          Monday             12\n## 2               2         Tuesday            261\n## 3               3          Monday             45\n## 4               4         Tuesday             29\n## 5               5          Monday             54\n## 6               6       Wednesday            234\n## 7               7        Thursday             45\n## 8               8       Wednesday             42\n## 9               9        Thursday              6\n## 10             10        Saturday            267\n## 11             11          Sunday             87\n## 12             12          Friday              3\n## 13             13        Saturday             12\n## 14             14          Sunday              9\nhead(a_dataframe)##   first_variable second_variable a_new_variable\n## 1              1          Monday             12\n## 2              2         Tuesday            261\n## 3              3          Monday             45\n## 4              4         Tuesday             29\n## 5              5          Monday             54\n## 6              6       Wednesday            234"},{"path":"getting-started-with-r-1.html","id":"functions","chapter":"3 Getting started with R","heading":"3.2.2 Functions","text":"function coded operation applies object (e.g.: number, textual feature etc.) transform based specific rules. function name (name function) arguments. Among arguments function always object value, instance numerical value, content function applied , possible arguments (either mandatory optional).Functions operations applied objects give certain output. E.g.: arithmetical operation “addition” function applies two numbers give, output, sum. arguments “sum” function numbers added together.name function written parentheses, arguments function inside parentheses:Arguments functions can numbers also textual features. instance, function paste creates string composed strings takes arguments.R can sometimes find “nested” syntax, can confusing. best practice keep things simple possible.sum , functions manipulate transform objects. Data wrangling, data visualization, well data analysis, performed functions.","code":"\nsum(5, 3)## [1] 8\npaste(\"the\", \"cat\", \"is\", \"at\", \"home\")## [1] \"the cat is at home\"\n# this comment, written after the hash mark, describe what is going on here: two \"paste\" function nested together have been used (improperly! because they make the code more complicated than necessary) to show how functions can be nested together. It would have been better to use the \"paste\" function just one time!\npaste(paste(\"the\", \"cat\", \"is\", \"at\", \"home\"), \"and\", \"sleeps\", \"on\", \"the\", \"sofa\")## [1] \"the cat is at home and sleeps on the sofa\""},{"path":"getting-started-with-r-1.html","id":"data-types","chapter":"3 Getting started with R","heading":"3.2.3 Data Types","text":"Variables can different R formats, :double: numbers include decimals (0.1, 5.676, 121.67). format appropriate continuous variables;integer: 1, 2, 3, 10, 400. format suitable count data;factors: categorical variables. Factors can ordered (e.g.: level agreement: “high”, “medium”, “low”), (e.g.: hair colors “blond”, “dark brown”, “brown”);characters: textual labels;logicals: format logical values (.e.: TRUE FALSE)dates: used represent days;POSIX: class R format represent dates times.\nFigure 3.1: R data formats. Tables Gaubatz, K. T. (2014). Survivor’s Guide R: Introduction Uninitiated Unnerved. SAGE Publications.\nbetter specify appropriate type data importing data set. example , data format specified using import process RStudio.Notice data type “date” requires users specify additional information regarding format dates. Indeed, dates can written many different ways, read dates R necessary specify structure date. example, dates format Year-Month-Day, represented R “%Y-%m-%d” (details provided another section book).","code":""},{"path":"getting-started-with-r-1.html","id":"excercise","chapter":"3 Getting started with R","heading":"3.2.4 Excercise","text":"Upload data set “election news small”, using appropriate data format;Open script “basic-r-script” perform following operations:\nCheck first rows data set;\nAccess single columns;\nSave data frame name “election_news_small_test” folder “data” using function “write.csv” (review procedure go section “Load Save Data” book);\nComment code (comments written hash sign #);\nSave script.\nCheck first rows data set;Access single columns;Save data frame name “election_news_small_test” folder “data” using function “write.csv” (review procedure go section “Load Save Data” book);Comment code (comments written hash sign #);Save script.","code":""},{"path":"basic-data-wrangling-with-tidyverse.html","id":"basic-data-wrangling-with-tidyverse","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4 Basic Data Wrangling with Tidyverse","text":"Data wrangling process transforming mapping data one “raw” data form another format intent making appropriate valuable variety downstream purposes analytics. goal data wrangling assure quality useful data. Data analysts typically spend majority time process data wrangling compared actual analysis data.Another definition follows: Data wrangling process profiling transforming datasets ensure actionable set analysis tasks. One central goal make data usable: put data form can parsed manipulated analysis tools. Another goal ensure data responsive intended analyses: data contain necessary information, acceptable level description correctness, support successful modeling decision-making.“manipulate” data sets R:use basic R functions;employ specific libraries tidyverse. Tidyverse R library composed functions allow users perform basic advanced data science operations. https://www.tidyverse.org.R, library (“package”) coherent collection functions, usually created specific purposes.work tidyverse library, necessary install first, using following command: install.packages(“tidyverse”).installed tidyverse (library), necessary load , can work functions current R session:Besides using function install.packages(NAME---LIBRARY) using line code, also possible use RStudio interface.","code":"\n# to load a library used the command library(NAME-OF-THE-LIBRARY)\nlibrary(tidyverse)"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"the-pipe-operator","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.1 The Pipe Operator %>%","text":"Tidyverse peculiar syntax makes use -called pipe operator %>%, like following example:manipulate data sets can rely functions included dplyr: grammar data manipulation, providing consistent set verbs help solve common data manipulation challenges, mutate, rename, summarize.","code":"\na_dataframe %>%\n  group_by(second_variable) %>%\n  summarize(mean = mean(a_new_variable))## # A tibble: 7 × 2\n##   second_variable  mean\n##   <chr>           <dbl>\n## 1 Friday            3  \n## 2 Monday           37  \n## 3 Saturday        140. \n## 4 Sunday           48  \n## 5 Thursday         25.5\n## 6 Tuesday         145  \n## 7 Wednesday       138\nlibrary(readr)\ntweets <- read_csv(\"data/tweets_covid_small.csv\", \n    col_types = cols(created_at = col_datetime(format = \"%Y-%m-%d %H:%M:%S\"), \n        retweet_count = col_integer()))\nhead(tweets)## # A tibble: 6 × 4\n##   created_at          screen_name   source             retweet_count\n##   <dttm>              <chr>         <chr>                      <int>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0\n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0\n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0\n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"mutate","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.2 Mutate","text":"function mutate adds new variables data.frame overwrites existing variables.","code":"\ntweets <- tweets %>%\n  mutate(log_retweet_count = log(retweet_count))\n  \nhead(tweets)## # A tibble: 6 × 5\n##   created_at          screen_name   source             retweet_count log_retweet_count\n##   <dttm>              <chr>         <chr>                      <int>             <dbl>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0           -Inf   \n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0           -Inf   \n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3              1.10\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0           -Inf   \n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4              1.39\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96              4.56"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"rename","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.3 Rename","text":"rename function change name columns (sometimes can useful).previous two steps can performed time, concatenating operations pipe %>% operator.check data format variables stored data.frame, can use command str().Sometimes variables stored data.frame wrong format (see paragraph “data type”), may want convert new format. purpose, can use function mutate along functions .integer, .numeric, .character, .factor, .logical, .Date, .POSIXct() depending desired data format (possible advisable upload data paying attention type data. upload data correct format, can skip step).","code":"\ntweets <- tweets %>%\n  # rename (new_name = old_name)\n  rename(device = source)\n\nhead(tweets)## # A tibble: 6 × 5\n##   created_at          screen_name   device             retweet_count log_retweet_count\n##   <dttm>              <chr>         <chr>                      <int>             <dbl>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0           -Inf   \n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0           -Inf   \n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3              1.10\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0           -Inf   \n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4              1.39\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96              4.56\n# load again the data set\nlibrary(readr)\ntweets <- read_csv(\"data/tweets_covid_small.csv\", \n    col_types = cols(created_at = col_datetime(format = \"%Y-%m-%d %H:%M:%S\"), \n        retweet_count = col_integer()))\n\ntweets <- tweets %>%\n  mutate(log_retweet_count = log(retweet_count+1)) %>%\n  rename(device = source)\n\nhead(tweets)## # A tibble: 6 × 5\n##   created_at          screen_name   device             retweet_count log_retweet_count\n##   <dttm>              <chr>         <chr>                      <int>             <dbl>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0              0   \n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0              0   \n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3              1.39\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0              0   \n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4              1.61\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96              4.57\nstr(tweets)## tibble [100 × 5] (S3: tbl_df/tbl/data.frame)\n##  $ created_at       : POSIXct[1:100], format: \"2021-03-24 08:53:52\" \"2021-03-24 08:53:25\" \"2021-03-24 08:53:52\" ...\n##  $ screen_name      : chr [1:100] \"DoYourThingUK\" \"DoYourThingUK\" \"AlexS1595\" \"MakesworthAcc\" ...\n##  $ device           : chr [1:100] \"Twitter for iPhone\" \"Twitter for iPhone\" \"Twitter for iPhone\" \"Twitter Web App\" ...\n##  $ retweet_count    : int [1:100] 0 0 3 0 4 96 0 1 0 3 ...\n##  $ log_retweet_count: num [1:100] 0 0 1.39 0 1.61 ...\ntweets %>%\n  mutate(device = as.character(device)) %>%\n  head()## # A tibble: 6 × 5\n##   created_at          screen_name   device             retweet_count log_retweet_count\n##   <dttm>              <chr>         <chr>                      <int>             <dbl>\n## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0              0   \n## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0              0   \n## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3              1.39\n## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0              0   \n## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4              1.61\n## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96              4.57"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"summarize-and-group_by","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.4 Summarize and group_by","text":"aggregate data calculate summary values (instance, average number tweets day), can use function group_by (aggregate data, instance day) summarize calculate summary values.also possible create one summary variables .","code":"\ntweets_summary <- tweets %>%\n  group_by(screen_name) %>%\n  summarize(average_retweets = mean(retweet_count))\n\nhead(tweets_summary)## # A tibble: 6 × 2\n##   screen_name     average_retweets\n##   <chr>                      <dbl>\n## 1 2EXvoZ6nublpw1F              164\n## 2 AdilHaiderMD                  80\n## 3 AlexS1595                      3\n## 4 Andecave                      20\n## 5 ApKido                       150\n## 6 BBVA_Trader                    0\ntweets_summary <- tweets %>%\n  group_by(screen_name) %>%\n  summarize(average_retweets = mean(retweet_count),\n            average_log_retweets = mean(log_retweet_count))\n\nhead(tweets_summary)## # A tibble: 6 × 3\n##   screen_name     average_retweets average_log_retweets\n##   <chr>                      <dbl>                <dbl>\n## 1 2EXvoZ6nublpw1F              164                 5.11\n## 2 AdilHaiderMD                  80                 4.39\n## 3 AlexS1595                      3                 1.39\n## 4 Andecave                      20                 3.04\n## 5 ApKido                       150                 5.02\n## 6 BBVA_Trader                    0                 0"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"count-occurrences","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.4.1 Count occurrences","text":"useful operation perform summarizing data count occurrences certain variable. instance, count number tweets sent user, can use function n() inside summarize function.","code":"\ntweets_summary <- tweets %>%\n  group_by(screen_name) %>%\n  summarize(average_retweets = mean(retweet_count),\n            average_log_retweets = mean(log_retweet_count),\n            number_of_tweets = n())\n\nhead(tweets_summary)## # A tibble: 6 × 4\n##   screen_name     average_retweets average_log_retweets number_of_tweets\n##   <chr>                      <dbl>                <dbl>            <int>\n## 1 2EXvoZ6nublpw1F              164                 5.11                1\n## 2 AdilHaiderMD                  80                 4.39                1\n## 3 AlexS1595                      3                 1.39                1\n## 4 Andecave                      20                 3.04                1\n## 5 ApKido                       150                 5.02                1\n## 6 BBVA_Trader                    0                 0                   1"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"arrange","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.5 Arrange","text":"explore data set, can useful sort data (e.g., lowest highest value variable). tidyverse, can order data.frame using function arrange.sort data highest lowest value (descending order), minus sign (desc function) added.Without minus sign (“desc” command), data sorted lowest highest value.","code":"\ntweets_summary  %>%\n  arrange(-number_of_tweets) %>%\n  head()## # A tibble: 6 × 4\n##   screen_name     average_retweets average_log_retweets number_of_tweets\n##   <chr>                      <dbl>                <dbl>            <int>\n## 1 iprdhzb                    0.667                0.462                3\n## 2 DoYourThingUK              0                    0                    2\n## 3 MakesworthAcc              2                    0.805                2\n## 4 benphillips76              3.5                  1.45                 2\n## 5 viralvideovlogs            3.5                  1.45                 2\n## 6 2EXvoZ6nublpw1F          164                    5.11                 1\ntweets_summary  %>%\n  arrange(desc(average_retweets)) %>%\n  head()## # A tibble: 6 × 4\n##   screen_name    average_retweets average_log_retweets number_of_tweets\n##   <chr>                     <dbl>                <dbl>            <int>\n## 1 Oliver_Miguel1             1988                 7.60                1\n## 2 Lil_3arbiii                1627                 7.40                1\n## 3 Kittyhawk681               1285                 7.16                1\n## 4 JulesFox12                 1091                 7.00                1\n## 5 rosaesaa26                  983                 6.89                1\n## 6 lewisabzueta                822                 6.71                1\ntweets_summary  %>%\n  arrange(number_of_tweets) %>%\n  head()## # A tibble: 6 × 4\n##   screen_name     average_retweets average_log_retweets number_of_tweets\n##   <chr>                      <dbl>                <dbl>            <int>\n## 1 2EXvoZ6nublpw1F              164                 5.11                1\n## 2 AdilHaiderMD                  80                 4.39                1\n## 3 AlexS1595                      3                 1.39                1\n## 4 Andecave                      20                 3.04                1\n## 5 ApKido                       150                 5.02                1\n## 6 BBVA_Trader                    0                 0                   1"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"filter","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.6 Filter","text":"function filter keeps cases (“rows”) want focus . arguments function conditions must fulfilled filter data: ) name column want filter, b) values kept.examples , notice use double equal sign ==, also quotation marks indicate modalities categorical variable.also possible use several conditions time.","code":"\ntweets %>%\n  filter(retweet_count >= 500) %>%\n  arrange(-retweet_count)## # A tibble: 10 × 5\n##    created_at          screen_name     device              retweet_count log_retweet_count\n##    <dttm>              <chr>           <chr>                       <int>             <dbl>\n##  1 2021-03-24 08:53:31 Oliver_Miguel1  Twitter for Android          1988              7.60\n##  2 2021-03-24 08:53:48 Lil_3arbiii     Twitter for iPhone           1627              7.40\n##  3 2021-03-24 08:53:48 Kittyhawk681    Twitter Web App              1285              7.16\n##  4 2021-03-24 08:53:37 JulesFox12      Twitter for Android          1091              7.00\n##  5 2021-03-24 08:53:42 rosaesaa26      Twitter for Android           983              6.89\n##  6 2021-03-24 08:53:42 lewisabzueta    Twitter for Android           822              6.71\n##  7 2021-03-24 08:53:42 jonvthvn08      Twitter for iPhone            768              6.65\n##  8 2021-03-24 08:53:34 florent61647053 Twitter for Android           768              6.65\n##  9 2021-03-24 08:53:37 Ritu89903967    Twitter for Android           709              6.57\n## 10 2021-03-24 08:53:33 Hurica3         Twitter for iPhone            575              6.36\ntweets %>%\n  filter(retweet_count == 1988) ## # A tibble: 1 × 5\n##   created_at          screen_name    device              retweet_count log_retweet_count\n##   <dttm>              <chr>          <chr>                       <int>             <dbl>\n## 1 2021-03-24 08:53:31 Oliver_Miguel1 Twitter for Android          1988              7.60\ntweets %>%\n  filter(device == \"Twitter for Android\")## # A tibble: 33 × 5\n##    created_at          screen_name     device              retweet_count log_retweet_count\n##    <dttm>              <chr>           <chr>                       <int>             <dbl>\n##  1 2021-03-24 08:53:49 marcin_lukawski Twitter for Android             1             0.693\n##  2 2021-03-24 08:53:49 LebodyRanya     Twitter for Android             0             0    \n##  3 2021-03-24 08:53:47 anshunandanpra4 Twitter for Android             2             1.10 \n##  4 2021-03-24 08:53:44 insoumise007    Twitter for Android             5             1.79 \n##  5 2021-03-24 08:53:43 Metamorfopsies  Twitter for Android             0             0    \n##  6 2021-03-24 08:53:43 keepsmiling_130 Twitter for Android           164             5.11 \n##  7 2021-03-24 08:53:43 lovebresil01    Twitter for Android            81             4.41 \n##  8 2021-03-24 08:53:42 LightHealing    Twitter for Android             1             0.693\n##  9 2021-03-24 08:53:42 lewisabzueta    Twitter for Android           822             6.71 \n## 10 2021-03-24 08:53:42 rosaesaa26      Twitter for Android           983             6.89 \n## # ℹ 23 more rows\ntweets %>%\n  filter(device == \"Twitter for Android\",\n         retweet_count > 200) %>%\n  arrange(-retweet_count)## # A tibble: 8 × 5\n##   created_at          screen_name     device              retweet_count log_retweet_count\n##   <dttm>              <chr>           <chr>                       <int>             <dbl>\n## 1 2021-03-24 08:53:31 Oliver_Miguel1  Twitter for Android          1988              7.60\n## 2 2021-03-24 08:53:37 JulesFox12      Twitter for Android          1091              7.00\n## 3 2021-03-24 08:53:42 rosaesaa26      Twitter for Android           983              6.89\n## 4 2021-03-24 08:53:42 lewisabzueta    Twitter for Android           822              6.71\n## 5 2021-03-24 08:53:34 florent61647053 Twitter for Android           768              6.65\n## 6 2021-03-24 08:53:37 Ritu89903967    Twitter for Android           709              6.57\n## 7 2021-03-24 08:53:27 aspeaker66      Twitter for Android           331              5.81\n## 8 2021-03-24 08:53:42 JamesAn26254230 Twitter for Android           201              5.31"},{"path":"basic-data-wrangling-with-tidyverse.html","id":"select","chapter":"4 Basic Data Wrangling with Tidyverse","heading":"4.7 Select","text":"select used keep columns original data.frame. instance, can apply function keep just columns device retweet_count.","code":"\ntweets %>%\n  dplyr::select(device, retweet_count) %>%\n  head()## # A tibble: 6 × 2\n##   device             retweet_count\n##   <chr>                      <int>\n## 1 Twitter for iPhone             0\n## 2 Twitter for iPhone             0\n## 3 Twitter for iPhone             3\n## 4 Twitter Web App                0\n## 5 Twitter Web App                4\n## 6 Twitter for iPad              96"},{"path":"basic-concepts.html","id":"basic-concepts","chapter":"5 Basic Concepts","heading":"5 Basic Concepts","text":"","code":""},{"path":"basic-concepts.html","id":"time-series","chapter":"5 Basic Concepts","heading":"5.1 Time Series","text":"time series serially sequenced set values representing variable value different points time (VanLear, “Time Series Analysis”). consists measures collected time, regular time intervals, unit observation, resulting set ordered values. regularity frequency time series (can , instance, hourly, weekly, monthly, quarterly, yearly etc.).Time series data different cross-sectional data, set data observed sample units taken given point time, time dimension relevant can ignored. Cross-sectional data snapshot population interest one particular point time, time series show dynamical evolution variable time. Panel data combine cross-sectional time series data observing units time.Time fundamental variable time series. often relevant types statistical analyses. Also sociological perspective (psychological well), can see past events influence future behaviors. Oftentimes, can make reasonable prediction future social behaviors just observing past behaviors. Actually, social reproduction behaviors time predictability future social behaviors based past experience shared knowledge essential social order, thus, fundamental dimension human society.statistical perspective, impact time resulting repeated measurements time single subject unit, introduce dependency among data points prevents use common statistical techniques. cross-sectional data, observations assumed independent: values observed one unit influence values observed units. Time series observations different nature: time series collection independent observations, observations taken independent units, collection successive observations unit. Observations taken across units time (without regards time), across time unit.dealing time series data, time important factor taken account. introduces new dimension data. instance, can calculate variable increases decreases time, peaks given moment time, regular intervals. consider just , much, variable correlated another variable, correlation time among , peaks one variable precedes peaks one, much time requires variable impact another one, much impact changes time.Importantly, dealing time series data, acknowledge sampling adjacent points time introduces correlation data. serial dependency creates correlated errors violates assumptions many traditional statistical analyses can bias estimation error confidence intervals significance tests. characteristic time series data, general, precludes use common statistical approaches linear regression correlation analysis, assume observations independent.application “standard” statistical techniques time series data might lead foolish, totally unreliable results. instance, statisticians George Udny Yule wrote:«fairly familiar knowledge sometimes obtain quantities varying time (time-variables) quite high correlations attach physical significance whatever, although ordinary test correlation held certainly “significant.” (…) occurrence “nonsense-correlations” makes one mistrust serious arguments sometimes put forward basis correlations time-series. […] successive x’s y’s sample longer form random series, series successive terms closely related one another, usual conceptions (correlation, ed.) accustomed fail totally entirely apply» (Yule, G.U. (1926). sometimes get nonsense-correlations Time-Series? study sampling nature time-series. Journal royal statistical society, 89(1), 1-63.)funny website reporting spurious time series correlation tylervigen.com.Despite can funny see improbable correlations, keep mind adopting right approach analyze data serious issue research. paper American Journal Political Science, can read, instance:results analysis strongly suggest way event counts analyzed hundreds important political science studies produced statistically substantively unreliable results. Misspecification, inefficiency, bias, inconsistency, insufficiency, problems result unknowing application two common methods without theoretical justification empirical utility type data.Due peculiarity time series data, time series analysis developed specific statistical methodology appropriate analysis time-dependent data. Time series analysis aims providing understanding underlying processes patterns change time unit observation relations variables observed time, handling time structure data proper way.","code":""},{"path":"basic-concepts.html","id":"time-series-analysis","chapter":"5 Basic Concepts","heading":"5.2 Time Series Analysis","text":"Time series analysis approach employed many disciplines. Almost every field study data characterized time development, every phenomenon temporal dimension can conceived time series analyzed time series analysis methods. Time series analysis important part data analysis disciplines economics, analyze, instance, inflation trends; marketing, analyze number clients store number accesses e-commerce website; demography, study growth national population time trends population ageing; engineering, analyze radio frequencies; neurology, analyze brain waves detected electroencephalograms. Political science can interested studying patterns alternation political parties government, digital communication can use time series analysis study series tweets using hashtag, news media coverage certain topic, trends user searches search engines, provided Google Trends.use time series analysis communication science, can observed :“Many major theories models field contain time central player: two-step flow, cultivation, spiral--silence, agenda-setting, framing, communication mediation models, name (Nabi & Oliver, 2009). articulates set processes play time: Messages work way media systems networks, citizens perceive world around decide communicate, , make choices participation, presumably product process includes communication exposure. Indeed, words animate field—effect, flow, influence, dynamic, cycle—reveal understanding communication process, processes temporal dimensions (Box-Steffensmeier, Freeman, Hitt, & Pevehouse, 2014). perspective time series analysis can help expand notions time’s role dynamics. see several ways can become attentive time field”. Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, ., & Yang, J. (2019). Temporal Turn Communication Research: Time Series Analyses Using Computational Approaches. International Journal Communication (19328036), 13.“One common applications time series analyses mass communication agenda-setting research. approach correlate national news coverage topic time public opinion public policy topic, often estimate lagged effects decay effects time. Likewise, trends cycles television programming, viewing, advertising, explored time series analyses. interpersonal literature, popular one important applications time series analysis investigation mutual adaptation form patterns reciprocity compensation conversational partners course interaction.” (C. Arthur VanLear, “Time Series Analysis”, Allen, M. (Ed.). (2017). SAGE encyclopedia communication research methods. Sage Publications).general, can distinguish least following objectives time series analysis study:DESCRIPTION: Description process characterized intrinsic temporal dimension. Simple examples related questions : upward trend? peak certain point time? regular pattern recurring every year, particular moment time? Descriptive questions like can answered via descriptive time series analysis.EVALUATION: Evaluation impact certain event, occurring particular point time, process. instance: change social media moderation policy, led banning accounts linked conspiracy theories, impact quantity fake news shared online users? Specific time series techniques can used perform kind analysis.EXPLANATION: Explanation phenomenon characterized time series structure basis related variables. instance: quantity news shared Facebook help explain polarization debate online? volume news media articles topic help explain growth debate online topic? Inferential statistical techniques, regression models developed time series, used answer questions like .FORECASTING: Prediction future values process. instance: can expect news media coverage certain topic keep growing near future? subject time series forecasting.can also distinguish univariate multivariate time series analysis. Time series analysis can used explain temporal dependencies within processes. temporal dependency within social process, mean current value variable , part, function previous values variable. analyze univariate structure time series, univariate techniques used. Temporal dependency social processes, conversely, indicates current value variable part function previous values variables. Multivariate time series analysis used explain relations time series.","code":""},{"path":"basic-concepts.html","id":"stochastic-and-deterministic-processes","chapter":"5 Basic Concepts","heading":"5.3 Stochastic and Deterministic Processes","text":"general distinction can made time series, based deterministic non-deterministic nature.deterministic time series one can explicitly expressed analytic expression. random probabilistic parts. always possible exactly predict future behavior, state behaved past. Deterministic processes pretty rare dealing individual social behaviors! Predicting future behaviors crowd, person, social group, can reasonably possible, sometimes, based past behaviors contextual information, since human behavior partly influenced past. However, totally determined past. always certain degree uncertainty prediction; human behaviors , generally speaking, fully predictable.Social individual behaviors, therefore, non-deterministic. non-deterministic time series fully described analytic expression. random, probabilistic component, prevents behavior explicitly described. possible say, probabilistic terms, future behavior might . However, always residual, unpredictable, component. time series may considered non-deterministic also information necessary describe explicitly available, although might principle, nature generating process, part , inherently random. can say time series analyzed social science always, least, stochastic component makes totally deterministic.Since non-deterministic time series random component, follow probabilistic rather deterministic laws. Random data defined explicit mathematical relations, rather statistical terms, , probability distributions parameters mean variance. Non-deterministic time series can analyzed assuming manifestations probabilistic stochastic processes.","code":""},{"path":"time-series-objects.html","id":"time-series-objects","chapter":"6 Time Series Objects","heading":"6 Time Series Objects","text":"","code":""},{"path":"time-series-objects.html","id":"time-series-objects-1","chapter":"6 Time Series Objects","heading":"6.1 Time Series Objects","text":"Every object manipulate R characterized specific structure. Objects’ structures vary depending type object: list, matrix, data.frame, different objects different structures. Every structure manipulation methods. instance, can accessed analyzed using different functions strings code.R many different types object. get overview can refer handbook, R manual, chapter 5 free online book. course going learn data structures deal conducting time series analysis R, , structure time series objects data sets.Time series data sets R can represented different objects. Specific libraries (coherent collections functions) can give different structures time series data sets.many R libraries handling working time series objects. general useful perform specific analysis. page can find comprehensive list R libraries time series analysis.now, just need know different libraries can create time series objects different structures can manipulated different functions. means objects can analyzed functions, well always compatibility R libraries.\nMany functions developed reference specific libraries objects, require particular object structure. consequence, creating time series object certain structure certain library can imply can use functions perform certain type analysis. words, specific type objects introduce specific constrains data analysis (visualization), wise plan advance necessary analyses, select necessary libraries data structure.now introduce three types objects commonly used store analyze time series data:data.frame (base R)ts object (base R)xts object (created library xts).analyze structures objects strengths limitations. next chapter, ’ll also learn methods available visualize .","code":""},{"path":"time-series-objects.html","id":"time-series-as-data-frames","chapter":"6 Time Series Objects","heading":"6.1.1 Time Series as Data Frames","text":"Data frames (data.frame) common data set structure R. data.frame simply table cases variables (row case column variable).see example data.frame containing time series data, can upload data set containing number news articles mentioning keyword “elections” published USA news media. retrieved data set MediaCloud, free open source platform studying media ecosystem tracks millions stories published online. can download data set link.can upload .csv file using function read.csv. main argument read.csv function path file.using function class can see data.frame.can check first rows data.frame using function head, shows first rows data set, get idea structure simple data.frame.data.frame contains time series data: first column contains dates, columns contain values observations. can also see data.frame seems contain daily data, row corresponds specific day. data frame also includes, column “count”, number news articles mentioning keyword “elections”, column “total_count”, total number news articles topics, column “ratio” proportion news articles mentioning keyword (count/total_count).function head (tail) can impractical data.frame including lot columns, better use function str check structure data.frame.can see output function str, format column date Factor. format , case, automatically attributed R, (already said) can specified importing data.Factor appropriate format categorical variables, R includes specific format dates times. case just date, can convert variable type date. can change format variable using function .Date.can also perform operation tidyverse, using function mutate.data.frame common format data sets, including time series data sets. can many things data stored format, creating plots performing various types analysis. However, handle time series R specific data formats.","code":"\nelections_news <- read.csv(\"./data/elections-stories-over-time-20210111144254.csv\")\nclass(elections_news)## [1] \"data.frame\"\nhead(elections_news)##         date count total_count      ratio\n## 1 2015-01-01   373       25611 0.01456405\n## 2 2015-01-02   387       31932 0.01211950\n## 3 2015-01-03   289       24646 0.01172604\n## 4 2015-01-04   322       25513 0.01262102\n## 5 2015-01-05   567       39982 0.01418138\n## 6 2015-01-06   626       42366 0.01477600\nstr(elections_news)## 'data.frame':    2192 obs. of  4 variables:\n##  $ date       : chr  \"2015-01-01\" \"2015-01-02\" \"2015-01-03\" \"2015-01-04\" ...\n##  $ count      : int  373 387 289 322 567 626 507 521 531 346 ...\n##  $ total_count: int  25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ...\n##  $ ratio      : num  0.0146 0.0121 0.0117 0.0126 0.0142 ...\nelections_news$date <- as.Date(elections_news$date)\nstr(elections_news)## 'data.frame':    2192 obs. of  4 variables:\n##  $ date       : Date, format: \"2015-01-01\" \"2015-01-02\" \"2015-01-03\" ...\n##  $ count      : int  373 387 289 322 567 626 507 521 531 346 ...\n##  $ total_count: int  25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ...\n##  $ ratio      : num  0.0146 0.0121 0.0117 0.0126 0.0142 ...\nlibrary(tidyverse)\n\nelections_news <- elections_news %>%\n  mutate(date = as.Date(date))"},{"path":"time-series-objects.html","id":"time-series-as-ts-objects","chapter":"6 Time Series Objects","heading":"6.1.2 Time Series as TS objects","text":"basic object created handle time series R object class ts. name stands “Time Series”.example ts object already present R name “AirPassengers”, time series data set ts format. can load data set function data.applying function class can see object class ts.AirPassengers small data set can print data set see structure, example standard structure ts object.calling str function get synthetic information object.AirPassengers data set univariate time series representing monthly totals international airline passengers 1949 1960. every time series, start date end date. also frequency, frequency observations taken.characteristics differentiate ts object data.frame. structure data.frame lacks start end date, frequency value.functions start, end, frequency, can applied ts object check values. started saying functions work objects types objects. example. functions, indeed, work ts objects just part structures, arguments usually specified kind object created. work applied data.frame object, since data.frame structure include start end date, frequency observations. can seen ts structure much specific time series data.Importantly, frequency time series assumed regular time. applies time series general, just ts objects. case, time series starts January 1949 ends December 1969, monthly frequency. Monthly frequency indicated ts “12”, meaning 12 months. Indeed, reference unit ts object year. , quarterly data, instance, frequency equal 4.create ts object necessary follow specific steps use specific functions. exemplify process creation ts object take example data contained AirPassengers data set, store data.frame (don’t need learn , just copy paste code). data.frame data set format probably start , can useful see create ts object starting data.frame.create “ts” time series object starting data.frame, need:specify column contains observations. case, column name “Passengers”.need specify start end date, case format year/month, can just years case yearly data. ts format start/end date following: c=(YEAR, MONTH). c represents concatenate function, concatenates year month single vector.Finally, indicate frequency time series observations. frequency specified based time period year, case frequency equal 12, monthly observation, meaning 12 observation per year.’s important notice yearly, quarterly, monthly data work fine ts structure, fine grained data create complications totally suitable ts structure.due fact time series objects require frequency observations regular ts observations regular reference year. Unfortunately, time series spans many years composed constant number days, since number days sometimes 365 time 366, case leap years. limitation ts objects. However, dealing monthly data data frequency lower one month (quarterly data), ts works great.","code":"\ndata(AirPassengers)\nclass(AirPassengers)## [1] \"ts\"\nAirPassengers##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n## 1949 112 118 132 129 121 135 148 148 136 119 104 118\n## 1950 115 126 141 135 125 149 170 170 158 133 114 140\n## 1951 145 150 178 163 172 178 199 199 184 162 146 166\n## 1952 171 180 193 181 183 218 230 242 209 191 172 194\n## 1953 196 196 236 235 229 243 264 272 237 211 180 201\n## 1954 204 188 235 227 234 264 302 293 259 229 203 229\n## 1955 242 233 267 269 270 315 364 347 312 274 237 278\n## 1956 284 277 317 313 318 374 413 405 355 306 271 306\n## 1957 315 301 356 348 355 422 465 467 404 347 305 336\n## 1958 340 318 362 348 363 435 491 505 404 359 310 337\n## 1959 360 342 406 396 420 472 548 559 463 407 362 405\n## 1960 417 391 419 461 472 535 622 606 508 461 390 432\nstr(AirPassengers)##  Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\nstart(AirPassengers)[1]## [1] 1949\nend(AirPassengers)[1]## [1] 1960\nfrequency(AirPassengers)[1]## [1] 12\ndate <- seq.Date(from = as.Date(\"1949-01-01\"), \n                 to = as.Date(\"1960-12-01\"), by=\"month\")\npassengers <- as.vector(AirPassengers)\n\ndata_frame_format <- data.frame(\"Date\" = date, \n                                \"Passengers\" = passengers)\nts_format <- ts(data = data_frame_format$Passengers, \n                start=c(1949, 01), \n                end=c(1960, 12), \n                frequency = 12)\nts_format##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n## 1949 112 118 132 129 121 135 148 148 136 119 104 118\n## 1950 115 126 141 135 125 149 170 170 158 133 114 140\n## 1951 145 150 178 163 172 178 199 199 184 162 146 166\n## 1952 171 180 193 181 183 218 230 242 209 191 172 194\n## 1953 196 196 236 235 229 243 264 272 237 211 180 201\n## 1954 204 188 235 227 234 264 302 293 259 229 203 229\n## 1955 242 233 267 269 270 315 364 347 312 274 237 278\n## 1956 284 277 317 313 318 374 413 405 355 306 271 306\n## 1957 315 301 356 348 355 422 465 467 404 347 305 336\n## 1958 340 318 362 348 363 435 491 505 404 359 310 337\n## 1959 360 342 406 396 420 472 548 559 463 407 362 405\n## 1960 417 391 419 461 472 535 622 606 508 461 390 432\nclass(ts_format)## [1] \"ts\""},{"path":"time-series-objects.html","id":"time-series-as-xtszoo-objects","chapter":"6 Time Series Objects","heading":"6.1.3 Time Series as XTS/ZOO objects","text":"Time series can stored object class xts/zoo. class objects created library xts, related extension package zoo (another package deal time series data). libraries, requires installed loaded.xts object flexible ts one.\ncan create xts time series starting data.frame just created. Similarly required ts, need specify:column data.frame (vector) containing data;column data.frame (vector) containing dates/times (date/time format);frequency observations.can use data.frame already created AirPassengers data create new xts object.Also structure object, like ts one, includes range dates time series, starting ending date.","code":"\n# install.packages(\"xts\")\nlibrary(xts)\nxts_format <- xts(x = data_frame_format$Passengers, \n                  order.by = data_frame_format$Date, \n                  frequency = 12)\nclass(xts_format)## [1] \"xts\" \"zoo\"\nstr(xts_format)## An xts object on 1949-01-01 / 1960-12-01 containing: \n##   Data:    double [144, 1]\n##   Index:   Date [144] (TZ: \"UTC\")\nhead(xts_format)##            [,1]\n## 1949-01-01  112\n## 1949-02-01  118\n## 1949-03-01  132\n## 1949-04-01  129\n## 1949-05-01  121\n## 1949-06-01  135"},{"path":"plot-time-series.html","id":"plot-time-series","chapter":"7 Plot Time Series","heading":"7 Plot Time Series","text":"","code":""},{"path":"plot-time-series.html","id":"plot-time-series-objects","chapter":"7 Plot Time Series","heading":"7.1 Plot Time Series Objects","text":"lecture going learn plot time series data.take account three main functions: ggplot tidyverse library, plot.ts base R, plot.xts xts library. Ggplot probably versatile function perspective graphical results can obtained, also complex, ordinary visualization, plot.ts probably easiest tool.Plotting time series important part analysis permits visualizing exploring data, univariate perspective (focusing characteristics single time series) multivariate perspective (focusing characteristics many time series, relations ). visualize explore relations time series, ’ll learn plot single time series well many different time series .","code":""},{"path":"plot-time-series.html","id":"plot.ts","chapter":"7 Plot Time Series","heading":"7.2 plot.ts","text":"can visualize time series using function plot.ts() applied time series data ts format.example ts time series provided AirPassengers dataset, already included R (can load data running data(“AirPassengers”)).can add many details plot, title, label y axis x axis, change colors plot (use colors() see list standard colors R), size line.can use plot.ts plot two () time series together, useful operation take look relations. different time series must structure (starting date, ending date, frequency), stored ts object.merge one “ts” object two time series already ts format, can employ function ts.union(). can plot time series plot, create two different plots, using option “plot.type” specifying single multiple.lwd control line width (line size):line width, positive number, defaulting 1. interpretation device-specific, devices implement line widths less one.\nlty control line type:Line types can either specified integer (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) one character strings “blank”, “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, “twodash”, “blank” uses ‘invisible lines’ (.e., draw ).parameter nc can control number columns used display data.explore long time series can useful focus limited time window. , can subset data using function window. function extracts subset data observed specified start end time. can use function window plot.ts function (alternatively, can create new object applying function window first, plot new object).window function, can also specify frequency, series re-sampled new frequency. instance, re-sampling quarterly frequency, function keeps observations made January, April, July, October, re-sampling six-month frequency, keeps observations made January July.can use window function also one time series.learn something graphical options plot.ts, can open read help page using ?plot.ts. question mark followed name function opens help page function.","code":"\ndata(\"AirPassengers\")\nplot.ts(AirPassengers)\nplot.ts(AirPassengers, \n     main = \"PASSENGERS\",\n     xlab = \"1949-1960 (monthly data)\",\n     ylab = \"Passengers (1000's)\",\n     col = \"violetred3\", \n     lwd=5)\n# create a \"toy\" time series with the same lenght of the AirPassenger one\nAirPassengers_2 <- AirPassengers + 100\nAirPassengers_3 <- AirPassengers + 300\n\nAirPassengers_multi <- ts.union(AirPassengers, AirPassengers_2, AirPassengers_3)\n\nplot.ts(AirPassengers_multi, \n        main = \"Three time series\",\n        xlab = \"TIME\", ylab = \"VALUES\",\n        col = c(\"blue\", \"red\", \"black\"), \n        lwd=c(1, 1, 1), lty=c(1, 2, 3),\n        plot.type = \"single\")\nplot.ts(AirPassengers_multi, \n        main = \"Three time series\",\n        xlab = \"TIME\", ylab = \"VALUES\",\n        col = \"blue\", \n        lwd=4,\n        plot.type = \"multiple\")\nplot.ts(AirPassengers_multi, \n        main = \"Three time series\",\n        xlab = \"TIME\", ylab = \"VALUES\",\n        col = \"orange\",\n        lwd=4,\n        plot.type = \"multiple\",\n        nc=3)\nplot.ts(window(AirPassengers, start=c(1950, 01), end=c(1954, 12)), \n     main = \"PASSENGERS (1950-1955)\",\n     xlab = \"1950-1955 (monthly data)\",\n     ylab = \"Passengers (1000's)\",\n     col = \"violetred3\", \n     lwd=5)\nplot.ts(window(AirPassengers, start=c(1950, 01), end=c(1954,12), frequency = 4), \n        main = \"PASSENGERS (1950-1955) - QUARTERLY DATA\",\n        xlab = \"1950-1955 (Quarterly data)\",\n        ylab = \"Passengers (1000's)\",\n        col = \"violetred3\", \n        lwd=5)\nplot.ts(window(AirPassengers_multi, start=c(1950, 01), end=c(1954,12), frequency = 4), \n        main = \"PASSENGERS (1950-1955) - QUARTERLY DATA\",\n        xlab = \"1950-1955 (Quarterly data)\",\n        ylab = \"Passengers (1000's)\",\n        col = \"violetred3\", \n        lwd=5)"},{"path":"plot-time-series.html","id":"plot.xts","chapter":"7 Plot Time Series","heading":"7.3 plot.xts","text":"plot xts object can similarly use plot.xts function.can create xts object xts function (see previous chapter), already ts object, can also convert xts object using function .xtsBy using multi.panel=TRUE, multi.panel=FALSE can plot time series panel using different panels.subset data, order visualize focus just one part series, instead function window, write dates squared brackets examples .can also change frequency observations using specific functions xts library.using function periodicity can find frequency time series.function .period can re-sample data “seconds”, “minutes”, “hours”, “days”, “weeks”, “months”, “quarters”, “years”. can re-sample data higher lower frequency, lower higher one. instance, monthly data, can aggregate data quarterly yearly data, create weekly hourly time series.result .period function contain open (first) close (last) value given period, well maximum minimum new period, reflected new high low, respectively.can plot values, select value using square brackets comma, followed number column want plot (see table , 4 columns). notation way access columns (rows) data.frame matrix. write name data.frame matrix, squared brackets indicate index rows, first position comma, index columns, second position, comma. , instance, access value second column second row data.set “data”, can write data[2,2], access values third column first row, can write data[1,3]. leave blank space column row space, get values column rows. Therefore, writing data[,2], data[,3], get values column 2 3, respectively.can also re-sample calculate average (another statistics) new period. instance, example re-sample data year calculate average. also possible calculate statistics , instance, median, just writing “median” instead “mean”.can find additional details xts plot.xts reading help functions. link can find synthetic presentation functions xts library.","code":"\nAirPassengers_xts <- as.xts(AirPassengers)\n\nplot.xts(AirPassengers_xts,\n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         col = \"steelblue2\", \n         lwd=5)\nAirPassengers_multi_xts <- as.xts(AirPassengers_multi)\n\nplot.xts(AirPassengers_multi_xts,\n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = T)\nplot.xts(AirPassengers_multi_xts,\n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = F)\nplot.xts(AirPassengers_multi_xts[\"1950-01/1954-12\"], \n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = F)\nplot.xts(AirPassengers_xts[\"1950-01/1956-06\"], \n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = F)\nperiodicity(AirPassengers_xts)## Monthly periodicity from Jan 1949 to Dec 1960\nto.period(AirPassengers_xts, period=\"years\")##          AirPassengers_xts.Open AirPassengers_xts.High AirPassengers_xts.Low AirPassengers_xts.Close\n## Dec 1949                    112                    148                   104                     118\n## Dec 1950                    115                    170                   114                     140\n## Dec 1951                    145                    199                   145                     166\n## Dec 1952                    171                    242                   171                     194\n## Dec 1953                    196                    272                   180                     201\n## Dec 1954                    204                    302                   188                     229\n## Dec 1955                    242                    364                   233                     278\n## Dec 1956                    284                    413                   271                     306\n## Dec 1957                    315                    467                   301                     336\n## Dec 1958                    340                    505                   310                     337\n## Dec 1959                    360                    559                   342                     405\n## Dec 1960                    417                    622                   390                     432\nplot.xts(to.period(AirPassengers_xts, period=\"years\")[,2],\n         main = \"PASSENGERS\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = c(\"blue\", \"orange\", \"black\"),\n         multi.panel = F)\nindex_years <- endpoints(AirPassengers_xts, on = \"year\")\nAirPassengers_xts_year_avg <- period.apply(AirPassengers_xts, INDEX=index_years, FUN=mean)## NOTE: `period.apply(..., FUN = mean)` operates by column, unlike other math\n##   functions (e.g. median, sum, var, sd). Please use `FUN = colMeans` instead,\n##   and use `FUN = function(x) mean(x)` to take the mean of all columns. Set\n##   `options(xts.message.period.apply.mean = FALSE)` to suppress this message.\nplot.xts(AirPassengers_xts_year_avg,\n         main = \"PASSENGERS (Year Average)\",\n         ylab = \"Passengers (1000's)\", \n         lwd=5, lty=1,\n         col = \"blue\",\n         multi.panel = F)"},{"path":"plot-time-series.html","id":"ggplot","chapter":"7 Plot Time Series","heading":"7.4 ggplot","text":"Ggplot2 tidyverse library data visualization. can use create time series plots many types plot.upload dataset first, set appropriate time format date.place dataset (“elections_news”) inside ggplot function. Notice ggplot syntax similar tidyverse one, uses plus sign instead pipe one (%>%).create line plot ggplot necessary use geom_line function. function requires two parameters: data x-axis data y-axis. parameters written inside aes function. can also specify colors size line. using additional functions, plus sign, can also set labels x- y-axes, title, subtitle, caption plot. can also change overall aspect plot using one themes included library.can also plot one series. instance, can create two plots, use function grid.arrange, library gridExtra combine plots together.can also plot two two series plot.focus shorter time window, can use dplyr function filter. Besides filtering data, add function “scale_x_datetime”, control labels x-axis, specifying want use monthly labels.can also use ggplot annotate date. create annotations ggplot can use function “annotate”, label data point, “geom_segment”, trace lines connecting data points labels.can learn annotation ggplot : https://ggplot2-book.org/annotations.html\nlines : https://ggplot2.tidyverse.org/reference/geom_segment.html","code":"\nggplot(elections_news) +\n  geom_line(aes(x = date, y = ratio), color = \"snow4\", size = 0.5) +\n  ylab(\"News Articles\") +\n  xlab(\"Date\") +\n  labs(title = \"Time Series of News Articles on Elections\",\n       subtitle = \"Data from MediaCloud\",\n       caption = \"Data Analysis II\") +\n  theme_classic()## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n# install.packages(\"gridExtra\")\nlibrary(gridExtra)## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine\np1 <- elections_news %>%\n  ggplot() +\n  geom_line(aes(x = date, y = ratio), col = \"black\", size = 0.5) +\n  ylab(\"News Articles (ratio)\") +\n  xlab(\"Date\") +\n  ggtitle(\"MediaCloud Data on Elections (Daily)\") \n\np2 <- elections_news %>%\n  ggplot() +\n  geom_line(aes(x = date, y = count), col=\"red\", size=0.5) +\n  ylab(\"News Articles (count)\") +\n  xlab(\"Date\") +\n  ggtitle(\"MediaCloud Data on Elections (Daily)\") \n\ngrid.arrange(p1,p2)\nelections_news <- elections_news %>%\n  mutate(time_series_data_2 = count*2,\n         time_series_data_3 = count*4)\n\nggplot(elections_news) +\n  geom_line(aes(x = date, y = count), col = \"black\", size = 0.5) +\n  geom_line(aes(x = date, y = time_series_data_2), col = \"blue\", size = 0.5) +\n  geom_line(aes(x = date, y = time_series_data_3), col=\"red\", size=0.5) +\n  ylab(\"\") +\n  xlab(\"Date\") +\n  ggtitle(\"MediaCloud Data on Elections (Daily)\") \nelections_news %>%\n  filter(date >= \"2016-01-01\" & date < \"2017-01-01\") %>%\n  ggplot() +\n  geom_line(aes(x = date, y = count), col = \"black\", size = 0.5) +\n  scale_x_date(breaks=\"month\", date_labels =\"%Y-%m\") +\n  theme(axis.text.x = element_text(angle = 45, hjust=1)) +\n  ylab(\"News Articles (Ratio)\") +\n  xlab(\"Day\") +\n  ggtitle(\"MediaCloud Data on Elections (Monthly) - 2016\") \nggplot(elections_news) +\n  geom_line(aes(x = date, y = count), col = \"grey50\", size = 0.25) +\n  ylim(c(0, 15000)) +\n  # 1 EVENT\n  annotate(\"label\", x = as.Date(\"2018-11-01\"), y = 14500, \n           label = \"Midterm Elections\\nNovember 2018\", color = \"white\", fill=\"orange\", fontface=\"bold\", size=3) +\n  # add a line. You can also use an arrow by adding in geom_segment: \n  # arrow = line(length = unit(0.2, \"cm\"), ends = \"last\") \n  geom_segment(aes(x = as.Date(\"2018-11-01\"), xend = as.Date(\"2018-11-01\"), y = 0, yend = 14500), \n               color = \"orange\", size = 0.2, linetype = 1) +\n  # 2 EVENT\n  annotate(\"label\", x = as.Date(\"2019-05-01\"), y = 12000, \n           label = \"Pennsylvania Elections\\nMay 2019\", color = \"white\", fill=\"orange\", fontface=\"bold\", size=3) +\n  geom_segment(aes(x = as.Date(\"2019-05-01\"), xend = as.Date(\"2019-05-01\"), y = 0, yend = 12000), \n               color = \"orange\", size = 0.2, linetype = 1) +\n  # 3 EVENT\n  annotate(\"label\", x = as.Date(\"2016-11-01\"), y = 12000, \n           label = \"Presidential Elections\\nNovember 2016\", color = \"white\", fill=\"orange\", fontface=\"bold\", size=3) +\n  geom_segment(aes(x = as.Date(\"2016-11-01\"), xend = as.Date(\"2016-11-01\"), y = 0, yend = 12000), \n               color = \"orange\", size = 0.2, linetype = 1) +\n  # 4 EVENT\n  annotate(\"label\", x = as.Date(\"2020-11-01\"), y = 12000, \n           label = \"Presidential\\nElections\\nNovember\\n2020\", color = \"white\", fill=\"orange\", fontface=\"bold\", size=3) +\n  geom_segment(aes(x = as.Date(\"2020-11-01\"), xend = as.Date(\"2020-11-01\"), y = 0, yend = 12000), \n               color = \"orange\", size = 0.2, linetype = 1) +\n  ylab(\"News Articles\") +\n  xlab(\"Date\") +\n  labs(title = \"MediaCloud Data on Elections\",\n       subtitle = \"Peaks annotated with relevant political events\",\n       caption = \"Advanced Data Analysis\n                  University of Vienna\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        plot.subtitle = element_text(hjust = 0.5),\n        plot.caption = element_text(face = \"italic\")) +\n  theme_gray()## Warning in geom_segment(aes(x = as.Date(\"2018-11-01\"), xend = as.Date(\"2018-11-01\"), : All aesthetics have length 1, but the data has 2192 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing a single row.## Warning in geom_segment(aes(x = as.Date(\"2019-05-01\"), xend = as.Date(\"2019-05-01\"), : All aesthetics have length 1, but the data has 2192 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing a single row.## Warning in geom_segment(aes(x = as.Date(\"2016-11-01\"), xend = as.Date(\"2016-11-01\"), : All aesthetics have length 1, but the data has 2192 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing a single row.## Warning in geom_segment(aes(x = as.Date(\"2020-11-01\"), xend = as.Date(\"2020-11-01\"), : All aesthetics have length 1, but the data has 2192 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing a single row."},{"path":"structural-decomposition.html","id":"structural-decomposition","chapter":"8 Structural Decomposition","heading":"8 Structural Decomposition","text":"","code":""},{"path":"structural-decomposition.html","id":"components-of-a-time-series","chapter":"8 Structural Decomposition","heading":"8.1 Components of a time series","text":"time series can considered composed 4 main parts: trend, cycle, seasonality, irregular remainder/residual part.","code":""},{"path":"structural-decomposition.html","id":"trend-and-cycle","chapter":"8 Structural Decomposition","heading":"8.1.1 Trend and Cycle","text":"Trend component longest-term behavior time series. simplest model trend linear increase decrease, trend linear. AirPassengers time series clear upward, linear trend.","code":"\nlibrary(xts)\ndata(\"AirPassengers\")\nAirPassengers_xts <- as.xts(AirPassengers)\nplot.xts(AirPassengers_xts)"},{"path":"structural-decomposition.html","id":"stochastic-and-deterministic-trend","chapter":"8 Structural Decomposition","heading":"8.1.2 Stochastic and Deterministic Trend","text":"distinction deterministic stochastic trends.deterministic trend fixed function time. series deterministic trend, increase (decrease) value series function time. instance, may appear grow decline steadily time. deterministic trend can linear, well non linear. Deterministic trends plausible explanations (example, deterministic increasing trend data may related increasing population). series deterministic trend also called trend stationary.stochastic trend wanders shows change direction unpredictable times. Time series stochastic trend also said difference stationary. example stochastic trend provided -called random walk process.Random Walk particular time series process current values combinations previous ones (\\(x_t = x_{t-1} + w_t\\), \\(x_{t-1}\\) value immediately \\(x\\), \\(w_t\\) random component). resulting time series characterized discernible pattern time exactly predictable (stochastic trend). Starting initial point, process can generate different time series.paper effectiveness social distancing containing Covid-19 shows example stochastic trend complex, deterministic nonlinear trends represented polynomials.\nFigure 8.1: Figure 5 shows actual number Covid-19 cases recorded UK 17 June 2020. stochastic trend estimated earlier superimposed actual observations two deterministic nonlinear trends represented polynomials degrees 5 6. can see stochastic trend captures slow growth beginning sample period whereas two deterministic trends . stochastic trend also better capturing sharp increase represented observation number 72. (original caption)\ntrend component series often considered along cyclic one (trend-cycle). cyclical component represented fluctuations (rises falls) occurring fixed frequency. cycle component therefore different seasonal variation (see ) follow fixed calendar frequency.","code":"\nset.seed(111)\nRandom_Walk <- arima.sim(n = 500, model = list(order = c(0,1,0)))\nplot.ts(Random_Walk, ylab = expression(italic(x)[italic(t)]), main = \"Random Walk\")\nset.seed(555)\nRandom_Walk <- arima.sim(n = 500, model = list(order = c(0,1,0)))\nplot.ts(Random_Walk, ylab = expression(italic(x)[italic(t)]), main = \"Random Walk\")"},{"path":"structural-decomposition.html","id":"seasonality","chapter":"8 Structural Decomposition","heading":"8.1.3 Seasonality","text":"Seasonal component repeated pattern occurring fixed time period time year day week (frequency seasonality, always fixed known frequency). clear seasonal variation AirPassenger time series: bookings highest summer months June, July, August lowest autumn/winter months.possible plot distributions data months using function boxplot cycle, visualize increasing number passengers summer months. case, cycle used refer positions observation (yearly, case) cycle observations (every year considered cycle 12 observations).library forecast, R package provides methods tools displaying analysing time series forecasts, includes function create “polar” seasonal plot.example weekly seasonality can found COVID-19 statistics.\nFigure 8.2: Covid statistics (Google)\nCyclic seasonal variations can look similar. cyclic seasonal variations ‘peak--trough’ patterns. main difference seasonal patterns period successive peaks (troughs) constant, cyclical patterns distance successive peaks irregular.","code":"\nplot.xts(AirPassengers_xts[\"1954-01/1955-12\"])\nboxplot(AirPassengers ~ cycle(AirPassengers))\n# install.packages(\"forecast\")\nlibrary(forecast)## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo## This is forecast 8.24.0 \n##   Want to meet other forecasters? Join the International Institute of Forecasters:\n##   http://forecasters.org/\nggseasonplot(AirPassengers, polar=TRUE)"},{"path":"structural-decomposition.html","id":"residuals","chapter":"8 Structural Decomposition","heading":"8.1.4 Residuals","text":"irregular remainder/residual component random-like part series.general, fit mathematical models time series data, residual error series represents discrepancies fitted values, calculated model, data. good model encapsulates deterministic features time series, residual error series therefore appear realization independent random variables probability distribution.analysis residuals thus important judge fit model. case, residual error series appears realization independent random variables. Often random variable conceived Gaussian random variable. ’ll return topics last part chapter.","code":"\nAirPassengers_Random <- decompose(AirPassengers, type=\"multiplicative\")$random\npar(mfrow = c(2,1))\nplot(AirPassengers_Random, xlab=\"\", ylab=\"\")\nhist(na.omit(AirPassengers_Random), main = \"\", xlab=\"\", ylab=\"\")"},{"path":"structural-decomposition.html","id":"structural-decomposition-1","chapter":"8 Structural Decomposition","heading":"8.2 Structural decomposition","text":"Along analysis peaks (see previous chapter), analyzing time series based structural parts can important exploratory step. helps understanding likely causes series features formulate appropriate time series model. instance, case AirPassengers series, hypothesize increasing trend due rising prosperity aftermath Second World War, greater availability aircraft, cheaper flights due competition airlines, increasing population. seasonal variation, instead, seems coincide vacation periods.Decomposition methods try identify separate mentioned parts time series. Usually consider together trend cycle (trend-cycle) - longer-term changes series - seasonal factors - periodic fluctuations constant length happening specific calendar frequency).two main ways elements can combined together: additive multiplicative form:additive model (\\(x_{t} = m_{t} + s_{t} + z_{t}\\), \\(x_{t}\\) observed time series, \\(m_{t}\\) trend-cycle component, \\(s_{t}\\) seasonal component \\(z_{t}\\) residual) useful seasonal variation relatively constant timeThe multiplicative model (\\(x_{t} = m_{t} * s_{t} * z_{t}\\)) useful seasonal effects tends increase trend increases.different methods decompose time series. consider function decompose. function defined Classical Seasonal Decomposition Moving Averages. function decompose uses moving average (MA) approach filter data. Moving average classical approach extract trend time series averaging seasonal effects.","code":""},{"path":"structural-decomposition.html","id":"moving-average","chapter":"8 Structural Decomposition","heading":"8.2.1 Moving Average","text":"Moving average process replaces value \\(x_{t}\\) average current value \\(x_{t}\\) immediate neighbors past future. instance, possible calculate simple moving average using closest neighbors point, follows: \\(x_{t} = \\frac{1}{3} (x_{t-1} + x_{t} + x_{t+1})\\). called Centered Moving Average.number neighbors past future determined analyst also called width window. time window moving average chosen considering frequency data seasonal effects. instance, monthly data, supposed show monthly seasonality (instance, AirPassengers data passengers summer months), can averaged using period 12 months (six months point. Since even number months, calculation necessary. instance, moving average value July, calculated averaging average January December, average February January. R functions ).centered moving average example smoothing procedure applied retrospectively time series objective identifying underlying signal trend. Smoothing procedures usually use points time smoothed estimate calculated. consequence smoothed series points missing beginning end unless smoothing algorithm adapted end points. case monthly data, instance, moving average filter determines lost first last six months data.Smoothing procedures like moving average, allows main underlying trend emerge filtering seasonality noise, used get idea long-term underlying process time series.","code":"\nelections_news <- read_csv(\"data/elections-stories-over-time-20210111144254.csv\", \n                           col_types = cols(date = col_date(format = \"%Y-%m-%d\")))\n\nen <- as.xts(x = elections_news$ratio, order.by = elections_news$date)\n\nen2 <- rollmean(en, k = 2)\nen4 <- rollmean(en, k = 4)\nen8 <- rollmean(en, k = 8)\nen16 <- rollmean(en, k = 16)\nen32 <- rollmean(en, k = 32)\n\nenALL <- merge.xts(en, en2, en4, en8, en16, en32)\n\n# notice the NA elements increasing as the width of the moving average increase\nhead(enALL, 10) ##                    en        en2        en4        en8       en16 en32\n## 2015-01-01 0.01456405 0.01334178         NA         NA         NA   NA\n## 2015-01-02 0.01211950 0.01192277 0.01275765         NA         NA   NA\n## 2015-01-03 0.01172604 0.01217353 0.01266199         NA         NA   NA\n## 2015-01-04 0.01262102 0.01340120 0.01332611 0.01285129         NA   NA\n## 2015-01-05 0.01418138 0.01447869 0.01320110 0.01253790         NA   NA\n## 2015-01-06 0.01477600 0.01300100 0.01294493 0.01250958         NA   NA\n## 2015-01-07 0.01122600 0.01141117 0.01241382 0.01267144         NA   NA\n## 2015-01-08 0.01159633 0.01182664 0.01169304 0.01285992 0.01257820   NA\n## 2015-01-09 0.01205695 0.01197492 0.01214178 0.01262867 0.01239793   NA\n## 2015-01-10 0.01189290 0.01245691 0.01277492 0.01226887 0.01234396   NA\nplot.xts(enALL[\"2015-01-01/2016-01-01\"], multi.panel = T)"},{"path":"structural-decomposition.html","id":"decompose","chapter":"8 Structural Decomposition","heading":"8.2.2 Decompose","text":"apply function decompose, need ts object.Considering AirPassengers time series, since seasonal effect tends increase trend increases, can use multiplicative model.example additive model can use data “Seatbelts” dataset.residual part model (approximately) random, indicates model explained () significant patterns data (“signal”), leaving “noise”.can re-create original time series starting elements (don’t actually need , just illustrative purposes).","code":"\nAirPassengers_dec <- decompose(AirPassengers, type=\"multiplicative\")\nplot(AirPassengers_dec)\ndata(\"Seatbelts\")\nseatbelts <- Seatbelts[,5]\nplot.ts(seatbelts)\nseatbelts_dec <- decompose(seatbelts, type=\"additive\")\nplot(seatbelts_dec)\npar(mfrow = c(1,2))\nplot.ts(seatbelts_dec$random, main=\"Residuals\", ylab=\"\")\nhist(seatbelts_dec$random, breaks = 25, freq = F, main = \"Histogram\")\nlines(density(seatbelts_dec$random, na.rm = T), col=\"red\")\npar(mfrow=c(2,1))\nplot(AirPassengers_dec$trend * AirPassengers_dec$seasonal * AirPassengers_dec$random,  \n     xlim=c(1950, 1960), ylim=c(0,600), main = \"'Re-composed' series\", ylab=\"\")\n\nplot(AirPassengers, xlim=c(1950, 1960),ylim=c(0,600),  main = \"Original series\", ylab=\"\")"},{"path":"structural-decomposition.html","id":"compare-additive-and-multiplicative-models","chapter":"8 Structural Decomposition","heading":"8.2.3 Compare Additive and Multiplicative Models","text":"Sometimes hard choose additive multiplicative models. general, seasonality variation around trend-cycle component change proportionally level series (trend, average), multiplicative model works better. However, might difficult assess variability series time series plot.Exploratory methods, representing variability data box plots can help. following “custom” function (called ts.year.boxplot) takes argument time series ts, shows spread data year.multiplicative time series like AirPassengers shows clear increasing spread level series goes :box plots additive time series look regular. evident systematicBelow can find another function (called compare.decomposition.methods) compares additive multiplicative decomposition models applied ts series. creates plots residuals (time series plot, histogram, acf pacf plots) (roughly) measures total residuals residuals’ autocorrelation (lower values better). also creates plot showing different fit additive multiplicative model data, includes boxplot introduced . can try looking plots total autocorrelation measure get hints appropriate method.case, multiplicative model looks better.","code":"\n# Run the code to \"create\" the function \nts.year.boxplot <- function(ts) {\n  \n  ts %>%\n    fortify() %>%\n    mutate(x = substring(x, 1, 4)) %>%\n    mutate(x = as.Date(x, \"%Y\")) %>%\n    mutate(year = format(as.Date(cut(x, breaks =\"year\")), \"%Y\")) %>%\n    ggplot() + \n    geom_boxplot(aes(x=year, y=y, group=year)) +\n    ylab(\"\") +\n    theme_linedraw()\n}\n# Apply the function\nts.year.boxplot(AirPassengers)\nts.year.boxplot(seatbelts)\ncompare.decomposition.methods <- function(ts){\n\n    boxplot_ts <- ts %>%\n    fortify() %>%\n    mutate(x = substring(x, 1, 4)) %>%\n    mutate(x = as.Date(x, \"%Y\")) %>%\n    mutate(year = format(as.Date(cut(x, breaks =\"year\")), \"%Y\")) %>%\n    ggplot() + \n    geom_boxplot(aes(x=year, y=y, group=year)) +\n    ylab(\"\") +\n    theme_linedraw()\n  \n  # decompose the series with both the methods\n  xad <- decompose(ts, type = \"additive\")\n  ymu <- decompose(ts, type = \"multiplicative\")\n  \n  # plots\n  print(boxplot_ts)\n  \n  par(mfrow=c(1,1))\n  plot(ts, main=\"ADDITIVE (BLUE) - MULTIPLICATIVE (RED)\")\n  lines(xad$seasonal+xad$trend, col=\"blue\", lty=2)\n  lines(ymu$seasonal*ymu$trend, col=\"red\", lty=2)\n  \n  par(mfrow=c(1,2))\n  \n  am <- as.vector(xad$seasonal)+as.vector(xad$trend)\n  am <- ts(am, start = c(start(ts)[1], start(ts)[2]), \n         end = c(end(ts)[1], end(ts)[2]),\n         frequency = frequency(ts))\n  am <- ts.union(am, ts)\n  am <- am[complete.cases(am),]\n  \n  mm <- as.vector(ymu$seasonal)*as.vector(ymu$trend)\n  mm <- ts(mm, start = c(start(ts)[1], start(ts)[2]), \n         end = c(end(ts)[1], end(ts)[2]),\n         frequency = frequency(ts))\n  mm <- ts.union(mm, ts)\n  mm <- mm[complete.cases(mm),]\n  \n  plot(as.vector(am[,1]), as.vector(am[,2]), xlab=\"ADDITIVE\")\n  plot(as.vector(mm[,1]), as.vector(mm[,2]), xlab=\"MULTIPLICATIVE\")\n\n  par(mfrow=c(2,4))\n  plot(xad$random, main=\"\", ylab=\"ADDITIVE\")\n  hist(xad$random, main=\"\", ylab=\"\")\n  acf(na.omit(xad$random), main=\"\", ylab=\"\")\n  pacf(na.omit(xad$random), main=\"\", ylab=\"\")\n\n  plot(ymu$random, main=\"\", ylab=\"MULTIPLICATIVE\")\n  hist(ymu$random, main=\"\", ylab=\"\")\n  acf(na.omit(ymu$random), main=\"\", ylab=\"\")\n  pacf(na.omit(ymu$random), main=\"\", ylab=\"\")\n\n\n  # Sum of squares of residual auto-correlation (acf)\n  cat(paste(\n    \"###################################\\nTOTAL AUTOCORRELATION (ABSOLUTE VALUES)\\n###################################\",\n            \"\\nADITTIVE MODEL = \", \n    round(sum(abs(acf(na.omit(xad$random), plot = F)$acf)),2),\n    \"\\nMULTIPLICATIVE MODEL = \", \n    round(abs(sum(acf(na.omit(ymu$random), plot = F)$acf)),2),\n        \"\\n\\n###################################\\nSUM OF RESIDUALS (ABSOLUTE VALUES)\\n###################################\",\n    \"\\nADITTIVE MODEL = \", \n    round(sum(abs(scale(xad$random)), na.rm=T),2),\n    \"\\nMULTIPLICATIVE = \", \n    round(sum(abs(scale(ymu$random)), na.rm=T),2)\n    ))\n}\ncompare.decomposition.methods(AirPassengers)## ###################################\n## TOTAL AUTOCORRELATION (ABSOLUTE VALUES)\n## ################################### \n## ADITTIVE MODEL =  8.19 \n## MULTIPLICATIVE MODEL =  0.16 \n## \n## ###################################\n## SUM OF RESIDUALS (ABSOLUTE VALUES)\n## ################################### \n## ADITTIVE MODEL =  98.35 \n## MULTIPLICATIVE =  95.97"},{"path":"structural-decomposition.html","id":"adjust-time-series","chapter":"8 Structural Decomposition","heading":"8.3 Adjust time series","text":"Sometimes analyst interested trend seasonal variation data, might want remove , order let underlying process emerge clearly. times, components time series can misleading, leading inflated spurious correlations, can preferable remove proceding analysis.","code":""},{"path":"structural-decomposition.html","id":"seasonal-adjusted-data","chapter":"8 Structural Decomposition","heading":"8.3.1 Seasonal adjusted data","text":"common find seasonally adjusted data, time series seasonal component removed. happen quite often economics, instance (disciplines well), certain growing trends can considered trivial, explained based solid theory. parts series instead considered important, removing seasonal component allow emerge clearly, summarized Granger, C. W. (1978), Seasonality: causation, interpretation, implications. Seasonal analysis economic time series:Presumably, seasonal treated fashion, economically uninportant, dull, superficially easily explained, easy forecast , time, statistically important major contributor total variance many series. presence seasonal said obscure movements components greater economic significance. (…) can certainly stated , considering level economic variable, low frequency components (trend-cycle, ed.) usually statistically economically important. (…) dual importance, desirable view component clearly possible , thus, interference season removed. (…) preference seasonally adjusted data can clearly see position local trends place business cycle. certainly true series containing strong season, difficult observe local trends without seasonal adjustment.Moreover, seasonality can lead spurious correlations:(…) relationship pair economic variables analyzed, obviously possible obtain spurious relationship two series contain important seasonals. adjusting series, one possible source spurious relationship removed.However, adjust seasonality series application specific, sometimes part series can interest:Firms seasonal fluctuations demand products, example, may need make decisions based largely seasonal component (…) local government may try partially control seasonal fluctuations unemployment. (…) adjusted unadjusted data available can potential users gain maximum benefit effort goes collecting information.many different methods adjust data seasonality. simple approach based results decomposition process, consists subtracting (case additive decomposition model) seasonal component original series, dividing original series seasonal component (case multiplicative model).Adjusting seasonal variations makes possible observe potentially noteworthy fluctuations. case AirPassengers data, instance, seasonally adjusted plot shows clearly anomaly year 1960 noticeable raw data.can find another example data social media (can download art1, art2), consisting posts published pages news media.calculating simple correlation original series de-seasonalized series, can observed correlation coefficients changes.","code":"\ndata(\"Seatbelts\")\nseatbelts <- Seatbelts[,5]\nseatbelts_dec <- decompose(seatbelts, type=\"additive\")\n\nseatbelts_deseason <- seatbelts - seatbelts_dec$seasonal\n\nseat <- ts.intersect(seatbelts, seatbelts_deseason)\n\nplot.ts(seat, \n        plot.type = \"single\",\n        col = c(\"red\", \"blue\"),\n        main = \"Original (red) and Seasonally Adjusted Series (blue)\")\nAirPassengers_decompose <- decompose(AirPassengers, type=\"multiplicative\")\nAirPassengers_seasonal <- AirPassengers_decompose$seasonal\nAirPassengers_deseasonal <- (AirPassengers/AirPassengers_seasonal)\n\nAirPass <- ts.intersect(AirPassengers, AirPassengers_deseasonal)\n\nplot.ts(AirPass, \n        plot.type = \"single\",\n        col = c(\"red\", \"blue\"),\n        lty = c(3,1),\n        main = \"Original (red) and Seasonally Adjusted Series (blue)\")\nart1 <- read_csv(\"data/art1.csv\")\nart2 <- read_csv(\"data/art2.csv\")\n\nart1_summary <- art1 %>%\n  mutate(post_created_date = as.Date(post_created_date)) %>%\n  complete(post_created_date = seq.Date(min(post_created_date), \n           max(post_created_date), by = \"day\")) %>%\n  group_by(post_created_date) %>%\n  summarize(posts = n())\n\nart2_summary <- art2 %>%\n  mutate(post_created_date = as.Date(post_created_date)) %>%\n  complete(post_created_date = seq.Date(min(post_created_date), \n           max(post_created_date), by = \"day\")) %>%\n  group_by(post_created_date) %>%\n  summarize(posts = n())\n\nart1_ts <- ts(data = art1_summary$posts, frequency = 7)\nart2_ts <- ts(data = art2_summary$posts, frequency = 7)\n\nart <- ts.intersect(art1_ts, art2_ts)\n\nart1_dec <- decompose(art1_ts)\nart2_dec <- decompose(art2_ts)\n\nart1_seas <- art1_dec$seasonal\nart2_seas <- art2_dec$seasonal\n\nart1_deseas <- art1_ts - art1_seas\nart2_deseas <- art2_ts - art2_seas\n\nart_des <- ts.intersect(art1_deseas, art2_deseas)\n\nart_all <- ts.intersect(art, art_des)\n\nplot.ts(art_all, \n        plot.type = \"single\",\n        col = c(\"red\", \"blue\", \"red\", \"blue\"),\n        lty = c(3,3,1,1),\n        main = \"Original (red) and Seasonally Adjusted Series (blue)\")\ncor(art)[1,2]## [1] 0.9641655\ncor(art_des)[1,2]## [1] 0.7209823"},{"path":"structural-decomposition.html","id":"detrended-series","chapter":"8 Structural Decomposition","heading":"8.3.2 Detrended series","text":"series can adjusted seasonality, can also adjusted trend based reasons. similar process can also used remove trend data, particular case deterministic trend.case stochastic trend, instead, usual practice detrend data differencing. Differencing means taking first difference consecutive points time \\(x_t\\) - \\(x_{t-1}\\). way, resulting series represents relative change one point time another.Detrending time series can important applying statistical techniques, instance calculating correlation two time series. Time series trend component can reveal spurious correlations, since correlations may exist just two variables trending time. detrending time series, can appropriately measured change one time series time related change another time series.Detrending time series also used researchers consider irrelevant trend. case trend considered obvious characteristic process. instance, economists can take granted increasing trend GDP due inflation, thus may want “clean” data eliminate trivial trend. interested deviations growth, growth consider “normal” characteristic process.also statistical tests ascertain presence trend.monotonic trend can detected Mann–Kendall trend test. null hypothesis data come population independent realizations identically distributed. two sided test, alternative hypothesis data follow monotonic trend (read help: ?mk.test). function calculate test included package “trend”.","code":"\nRandom_Walk <- arima.sim(n = 500, model = list(order = c(0,1,0)))\n\nRandom_Walk_diff <- diff(Random_Walk)\n\nplot.ts(Random_Walk,\n        main = \"Random Walk\", \n        col = \"blue\", ylab=\"\")\nplot.ts(Random_Walk_diff, \n        main = \"Differenced Random Walk\", \n        col = \"blue\", ylab=\"\")\n# install.packages(\"trend\")\nlibrary(trend)\nmk.test(AirPassengers, alternative = \"greater\")## \n##  Mann-Kendall trend test\n## \n## data:  AirPassengers\n## z = 14.382, n = 144, p-value < 2.2e-16\n## alternative hypothesis: true S is greater than 0\n## sample estimates:\n##            S         varS          tau \n## 8.327000e+03 3.351643e+05 8.098232e-01"},{"path":"structural-decomposition.html","id":"other-decomposition-methods-in-r","chapter":"8 Structural Decomposition","heading":"8.3.3 Other Decomposition Methods in R","text":"many different methods decompose (adjust) time series. Besides classic decompose function, following can mentioned:STL (base-R stats library): Decompose time series seasonal, trend irregular components using loess[X11], method decomposing quarterly monthly data developed US Census Bureau Statistics Canada, [SEATS] methods, implemented seasonal package.","code":""},{"path":"structural-decomposition.html","id":"white-noise-and-stationarity","chapter":"8 Structural Decomposition","heading":"8.4 White Noise and Stationarity","text":"said residual part model (approximately) random, indicates model explained significant patterns data (“signal”), leaving “noise”.standard model independent random variation time series analysis known white noise (term coined article published Nature 1922, used refer series contained frequencies equal proportions, analogous white light). charts show white noise process looks like.introduced concept deterministic stochastic trend, said series showing first type trend also called trend stationary, series showing second type trend also called difference stationary. names refer concept stationarity.process stationary homogeneous, , distinguished points times , words, statistical qualities point time. less stringent definition stationarity (strict weak stationarity), used practical purposes -called weak-stationarity. sense, time series said stationary :trend (systematic change mean, , time invariant mean), seasonality (periodic variations);change variance time (time invariant variance);auto-correlation (’ll return topic next chapters)White noise example stationary time series. can see chart , white noise time series pretty regular, mean always (0) changes variance time: plot looks much point time!Also differencing, series can achive stationary form. case, particular, series stochastic trend, also called difference-stationary, exactly differencing become stationary.process stationarity reached also called Pre-Whitening, can used pre-processing phase conducting correlation regression analysis:form(s) serial dependency best account series identified, removed series. called prewhitening used produce series “white noise” process (.e., process free serial dependency value statistically independent values series). pre-whitening accomplished values series can correlated , used predict, predicted , values contemporaneous time series (usually also pre-whitened) representing variables interest. removing serial dependency, pre-whitening process makes analyses free correlated errors. also removes possibility common temporal trend pattern confounding explanation observed association two variable series (VanLear, “Time Series Analysis”)logic behind process importance white noise also well explained sentences:“residual” part data, indeed, can used dependent variable, giving analyst confidence time series properties data account observed correlation covariates dependent variable. univariate context, white noise process important like “recover” data – stripping away ways univariate\nseries can essentially explain . removing time series properties data, leaving white noise, series can explained sources variation. Another way conceptualize white noise exogenous portion data-generating process. data series function forces cause series rise fall (independent variables normally include test hypotheses) time series properties lead forces \nless “sticky”. filter away time series properties, left forces driving data higher lower. often refer forces shocks – shocks reverberate data, sometimes short spell long spell even infinitely. goal time series analysis separately model time series properties (reverberations) shocks (.e., white noise) can captured. (Box-Steffensmeier, J. M., Freeman, J. R., Hitt, M. P., & Pevehouse, J. C. (2014). Time series analysis social sciences. Cambridge University Press.)","code":"\nplot.ts(Random_Walk_diff, \n        main = \"Differenced Random Walk\", \n        col = \"blue\", ylab=\"\")"},{"path":"correlations-and-arima.html","id":"correlations-and-arima","chapter":"9 Correlations and ARIMA","heading":"9 Correlations and ARIMA","text":"","code":""},{"path":"correlations-and-arima.html","id":"auto-correlation-acf-and-pacf","chapter":"9 Correlations and ARIMA","heading":"9.1 Auto-Correlation (ACF and PACF)","text":"previous chapter said time series said stationary :trend (systematic change mean, , time invariant mean), seasonality (periodic variations);change variance time (time invariant variance);auto-correlation (’ll return topic next chapters)Auto-correlation serial correlation important characteristic time series data can defined correlation variable different time points.Autocorrelation many consequences. prevents us use traditional statistical methods linear regression, assume observations independent . presence autocorrelation, estimated standard errors parameter estimates tend less true value. lead erroneously high statistical significance attributed statistical tests (p values smaller ).section introduce important tool diagnosis properties time series, including autocorrelation: correlogram. accurate study correlogram common step many time series analysis procedures.","code":""},{"path":"correlations-and-arima.html","id":"correlogram-acf-and-pacf","chapter":"9 Correlations and ARIMA","heading":"9.1.1 Correlogram: ACF and PACF","text":"correlogram chart presents one two statistics:autocorrelation function (ACF).ACF statistic measures correlation \\(x_t\\) \\(x_{t+k}\\) k number lead periods future. measures correlation two points based given interval. strictly equivalent Pearson product moment correlation. R, ACF calculated visualized function “acf”;partial autocorrelation function (PACF). PACF(k) measure correlation times series observations k units apart, correlation intermediate lags controlled “partialed” . words, PACF measures correlation \\(x_t\\) \\(x_{t+k}\\) stripped effect intermediate x’s. R, PACF calculated visualized function “pacf”. useful detect correlations evident ACF.Let’s consider, example, correlogram random walk process. know particular time series process current values combinations previous ones (\\(x_t = x_{t-1} + w_t\\), \\(x_{t-1}\\) value immediately x, \\(w_t\\) random component). resulting time series characterized discernible pattern time exactly predictable (stochastic trend). ACF random walk time series, indeed, shows correlation values series: even values close notwithstanding correlated.Instead, PACF, removes correlations intermediate values, shows correlation lag 1, , shows overall correlation depends consequent values. dotted blue lines signal boundaries statistical significance.can clearly visualize auto-correlation using simple scatterplot, plotting two consecutive lines points.ACF PACF white noise process different. know white noise stationary process, without distinguishable points time correlation points. Indeed, ACF white noise shows correlation (line statistical significance zero, nothing worried , since just means point correlated ).PACF can see nothing dotted line (means nothing statistically significant).plot two consecutive lists points using scatterplot, can see serial correlation (pattern visible):","code":"\nRandom_Walk <- arima.sim(n = 499, model = list(order = c(0,1,0)))\n\nacf(Random_Walk)\npacf(Random_Walk)\nplot(Random_Walk[1:499], Random_Walk[2:500])\nWhite_Noise <- arima.sim(n = 500, model = list(order = c(0,0,0)))\n\nacf(White_Noise)\npacf(White_Noise)\nplot(White_Noise[1:499], White_Noise[2:500])"},{"path":"correlations-and-arima.html","id":"arima-models","chapter":"9 Correlations and ARIMA","heading":"9.2 ARIMA models","text":"ACF PACF plots can used diagnose main characteristics time series find proper statistical model. talk univariate models, since models describe single time series. Univariate time series can modeled Auto Regressive (AR), Integrated (), Moving Average (MA) processes. models synthesized using acronym ARIMA. seasonal (S) component also taken account, also use acronym SARIMA.","code":""},{"path":"correlations-and-arima.html","id":"auto-regressive-ar-models","chapter":"9 Correlations and ARIMA","heading":"9.2.1 Auto Regressive (AR) models","text":"just said time series often characterized auto-correlation, can clearly deduce can model using regression model, , regressing time series past values. way auto-regressive model: regression \\(x_{t}\\) past terms \\(x_{t-k}\\) series.time series analysis, past terms \\(x_{t-k}\\) series called lags. lagged values time series delayed values, delay can arbitrary amount time \\(k\\). instance, considering simple series 4 data points distributed time \\({t+0}\\) (first data point) time \\({t+3}\\) (last data point) \\({x_{t+0}, x_{t+1}, x_{t+2}, x_{t+3}}\\), corresponding lagged series, assuming \\(k=1\\), \\({NA, x_{t+0}, x_{t+1}, x_{t+2}}\\). Notice first data point missing since data point behind , data points shifted one time point ahead.auto-regressive (AR) model can described follows (\\(\\alpha\\) coefficients, \\(t\\) time points, \\(w\\) random component white noise):\\[\n{x_t} = \\alpha x_{t-1} + \\alpha x_{t-2} + \\alpha x_{t-k} + {w_t}\n\\]ACF autoregressive process typically shows slow gradual decay autocorrelation time.PACF autoregressive process shows peak correspondence order model. case AR(1) peak time 1.case AR(3) peak time 1, 2, 3.Now can see random walk process seen particular case auto-regressive model. random walk process, value previous one plus random part:\\[\nx_t = x_{t-1} + w_t\n\\]Thus point \\(x_t\\) correlated previous one \\(x_{t-k}\\) lag value \\(k\\) equal 1 (\\({k=1}\\)). Therefore, random walk process auto-regressive model order 1, since just 1 lag taken consideration auto-regressive model (\\(\\alpha = 1\\)). order auto-regressive model indicated parenthesis, e.g.: AR(1).AR process can different characteristics (different ACF PACF) based parameters.","code":"\nAR_1 <- arima.sim(n = 500, list(order = c(1,0,0), ar = 0.90))\nplot(AR_1, main = \"AR(1)\")\nacf(AR_1)\npacf(AR_1)\nAR_3 <- arima.sim(n = 500, list(order = c(3,0,0), ar = c(0.3, 0.3, 0.3)))\nplot(AR_3, main = \"AR(3)\")\npacf(AR_3)"},{"path":"correlations-and-arima.html","id":"moving-average-ma-models","chapter":"9 Correlations and ARIMA","heading":"9.2.2 Moving Average (MA) models","text":"already know Moving Average method smooth time series detect trend. referring Moving Average process (MA), refer process values series function weighted average past errors. terms, moving average (MA) process linear combination current white noise term \\(q\\) recent past white noise terms:\\[\n{x_t} = w_t + \\beta w_{t-1} + ... + \\beta w_{t-q}\n\\]order MA process indicates lags white noise taken account model (e.g: MA(3)).ACF plot MA process shows clear cut-term corresponding order ot process. different ACF AR process, shows gradual decay.PACF MA process shows --movement shut , instead tapers toward 0 manner.","code":"\nMA_3 <- arima.sim(n = 500, list(order = c(0,0,3), ma = c(0.3, 0.3, 0.3)))\nplot(MA_3, main = \"MA(3)\")\nacf(MA_3)\npacf(MA_3)"},{"path":"correlations-and-arima.html","id":"integrated-i-process","chapter":"9 Correlations and ARIMA","heading":"9.2.3 Integrated (I) process","text":"integrated process non-stationary time series process becomes stationary transformed differencing. words, integrated process difference-stationary process, process stochastic trends (see previous chapter).","code":"\nI_1 <- arima.sim(n = 500, list(order = c(0,1,0)))\nplot(I_1, main = \"I(1)\")\nplot(diff(I_1), main = \"I(1) after 'differencing'\")"},{"path":"correlations-and-arima.html","id":"seasonal-s-models","chapter":"9 Correlations and ARIMA","heading":"9.2.4 Seasonal (S) models","text":"already introduced seasonal model. instance, dataset showing seasonal component AirPassengers. seasonality appears yearly fluctuations ACF spikes occurring 12 months PACF.","code":"\ndata(\"AirPassengers\")\n\n# this function par(mfrow=c(..., ...))\n# is used to combine more than one plot\n# in the same frame. The two numerical values\n# indicates number of rows and columns\n# the frame is made of\npar(mfrow=c(1,2))\n\nacf(AirPassengers, lag.max = 48)\npacf(AirPassengers, lag.max = 48)"},{"path":"correlations-and-arima.html","id":"fit-sarima-models","chapter":"9 Correlations and ARIMA","heading":"9.2.5 Fit (S)ARIMA models","text":"examples represent simple processes, real time series often result complex mixtures different types process, therefore complex identify appropriate model data.popular methods find appropriate model Box-Jenkins method, recursive process involving analysis time series, guess possible (S)ARIMA models, fit hypothesized models, meta-analysis determine best specification. best-fitting model \nfound, correlogram residuals verified white noise.Box-Jenkins method time-consuming requires expertise. ACF/PACF can also become difficult read case complex models, appropriate interpretation require lot expertise well. Fortunately, experts developed automated methods allow us automatically found fit ARIMA model. case auto.arima function implemented forecast package (package time series analysis especially forecasting, developed Rob J. Hyndman, professor statistics time series analysis expert).said , evaluate fit model analyze residuals, ascertain behave white noise. object resulting function auto.arima slot including residuals. ascertain residuals white noise can plot ACF PACF (spike significant) also histogram.forecast package also implements function checkresiduals create nice complete plots residual diagnostics using simple function.Besides creating plots, function calulate Ljung-Box test (default), Breusch-Godfrey test (specify test=“BG” inside function):Ljung-Box test (also Breusch–Godfrey test) diagnostic tool, applied residuals time series fitting ARIMA model, test lack fit. test examines autocorrelations residuals. significant autocorrelations, can concluded model exhibit significant lack fit. pass test, p-value significance level (usually 0.05)","code":"\nset.seed(7623)\narima_112 <- arima.sim(n = 500, list(order = c(1,1,2), ar = 0.8, ma = c(0.7, 0.2)))\nplot(arima_112, main = \"ARIMA(1,1,2)\")\n# Install the package if you haven't installed it yet\n# install.packages(\"forecast\")\nlibrary(forecast)\n\narima_fit <- auto.arima(arima_112)\n\narima_fit## Series: arima_112 \n## ARIMA(1,1,2) \n## \n## Coefficients:\n##          ar1     ma1     ma2\n##       0.7649  0.7387  0.2484\n## s.e.  0.0356  0.0528  0.0534\n## \n## sigma^2 = 1.035:  log likelihood = -717.9\n## AIC=1443.8   AICc=1443.88   BIC=1460.65\nlayout(matrix(c(1,2,3,3), nrow = 2))\nacf(arima_fit$residuals)\npacf(arima_fit$residuals)\nhist(arima_fit$residuals, main = \"Histogram of residuals\")\ncheckresiduals(arima_fit)## \n##  Ljung-Box test\n## \n## data:  Residuals from ARIMA(1,1,2)\n## Q* = 5.8977, df = 7, p-value = 0.5517\n## \n## Model df: 3.   Total lags used: 10"},{"path":"correlations-and-arima.html","id":"sarima","chapter":"9 Correlations and ARIMA","heading":"9.2.5.1 SARIMA","text":"fit model AirPassengers dataset, seasonal component, find Seasonal Autoregressive Integrated Moving Average model (SARIMA). seasonal component AirPassenger dataset evident plot series ACF PACF. forecast package useful function ggtsdisplay plot time series along ACF PACF.seasonal part ARIMA model consists terms similar non-seasonal components model, involves lagged values seasonal period.","code":"\nggtsdisplay(AirPassengers)\nAirPassengers_sarima <- auto.arima(window(AirPassengers))\nAirPassengers_sarima## Series: window(AirPassengers) \n## ARIMA(2,1,1)(0,1,0)[12] \n## \n## Coefficients:\n##          ar1     ar2      ma1\n##       0.5960  0.2143  -0.9819\n## s.e.  0.0888  0.0880   0.0292\n## \n## sigma^2 = 132.3:  log likelihood = -504.92\n## AIC=1017.85   AICc=1018.17   BIC=1029.35"},{"path":"correlations-and-arima.html","id":"forecasting","chapter":"9 Correlations and ARIMA","heading":"9.2.5.2 Forecasting","text":"Based ARIMA models found, can also try forecast future values. can use function forecast homonym library. instance, try forecast values AirPassenger dataset next four years.","code":"\nAirPassengers_forecast <- forecast(AirPassengers_sarima, h=48, level = 90)\nplot(AirPassengers_forecast, main = \"AirPassengers forecast\")\nAirPassengers_sarima <- auto.arima(AirPassengers)"},{"path":"correlations-and-arima.html","id":"cross-correlation","chapter":"9 Correlations and ARIMA","heading":"9.3 Cross-correlation","text":"Cross-correlation correlation (lagged) values time series values another series. Similarly ACF PACF, specific plot shows cross-correlation two time series, specific R function: ccf.cross-correlation can useful understand wich lagged values X series can used predict values Y series, thus used, instance, time series regression model.Unfourtunately, problem cross-correlation function , said preceding sections, autocorrelated data difficult assess dependence two processes, possible find spurious correlations.Thus, pertinent disentangle linear association X Y autocorrelation. useful device prewhitening. prewhitening method works follows:determine ARIMA time series model X-variable, store residuals model;fit ARIMA X-model Y-variable, keep residuals;examines CCF X Y model residuals.can implement procedure writing necessary code, using library forecast, also, alternatively, library TSA.make example, apply method two simulated two series. Y-variable created way correlated lagged values time \\(x_{t-3}\\) \\(x_{t-4}\\). Therefore, find correlation lags.cross-correlation applied original series results plot everything seems correlated. “real” correlation \\(x_{t-3}\\) \\(x_{t-4}\\) discernible .using forecast library, can calculate pre-withened ccf follows:Now, ’s clear X-variable correlated Y-variable \\(x_{t-3}\\) \\(x_{t-4}\\).previous steps show detail steps involved pre-whitening strategy, possible use original series prewhiten function TSA library. Although function can take, argument, pre-fitted ARIMA model, greater advantage can take care necessary steps prewithen series. particular, model specified, library automatically applies simple AR model. Although model can just approximation “true” model (can complex), approximation can enough pre-whiten series find proper cross-correlation (, also simpler approximate model can job).","code":"\nx_series <- arima.sim(n = 200, list(order = c(1,1,0), ar = 0.7, sd=1))\nz <- ts.intersect(x_series, stats::lag(x_series, -3), stats::lag(x_series, -4)) \ny_series <- 15 + 0.8*z[,2] + 1.5*z[,3] + rnorm(197,0,1)\nccf(x_series, y_series, na.action = na.omit)\n# fit an ARIMA model\nx_model <- auto.arima(x_series)\n# keep the residuals (\"white noise\")\nx_residuals <- x_model$residuals\n\n# fit the same ARIMA model to the Y-series\n# by using the \"Arima\" function in forecast\ny_model <- Arima(y_series, model=x_model)\n# keep the residuals\ny_filtered <- residuals(y_model)\n\n# apply the ccf to the residuals\nccf(x_residuals, y_filtered)\n# install.packages(\"TSA\")\nlibrary(TSA)## Registered S3 methods overwritten by 'TSA':\n##   method       from    \n##   fitted.Arima forecast\n##   plot.Arima   forecast## \n## Attaching package: 'TSA'## The following object is masked from 'package:readr':\n## \n##     spec## The following objects are masked from 'package:stats':\n## \n##     acf, arima## The following object is masked from 'package:utils':\n## \n##     tar\nprewhiten(x_series, y_series)"},{"path":"correlations-and-arima.html","id":"examples-in-literature","chapter":"9 Correlations and ARIMA","heading":"9.4 Examples in literature","text":"examples exemplify use ARIMA Cross-Correlation scientific literature, specific reference communication science.Scheufele, B., Haas, ., & Brosius, H. B. (2011). Mirror molder? study media coverage, stock prices, trading volumes Germany. Journal Communication, 61(1), 48-70, authors investigate “short-term relationship media coverage, stock prices, trading volumes eight listed German companies”, using ARIMA cross-correlation, particular asking:RQ2: cross-lagged correlations media coverage stock prices trading volumes differ according amount valence coverage?\nRQ3: cross-lagged correlations media coverage stock prices trading volumes differ according type media (Financial Web sites, daily newspapers, stock market TV shows) reports company stock?answer questions, authors made use time series analysis. particular, :estimated cross-lagged correlations media coverage stock prices trading volumes, respectively. Basically, two steps time-series analysis can distinguished: () first step, media time-series time-series trading volumes adjusted ARIMA (Autoregressive Integrated Moving Average) modeling separately. differences original time-series ARIMA model called residuals used analysis. Like ordinary least squares regression, residuals auto-correlated. residuals auto-correlated, time-series analysis speaks White Noise. modeling technique called prewhitening necessary avoid spurious correlations. (…) (b) next step, cross-correlations adjusted media time-series adjusted stock series calculated. (…) coefficient expresses strength correlation, whereas lags offer insight dynamics: Correlations positive (negative) lags indicate changes media coverage proceeded (succeeded) shifts stock prices trading volumes.Groshek, J. (2010). time-series, multinational analysis democratic forecasts Internet diffusion. International Journal Communication, 4, 33, author examines democratic effects Internet shown using macro- level, cross-national data sequence time–series statistical tests:study relies principally macro-level time–series democracy data historical sample includes 72 countries, reaching back far 1946 cases, least 1954 2003. sample, sequence ARIMA (autoregressive integrated moving average) time–series regressions modeled country least 40 years prior 1994. models used generate statistically-forecasted democracy values country, year 1994 2003. 95% confidence interval upper lower democracy score constructed around forecasted values using dynamic mean squared errors. actual democracy scores country year 1994 2003 compared upper lower values confidence interval.\nevent actual democracy level country greater upper value forecasted democracy score time period 1994 2003, Internet diffusion investigated case studies possible causal mechanism.terms, author used forecasting approach predict values series 1994 2003, order find statistically significant differences predicted actual values. discrepancies interpreted caused factors present past, possibly introduction Internet.study found , based results 72 countries reported , diffusion Internet considered democratic panacea, rather component contemporary democratization processes","code":""},{"path":"regression.html","id":"regression","chapter":"10 Regression","heading":"10 Regression","text":"chapter going see conduct regression analysis time series data.Regression analysis used estimating relationships dependent variable (DV) (also called outcome response) one independent variables (IV) (also called predictors explanatory variables).standard regression model \\(Y\\) = \\(\\beta\\) + \\(\\beta x\\) + \\(\\epsilon\\) time component. Differently, time series regression model includes time dimension can written, simple general formulation, using just one explanatory variable, follows:\\[\ny_t = \\beta_0 + \\beta_1x_t + \\epsilon_t\n\\]equation, \\(y_t\\) time series try understand/predict (dependent variable (DV)), \\(\\beta_0\\) intercept (constant value represents expected mean value \\(y_t\\) \\(x_t = 0\\)), coefficient \\(\\beta_1\\) slope, representing average change \\(y\\) one unit increase \\(x\\) (independent variable (IV) explanatory variable), \\(\\epsilon_t\\) time series residuals (error term).multiple regression, one explanatory variable, can written follows:\\[\ny_t = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t} + ... + \\beta_kx_{k,t} + \\epsilon_t\n\\]","code":""},{"path":"regression.html","id":"static-and-dynamic-models","chapter":"10 Regression","heading":"10.1 Static and Dynamic Models","text":"time series analysis perspective, general distinction can made “static” “dynamic” regression models:static regression model includes just contemporary relations explanatory variables (independent variables) response (dependent variable). model appropriate expected value response changes immediately value explanatory variable changes. Considering model \\(k\\) independent variables {\\(x_1\\), \\(x_2\\), …, \\(x_k\\)}, static (multiple) regression model, form just seen :\\[\ny_t = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t} + ... + \\beta_kx_{k,t} + \\epsilon_t\n\\]\\(\\beta\\) coefficient models instant change conditional expected value response variable \\(y_t\\) value \\(x_{k,t}\\) changes one unit, keeping constant predictors (.e.: \\(x_{k,t}\\)):dynamic regression model includes relations current lagged (past) values explanatory (independent) variables, , expected value response variable may change change values explanatory variables.\\[\n\\begin{aligned}\ny_t = \\beta_0  & + \\beta_{10}x_{1,t} + \\beta_{11}x_{1,t-1} + ... + \\beta_{1m}x_{1,t-m} \\\\\n& + \\beta_{20}x_{2,t} + \\beta_{21}x_{2,t-1} + ... + \\beta_{2m}x_{2,t-m} \\\\\n& + \\dots \\\\\n& + \\beta_{k0}x_{k,t} + \\beta_{k1}x_{k,t-1} + ... + \\beta_{km}x_{k,t-m} \\\\\n& + \\epsilon_t \\\\\n\\end{aligned}\n\\]Despite differences two analytic perspectives, term dynamic regression also used, literature, general way refer regression models autocorrelated errors (also used analyze contemporary relations variables).","code":""},{"path":"regression.html","id":"regression-models","chapter":"10 Regression","heading":"10.2 Regression models","text":"Except possible use lagged regressors, typical time series, described statistical models standard regression models, commonly used cross-sectional data.Standard linear regression models can sometimes work well enough time series data, specific conditions met. Besides standard assumptions linear regression1, careful analysis done order ascertain residuals autocorrelated, since can cause problems estimated model.chapter ’ll see deal autocorrelated residuals. However, even , important series stationary, order avoid possible spurious correlations.","code":""},{"path":"regression.html","id":"stationarity","chapter":"10 Regression","heading":"10.2.1 Stationarity","text":"already discussed stationarity previous chapters. can observe time series can nonstationary due different reasons, thus different strategies can employed stationarize data.instance, nonstationary series can series unequal variance time. common way try fix problem applying log-transformation.Another reason nonstationarity periodic variation due seasonality (regular fluctuations time series follow specific time pattern, e.g.: social media activity week-ends, Christmas effect consumption, etc.).remove seasonal pattern, might want use seasonally-adjusted time series. Otherwise, create dummy variable seasonal period (, variable follows seasonal pattern data order account, model, fluctuations).important reason nonstationarity also presence trend data. stochastic trends deterministic trends. Deterministic trends fixed function time, stochastic trends change unpredictable way.Series deterministic trend also called trend stationary can stationary around deterministic trend, possible achieve stationarity removing time trend. trend stationary processes, shocks process transitory process mean reverting.Processes stochastic trend also called difference stationary can become stationary differencing. series stochastic trends see shocks permanent effects.dealing deterministic trend, might want work detrended series.Otherwise, regression analysis, common add dummy variable consisting value increases time, account linear deterministic time trend. time-count variable remove deterministic trend dependent variable, allowing predictors explain remaining variance.series stochastic trend, can achieve stationarity differencing.","code":"\nlibrary(xts)\n\nelections_news <- read.csv(\"data/elections-stories-over-time-20210111144254.csv\")\nelections_news$date <- as.Date(elections_news$date)\n\nelections_news <- xts(elections_news$count, order.by = elections_news$date)\nelections_news_log <- log(elections_news+1)\nelections_news_xts <- merge.xts(elections_news, elections_news_log)\n\nplot.xts(elections_news_xts, col = c(\"blue\", \"red\"),\n         multi.panel = TRUE, yaxis.same = FALSE,\n         main = \"Original vs Log-transformed series\")\n# load the ts dataset AirPassenger\ndata(\"AirPassengers\")\n\n# remove seasonality from a multiplicative model\nAirPassengers_decomposed <- decompose(AirPassengers, type=\"multiplicative\")\nAirPassengers_seasonal_component <- AirPassengers_decomposed$seasonal\nAirPassengers_seasonally_adjusted <- AirPassengers/AirPassengers_seasonal_component\n\npar(mfrow=c(1,2))\nplot.ts(AirPassengers, col = \"blue\", main = \"Original series\")\nplot.ts(AirPassengers_seasonally_adjusted, col = \"blue\",  \n        main = \"Seasonally-adjusted series\", \n        ylab = \"Seasonally-adjusted values\")\n# remove the trend from a multiplicative model\nAirPassengers_decomposed <- decompose(AirPassengers, type=\"multiplicative\")\nAirPassengers_trend_component <- AirPassengers_decomposed$trend\nAirPassengers_detrended <- AirPassengers/AirPassengers_trend_component\n\npar(mfrow=c(1,2))\nplot.ts(AirPassengers, col = \"blue\", main = \"Original series\")\nplot.ts(AirPassengers_detrended, col = \"blue\",  \n        main = \"Detrended series\", \n        ylab = \"Detrended values\")\n# create a simulate series\nset.seed(1312)\ntoy_data <- arima.sim(n = 100, model = list(order = c(0,0,0)))\n\n# add a deterministic trend to the series\ntoy_data_trend <- toy_data + 0.2*1:length(toy_data)\n\npar(mfrow=c(1,3))\nplot.ts(toy_data, main = \"Original series\")\nplot.ts(toy_data_trend, main = \"Series with Trend\")\n\ndummy_trend <- 1:length(toy_data_trend)\nlm_toydata <- lm(toy_data_trend ~ dummy_trend)\nplot.ts(lm_toydata$residuals, main = \"Residuals (detrended)\")\nset.seed(111)\nRandom_Walk <- arima.sim(n = 500, model = list(order = c(0,1,0)))\n\nRandom_Walk_diff <- diff(Random_Walk)\n\npar(mfrow=c(1,2))\nplot.ts(Random_Walk,\n        main = \"Random Walk\", \n        col = \"blue\", ylab=\"\")\n\nplot.ts(Random_Walk_diff, \n        main = \"Differenced Random Walk\", \n        col = \"blue\", ylab=\"\")"},{"path":"regression.html","id":"tests-for-stochastic-and-deterministic-trend","chapter":"10 Regression","heading":"10.2.1.1 Tests for Stochastic and Deterministic Trend","text":"correct detrending method depends type trend. First differencing appropriate intergrated (1) time series time-trend regression appropriate trend stationary (0) time series.case deterministic trend, differencing incorrect solution, detrending series function time (regressing series variable time saving residuals) correct solution. Differencing none required (-differencing) may induce dynamics series part data-generating process (instance, create first-order moving average process).Specific statistical tests developed distinguish two types trends. particular, unit root tests stationary test can used determine trending data first differenced regressed deterministic functions time render data stationary.Considering simple model like following, \\(Td\\) deterministic linear trend \\(z_t\\) autoregressive process order 1 AR(1). difference process stochastic deterministic trend can traced back parameter \\(|\\phi|\\): \\(|\\phi| = 1\\), \\(z_t\\) stochastic trend \\(y_t\\) integrated process (1) drift (-called “drift” refers presence constant term, case \\(\\kappa\\)). \\(\\phi < 1\\), process integrated ((0)) \\(y_t\\) exhibits deterministic trend2:\\[\n\\begin{aligned}\n& y_t = Td_t + z_t \\\\\n& Td_t = \\kappa + \\delta_t \\\\\n& z_t = \\phi z_{t-1} + \\epsilon_t, \\ \\epsilon_t \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\nLet’s simulate visualize equation (\\(y_t = \\kappa + \\delta_t + \\phi z_{t-1} + \\epsilon_t\\)):Unit root tests aimed testing null hypothesis \\(|\\phi| = 1\\) (difference stationary), alternative hypothesis \\(|\\phi| < 1\\) (trend stationary).Stationarity tests take null hypothesis \\(y_t\\) trend stationary, based testing moving average element \\(\\Delta z_t\\) (\\(\\Delta\\) represents operation differencing).$$\\(\\Delta z_t\\) can also written :\\[\n\\Delta \\epsilon_t = \\phi \\Delta z_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}\n\\]\\(\\theta = -1\\). , series trend stationary, taking first difference results overdifferencing creation moving average (MA) term \\(\\theta \\epsilon_{t-1}\\). creation moving average element, missing original series, also differencing trend-stationary process problematic.","code":"\nset.seed(123)\nt <- 1:500 \nkappa <- 5 # costant term (or \"drift\")\ndelta <- 0.1 \nepsilon <- function(n){ # function for the error term\n    rnorm(n = 500, mean = 0, sd = 0.8)\n    } \n\ny_I1 <- kappa + (delta * t) + arima.sim(n=499, list(order = c(0,1,0)), \n                                        rand.gen = epsilon)\ny_I0 <- kappa + (delta * t) + arima.sim(n=500, list(order = c(1,0,0), ar = 0.8),\n                                        rand.gen = epsilon)\n\nts_y <- ts.intersect(y_I1, y_I0)\n\nplot.ts(ts_y, plot.type = \"single\", \n        lty=c(1,3), col=c(\"red\", \"blue\"),\n        main = \"Stochastic w/ drift (red) Deterministic Trend (blue)\",\n        ylab=\"\")"},{"path":"regression.html","id":"kpss-test","chapter":"10 Regression","heading":"10.2.1.1.1 KPSS Test","text":"test verify series trend stationary Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. one commonly used stationarity test, implemented library tseries (function kpss.test). KPSS test null hypothesis series trend stationary.case, p-value test higher 0.05, test reject null hypothesis trend stationarity. say, evidence trend-stationary process.changing null “Trend” “Level”, KPSS test can also test null hypothesis level stationarity. level stationary time series time series non-zero constant mean, say, without trend.case, KPSS test level stationarity reject null hypothesis, say, process seems level stationary. Considered together, KPSS tests suggest series deterministic trend.use KPSS test test stochastic trend series created trend level stationary, test rejects null hypothesis (.e.: reject hypothesis trend level stationary process).series stochastic trend, can achieve stationarity differencing. Indeed, KPSS test reject null hypothesis level stationarity applied stochastic-trend series, differenced.cases KPSS results correct, since simulated tested time series deterministic stochastic trend. However, kind tests can also wrong. instance, possible reject null hypothesis actually true (“Type error”). reason, can useful use one test. instance, KPSS can used along Augmented Dickey-Fuller Test (ADF), popular unit root test.","code":"\n# install.packages(\"tseries\") # install the library if not yet installed\nlibrary(tseries)\nkpss.test(y_I0, null = \"Trend\")## \n##  KPSS Test for Trend Stationarity\n## \n## data:  y_I0\n## KPSS Trend = 0.10404, Truncation lag parameter = 5, p-value = 0.1\nkpss.test(y_I0, null = \"Level\")## \n##  KPSS Test for Level Stationarity\n## \n## data:  y_I0\n## KPSS Level = 8.4092, Truncation lag parameter = 5, p-value = 0.01\nkpss.test(y_I1, null = \"Trend\")## \n##  KPSS Test for Trend Stationarity\n## \n## data:  y_I1\n## KPSS Trend = 1.5, Truncation lag parameter = 5, p-value = 0.01\nkpss.test(y_I1, null = \"Level\")## \n##  KPSS Test for Level Stationarity\n## \n## data:  y_I1\n## KPSS Level = 7.7377, Truncation lag parameter = 5, p-value = 0.01\nkpss.test(diff(y_I1), null = \"Level\")## \n##  KPSS Test for Level Stationarity\n## \n## data:  diff(y_I1)\n## KPSS Level = 0.18432, Truncation lag parameter = 5, p-value = 0.1"},{"path":"regression.html","id":"augmented-dickey-fuller-adf-test","chapter":"10 Regression","heading":"10.2.1.1.2 Augmented Dickey-Fuller (ADF) Test","text":"Augmented Dickey-Fuller Test (ADF) popular unit root test. R implementation test can found library tseries (function adf.test). null hypothesis series unit root, alternative hypothesis series stationary trend stationary.use ADF test integrated series (unit root), test fails reject null hypothesis unit root, correct.use ADF test trend-stationary series (without unit root), test rejects null hypothesis unit root, correct.use ADF test integrated series, transformed differencing, test rejects null hypothesis unit root, correct.","code":"\nadf.test(y_I1)## \n##  Augmented Dickey-Fuller Test\n## \n## data:  y_I1\n## Dickey-Fuller = -1.7632, Lag order = 7, p-value = 0.6785\n## alternative hypothesis: stationary\nadf.test(y_I0)## \n##  Augmented Dickey-Fuller Test\n## \n## data:  y_I0\n## Dickey-Fuller = -5.8397, Lag order = 7, p-value = 0.01\n## alternative hypothesis: stationary\nadf.test(diff(y_I1))## \n##  Augmented Dickey-Fuller Test\n## \n## data:  diff(y_I1)\n## Dickey-Fuller = -7.8913, Lag order = 7, p-value = 0.01\n## alternative hypothesis: stationary"},{"path":"regression.html","id":"phillips-perron-test","chapter":"10 Regression","heading":"10.2.1.1.3 Phillips-Perron Test","text":"Another unit root test Phillips-Perron test. differs ADF test aspects (deals serial correlation heteroskedasticity errors). Also test implemented library tseries (funtion pp.test). Results test similar ADF test:case uncertainty, one test can used.","code":"\n# series with deterministic trend\npp.test(y_I0)## \n##  Phillips-Perron Unit Root Test\n## \n## data:  y_I0\n## Dickey-Fuller Z(alpha) = -144.04, Truncation lag parameter = 5, p-value = 0.01\n## alternative hypothesis: stationary\n# series with unit roots\npp.test(y_I1)## \n##  Phillips-Perron Unit Root Test\n## \n## data:  y_I1\n## Dickey-Fuller Z(alpha) = -5.4817, Truncation lag parameter = 5, p-value = 0.8039\n## alternative hypothesis: stationary\n# series with unit roots, differenced\npp.test(diff(y_I1))## \n##  Phillips-Perron Unit Root Test\n## \n## data:  diff(y_I1)\n## Dickey-Fuller Z(alpha) = -489.82, Truncation lag parameter = 5, p-value = 0.01\n## alternative hypothesis: stationary"},{"path":"regression.html","id":"non-autocorrelated-residuals","chapter":"10 Regression","heading":"10.2.2 Non-autocorrelated residuals","text":"try fit linear regression model. First, create two series \\(x\\) \\(y\\), \\(x\\) correlated \\(y\\) lags \\(x_{t-3}\\) \\(x_{t-4}\\).real model (case know created simulation), follows:\\[\ny_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \\epsilon_t \\\\\n\\epsilon \\sim N(0, 1)\n\\]\n#### lmTo fit linear regression, can use function lm (standard funtion perform linear regression analysis base R, additional packages necessary).function summary prints summary model, includes estimates (“coefficients” variables), standard errors, statistical significance variables, information.said regression models sometimes work well enough time series data, specific conditions met. Regards conditions (assumptions), particular, residuals models zero mean, shouldn’t show significant autocorrelation, normally distributed.check whether assumptions met, can visualize plot residuals, ACF/PACF histogram, also test residuals possible autocorrelation using statistical test like Breusch-Godfrey test (test default forecast library linear regression object lm tested).create plots can use base R functions, can use convenient checkresiduals function forecast package.case everything seems fine.look model summary printed , can see estimated model following (standard deviation residuals misnamed “residual standard error” summary lm):\\[\n\\begin{equation}\ny_t = 14.96869 + 0.85549x_{t-3} + 1.42126x_{t-4} + \\epsilon_t \\\\\n\\epsilon \\sim N(0, 1.002^2)\n\\end{equation}\n\\]\nestimated model also close “true” model:\\[\ny_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \\epsilon_t \\\\\n\\epsilon \\sim N(0, 1)\n\\]\n#### dynmlInstead lm, package dynml function name (dynml) can used fit dynamic regression models R. One main advantages package allows users fit time series linear regression models without calculating lagged values hand. add lagged variable, can simply used L (Lag) function. L function takes arguments name variable lag length. instance L(x, 4) corresponds \\(x_{t-4}\\).dynlm function also permits include trend (function trend) seasonal (function season) components model (also possible change reference value seasonal period, see ?dynlm). Just make example code perform dynamic regression dynlm:","code":"\n# simulated data of x series correlated to y at lag 3 and 4\nset.seed(999)\nx_series <- arima.sim(n = 200, list(order = c(1,0,0), ar = 0.7, sd=1))\nz <- ts.intersect(stats::lag(x_series, -3), stats::lag(x_series, -4)) \ny_series <- 15 + 0.8*z[,1] + 1.5*z[,2] + rnorm(196,0,1)\n\nxy_series <- ts.intersect(y_series, z)\nlm1 <- lm(xy_series[,1] ~ xy_series[,2] + xy_series[,3])\nsummary(lm1)## \n## Call:\n## lm(formula = xy_series[, 1] ~ xy_series[, 2] + xy_series[, 3])\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.57051 -0.73392  0.06238  0.74187  2.14794 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    14.96869    0.07238  206.80   <2e-16 ***\n## xy_series[, 2]  0.85549    0.07680   11.14   <2e-16 ***\n## xy_series[, 3]  1.42126    0.07678   18.51   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.002 on 196 degrees of freedom\n## Multiple R-squared:  0.873,  Adjusted R-squared:  0.8717 \n## F-statistic: 673.5 on 2 and 196 DF,  p-value: < 2.2e-16\n# install.package(\"forecast\") # install the package if necessary\nlibrary(forecast)\ncheckresiduals(lm1)## \n##  Breusch-Godfrey test for serial correlation of order up to 10\n## \n## data:  Residuals\n## LM test = 8.689, df = 10, p-value = 0.5618\n# install.packages(\"dynml\") # install the package if necessary\nlibrary(dynlm)\n\ndynlm.fit <- dynlm(y_series ~ L(x_series, 3) + L(x_series, 4))\nsummary(dynlm.fit)## \n## Time series regression with \"ts\" data:\n## Start = 5, End = 203\n## \n## Call:\n## dynlm(formula = y_series ~ L(x_series, 3) + L(x_series, 4))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.57051 -0.73392  0.06238  0.74187  2.14794 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    14.96869    0.07238  206.80   <2e-16 ***\n## L(x_series, 3)  0.85549    0.07680   11.14   <2e-16 ***\n## L(x_series, 4)  1.42126    0.07678   18.51   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.002 on 196 degrees of freedom\n## Multiple R-squared:  0.873,  Adjusted R-squared:  0.8717 \n## F-statistic: 673.5 on 2 and 196 DF,  p-value: < 2.2e-16\nset.seed(123)\ndata(\"AirPassengers\")\nap <- log(AirPassengers)\nap_x <- 2 * stats::lag(ap, -3) + rnorm(length(ap), 0, 0.2)\n\nap_fm <- dynlm(ap ~ trend(ap) + season(ap) + L(ap_x, 3))\nsummary(ap_fm)## \n## Time series regression with \"ts\" data:\n## Start = 1949(7), End = 1960(12)\n## \n## Call:\n## dynlm(formula = ap ~ trend(ap) + season(ap) + L(ap_x, 3))\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.156306 -0.035098  0.007821  0.041535  0.141713 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    4.116392   0.224848  18.307  < 2e-16 ***\n## trend(ap)      0.105758   0.005596  18.899  < 2e-16 ***\n## season(ap)Feb -0.026936   0.024867  -1.083 0.280811    \n## season(ap)Mar  0.127650   0.026209   4.870 3.32e-06 ***\n## season(ap)Apr  0.111029   0.028324   3.920 0.000146 ***\n## season(ap)May  0.132132   0.031750   4.162 5.86e-05 ***\n## season(ap)Jun  0.248250   0.030001   8.275 1.71e-13 ***\n## season(ap)Jul  0.334578   0.027595  12.125  < 2e-16 ***\n## season(ap)Aug  0.333578   0.029138  11.448  < 2e-16 ***\n## season(ap)Sep  0.172679   0.026344   6.555 1.35e-09 ***\n## season(ap)Oct  0.034292   0.026311   1.303 0.194866    \n## season(ap)Nov -0.101556   0.027526  -3.689 0.000335 ***\n## season(ap)Dec -0.010445   0.024767  -0.422 0.673966    \n## L(ap_x, 3)     0.061495   0.022439   2.741 0.007039 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.05831 on 124 degrees of freedom\n## Multiple R-squared:  0.9829, Adjusted R-squared:  0.9811 \n## F-statistic: 546.9 on 13 and 124 DF,  p-value: < 2.2e-16"},{"path":"regression.html","id":"regression-with-arma-errors","chapter":"10 Regression","heading":"10.2.3 Regression with ARMA errors","text":"previous case standard linear model works well, often case residuals times series regressions autocorrelated, linear regression model can suboptimal even wrong. instance, let’s create two time series , previous ones, cross-correlated lag 3 4, bit complicated structure.Considering autocorrelated structure series, true model can written follows:\\[\n\\begin{aligned}\n& y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \\eta_t \\\\\n& \\eta_t = 0.7\\eta_{t-1} + \\epsilon_t + 0.6\\epsilon_{t-1} \\\\\n& \\epsilon \\sim N(0, 1)\n\\end{aligned}\n\\]\npossible calculate regression using lm function, calculating lagged variables hand, use dynml library function.estimated model following:\\[\n\\begin{aligned}\n& y_t = 14.9005 + 1.0407x_{t-3} + 1.5171x_{t-4} + \\epsilon_t \\\\\n& \\epsilon \\sim N(0, 2.028^2)\n\\end{aligned}\n\\]\noriginal series can also visualized fitted values (values resulting model), visually inspect well model represents original series. differences original fitted series residuals.diagnostic plots residuals show presence autocorrelation, Breusch-Godfrey test highly significant (value far lower critical value \\(\\alpha = 0.05\\))case, ’s better take account residuals’ autocorrelation using regression model capable handle autocorrelated time series structures.previous chapter said ARIMA models special type regression model, dependent variable time series , independent variables lags time series. model capable take account autocorrelated structure time series.ARIMA modeling technique can applied single time series, can extended include additional, exogenous variables. ARIMA model including exogenous regressors (.e.: time series besides lagged dependent variable) like multiple regression models time series. particular, can considered regression model capable control autocorrelation residuals.possible use one option fit ARIMA model external regressors. convenient option provided function auto.arima, package forecast. library argument xreg can use numerical vector matrix external regressors, must number rows y (see ?auto.arima).resulting model seems appropriate previous one, fitted using just “classic” linear regression. clear also comparing two models AIC criterion (Akaike information criterion). AIC value used compare goodness--fit different models fitted dataset. lower AIC value, better fit (see also next paragraph).auto.arima function prints AIC value default, value given lm function. get , need use AIC function.case, ARIMA regression model results far better model (AIC=543.52) compared classic linear model (AIC=821.45).\\[\n\\begin{aligned}\n& y_t = 14.8532 + 0.9506x_{t-3} + 1.5732x_{t-4} + \\eta_t \\\\\n& \\eta_t = 0.6863\\eta_{t-1} + \\epsilon_t + 0.6491\\epsilon_{t-1} \\\\\n& \\epsilon \\sim N(0, 0.9482)\n\\end{aligned}\n\\]\nDiagnostic analysis residuals, shows concerning sign autocorrelation residuals, looks like white noise. Also test autocorrelated errors significant (default test autocorrelation testing ARIMA models external regressors forecast package Ljung-Box test)3).Also visually inspect original series along fitted series (values resulting model), can seen model better previous one.can also compare fitted versus original values using scatterplot. better model produces thinner diagonal line.\nauto.arima function give statistical significance coefficients (approach adopted forecast library different, based choice best model forecasting), possible get using function coeftest library lmtest.","code":"\n# another set of simulated data \n# the x series is correlated at lag 3 and 4\nset.seed(999)\nx2_series <- arima.sim(n = 200, list(order = c(1,0,0), ar = 0.7, sd=1))\nz2 <- ts.intersect(x2_series, stats::lag(x2_series, -3), stats::lag(x2_series, -4)) \ny2_series <- 15 + 0.8*z2[,2] + 1.5*z2[,3] \ny2_errors <- arima.sim(n = 196, list(order = c(1,0,1), ar = 0.6, ma = 0.6), sd=1)\ny2_series <- y2_series + y2_errors\n\n# check the cross-correlations at lag 3 and 4\nlibrary(TSA)\nprew <- prewhiten(x2_series, y2_series) \nprewhiten(x2_series, y2_series)\nprew## $ccf\n## \n## Autocorrelations of series 'X', by lag\n## \n##    -19    -18    -17    -16    -15    -14    -13    -12    -11    -10     -9     -8     -7     -6 \n## -0.021 -0.029  0.012  0.065  0.024  0.008  0.076  0.107  0.001 -0.031 -0.024 -0.039  0.048  0.070 \n##     -5     -4     -3     -2     -1      0      1      2      3      4      5      6      7      8 \n## -0.021  0.620  0.425  0.036 -0.001  0.052  0.080  0.068 -0.008 -0.011  0.096  0.124  0.062 -0.022 \n##      9     10     11     12     13     14     15     16     17     18     19 \n##  0.030  0.023 -0.035 -0.039 -0.008  0.016 -0.059 -0.149 -0.120 -0.027 -0.020 \n## \n## $model\n## \n## Call:\n## ar.ols(x = x)\n## \n## Coefficients:\n##       1        2        3        4        5        6        7        8        9       10       11  \n##  0.6362   0.1109   0.0496  -0.1533  -0.0240  -0.0499   0.2041   0.0221  -0.0990  -0.0981   0.1703  \n##      12       13       14  \n## -0.0558  -0.0812   0.1179  \n## \n## Intercept: 0.006091 (0.06307) \n## \n## Order selected 14  sigma^2 estimated as  0.7336\n# Calculate the lagged variables by hand and apply the lm function...\nx2Lagged <- cbind(\n    xLag0 = x2_series,\n    xLag3 = stats::lag(x2_series,-3),\n    xLag4 = stats::lag(x2_series,-4))\n\nxy2_series <- ts.union(y2_series, x2Lagged)\n\nlm2 <- lm(xy2_series[,1] ~ xy2_series[,3:4])\nsummary(lm2) # AIC: 821.45## \n## Call:\n## lm(formula = xy2_series[, 1] ~ xy2_series[, 3:4])\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.9200 -1.4508  0.0667  1.5395  5.1083 \n## \n## Coefficients:\n##                                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                      14.9005     0.1486 100.273  < 2e-16 ***\n## xy2_series[, 3:4]x2Lagged.xLag3   1.0407     0.1595   6.523 6.14e-10 ***\n## xy2_series[, 3:4]x2Lagged.xLag4   1.5171     0.1599   9.488  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.028 on 189 degrees of freedom\n##   (12 observations deleted due to missingness)\n## Multiple R-squared:  0.6735, Adjusted R-squared:   0.67 \n## F-statistic: 194.9 on 2 and 189 DF,  p-value: < 2.2e-16\n# ... or use the dynml function\ndynlm.fit2 <- dynlm(y2_series ~ L(x2_series, 3) + L(x2_series, 4))\nsummary(dynlm.fit2)## \n## Time series regression with \"ts\" data:\n## Start = 5, End = 196\n## \n## Call:\n## dynlm(formula = y2_series ~ L(x2_series, 3) + L(x2_series, 4))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.9200 -1.4508  0.0667  1.5395  5.1083 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)      14.9005     0.1486 100.273  < 2e-16 ***\n## L(x2_series, 3)   1.0407     0.1595   6.523 6.14e-10 ***\n## L(x2_series, 4)   1.5171     0.1599   9.488  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.028 on 189 degrees of freedom\n## Multiple R-squared:  0.6735, Adjusted R-squared:   0.67 \n## F-statistic: 194.9 on 2 and 189 DF,  p-value: < 2.2e-16\nlm2d <- ts.intersect(na.omit(xy2_series[,1]), lm2$fitted.values)\n\nplot.ts(lm2d, plot.type = \"single\", col=c(\"orange\",\"blue\"), \n        lty=c(1,4), lwd=c(1,1),\n        main = \"'Classic' Linear Model - Original (orange) and Fitted series (blue)\") \ncheckresiduals(lm2)## \n##  Breusch-Godfrey test for serial correlation of order up to 10\n## \n## data:  Residuals\n## LM test = 149.45, df = 10, p-value < 2.2e-16\npacf(lm2$residuals)\narima1 <- auto.arima(xy2_series[,1], xreg = xy2_series[,3:4])\narima1## Series: xy2_series[, 1] \n## Regression with ARIMA(1,0,1) errors \n## \n## Coefficients:\n##          ar1     ma1  intercept  x2Lagged.xLag3  x2Lagged.xLag4\n##       0.6863  0.6491    14.8532          0.9506          1.5732\n## s.e.  0.0555  0.0528     0.3607          0.0757          0.0762\n## \n## sigma^2 = 0.9482:  log likelihood = -265.76\n## AIC=543.52   AICc=543.97   BIC=563.06\nAIC(lm2)## [1] 821.4495\ncheckresiduals(arima1)## \n##  Ljung-Box test\n## \n## data:  Residuals from Regression with ARIMA(1,0,1) errors\n## Q* = 6.3861, df = 8, p-value = 0.6041\n## \n## Model df: 2.   Total lags used: 10\narima1d <- ts.intersect(na.omit(xy2_series[,1]), arima1$fitted)\nplot.ts(arima1d, plot.type = \"single\", col=c(\"orange\",\"blue\"), \n        lty=c(1,4), lwd=c(1,1),\n        main = \"ARIMA errors model - Original (orange) and Fitted series (blue)\") \npar(mfrow=c(1,2))\nplot(na.omit(xy2_series[,1]), lm2$fitted.values, main = \"LM\", xlab=\"Original\", ylab=\"Fitted\")\nplot(na.omit(xy2_series[,1]), arima1$fitted, main = \"ARIMA regression model\", xlab=\"Original\", ylab=\"Fitted\")\n# install.packages(lmtest) # installa the package\nlibrary(lmtest)\ncoeftest(arima1)## \n## z test of coefficients:\n## \n##                 Estimate Std. Error z value  Pr(>|z|)    \n## ar1             0.686330   0.055525  12.361 < 2.2e-16 ***\n## ma1             0.649134   0.052850  12.283 < 2.2e-16 ***\n## intercept      14.853152   0.360689  41.180 < 2.2e-16 ***\n## x2Lagged.xLag3  0.950588   0.075705  12.557 < 2.2e-16 ***\n## x2Lagged.xLag4  1.573242   0.076224  20.640 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"regression.html","id":"count-regression-models","chapter":"10 Regression","heading":"10.2.4 Count regression models","text":"models described mostly used continuous variables (expressed numeric double R data format). However, often case time series composed integer values, count data (expressed integer).Sometimes, mentioned methods work well also type data (instance, counts large). times, time series model developed count data can better choice (instance, series include mostly small integer values).Two common statistical models deal count data based Poisson Negative Binomial distributions. probability distributions ones usually employed model count data.libraries fit count time series regression models R. take consideration tscount, function tsglm.consider, example tscount function fit count time series regression models, dataset “Seatbelts” (monthly number killed drivers light goods vehicles Great Britain January 1969 December 1984), following paper describes tscount library.authors package write paper (par. 7.2) describing library:time series part dataset first considered Harvey Durbin (1986) studying effect compulsory wearing seatbelts introduced 31 January 1983. dataset, including additional covariates, available R object Seatbelts. paper Harvey Durbin (1986) analyze numbers casualties drivers passengers cars, large can treated methods continuous-valued data. monthly number killed drivers vans analyzed much smaller (minimum 2 maximum 17) therefore methods count data preferred.going fit model aimed capturing first order autoregressive AR(1) term yearly seasonality 12th order autoregressive term.function tsglm allows users declare autoregressive seasonal autoregressive terms convenient way (following part function: model = list(past_obs = c(1, 12))).possible check residuals usual plots.function summary can used get parameter estimates model (case function can also employ parametric bootstrap procedure (B) obtain standard errors confidence intervals regression parameters. authors use B=500 original paper, since experience value yields stable results. Higher B values can precise require time calculated).model follows:\\[\nlog(\\lambda_t) = 1.83 + 0.09Y_{t-1} + 0.15Y_{t-12} + 0.83X_t - 0.003t\n\\]\nequation notice , Poisson regression, models logarithm Y values times t (expressed \\(log(\\lambda_t)\\)).Another example (using dataset can download ):Besides checking residuals, possible plot PIT histogram, provided function pit tscount:PIT histogram tool evaluating statistical consistency probabilistic forecast observation. predictive distributions observations compared actual observations. predictive distribution ideal result flat PIT histogram bin extraordinary high low level. information PIT histograms see references listed .library included diagnostic tools metrics can help choosing poisson negative binomial models (see paper information).function summary prints coefficients model confidence interval.","code":"\n# install.packages(\"tscount\")\nlibrary(tscount)\ndata(\"Seatbelts\")\n\ntimeseries <- Seatbelts[, \"VanKilled\"]\n\nregressors <- cbind(PetrolPrice = Seatbelts[, c(\"PetrolPrice\")], \n                    linearTrend = seq(along = timeseries))\n\ntimeseries_until1981 <- window(timeseries, end = c(1981, 12))\nregressors_until1981 <- window(regressors, end = c(1981, 12))\npar(mfrow=c(2,2))\nplot.ts(timeseries, main=\"\")\nhist(timeseries, main=\"\")\nacf(timeseries, main=\"\")\npacf(timeseries, main=\"\")\npoisson_fit <- tsglm(timeseries_until1981,\n                     model = list(past_obs = c(1, 12)), \n                     xreg = regressors_until1981,\n                     distr = \"poisson\", link = \"log\")\npar(mfrow=c(2,2))\n\nplot(poisson_fit$residuals, main=\"\")\nhist(poisson_fit$residuals, main=\"\")\nacf(poisson_fit$residuals, main=\"\")\npacf(poisson_fit$residuals, main=\"\")\nsummary(poisson_fit)## \n## Call:\n## tsglm(ts = timeseries_until1981, model = list(past_obs = c(1, \n##     12)), xreg = regressors_until1981, link = \"log\", distr = \"poisson\")\n## \n## Coefficients:\n##              Estimate  Std.Error  CI(lower)  CI(upper)\n## (Intercept)   1.83284   0.367830    1.11191    2.55377\n## beta_1        0.08672   0.080913   -0.07187    0.24530\n## beta_12       0.15288   0.084479   -0.01269    0.31846\n## PetrolPrice   0.83069   2.303555   -3.68420    5.34557\n## linearTrend  -0.00255   0.000653   -0.00383   -0.00127\n## Standard errors and confidence intervals (level =  95 %) obtained\n## by normal approximation.\n## \n## Link function: log \n## Distribution family: poisson \n## Number of coefficients: 5 \n## Log-likelihood: -396.1765 \n## AIC: 802.3529 \n## BIC: 817.6022 \n## QIC: 802.3529\n# summary(poisson_fit, B=500) # to use the bootstrap procedure\nlibrary(tidyverse)\ngtrend_fakenews_qanon <- read_csv(\"data/gtrend_fakenews_qanon.csv\",\n                                  col_types = cols(date = col_date(format = \"%Y-%m\"),\n                                                   fake_news = col_integer(), \n                                                   qanon = col_integer()))\n\n# head(gtrend_fakenews_qanon$date,1) # 2015-01-01\n# tail(gtrend_fakenews_qanon$date,1) # 2020-12-01\n\nfake_news <- ts(gtrend_fakenews_qanon$fake_news, \n                start = c(2015,1), end = c(2020,12), frequency = 12)\n\nqanon <- ts(gtrend_fakenews_qanon$qanon, \n            start = c(2015,1), end = c(2020,12), frequency = 12)\nlayout(matrix(c(1,1,2,3,4,5), 2,3, byrow=T))\nplot.ts(qanon, main=\"\")\nhist(qanon, main=\"\")\nacf(qanon, 48, main=\"\")\npacf(qanon, 48, main=\"\")\nqanon_fake_ccf <- prewhiten(fake_news, qanon, main=\"\")\nqanon_fake_ccf$ccf## \n## Autocorrelations of series 'X', by lag\n## \n## -1.2500 -1.1667 -1.0833 -1.0000 -0.9167 -0.8333 -0.7500 -0.6667 -0.5833 -0.5000 -0.4167 -0.3333 \n##  -0.127   0.064  -0.013   0.008   0.033   0.022  -0.041  -0.278   0.120  -0.313   0.417   0.282 \n## -0.2500 -0.1667 -0.0833  0.0000  0.0833  0.1667  0.2500  0.3333  0.4167  0.5000  0.5833  0.6667 \n##  -0.061  -0.025  -0.075  -0.012  -0.180   0.063   0.039  -0.154  -0.070  -0.040   0.029   0.068 \n##  0.7500  0.8333  0.9167  1.0000  1.0833  1.1667  1.2500 \n##  -0.008  -0.045  -0.221  -0.068  -0.039   0.112   0.177\nreg <- cbind(fake_news_lag4 = stats::lag(fake_news, -4),\n             fake_news_lag5 = stats::lag(fake_news, -5),\n             fake_news_lag6 = stats::lag(fake_news, -6))\n\n# NA values derives from the application of the \"lag\" function\n# and have to be removed, since the regression function cannot\n# work properly with them\nreg <- na.omit(reg)\n\n# start(reg) # 2015, 6\n# end(reg) # 2021, 4\n\nreg <-  window(reg, start = c(2015, 7), end = c(2020, 12))\nqanon <- window(qanon, start = c(2015, 7), end = c(2020, 12))\npoisson_gtrend_fit <- tsglm(qanon,\n                            model = list(past_obs = 1), \n                            xreg = reg,\n                            distr = \"poisson\", link = \"log\")\nlayout(matrix(c(1,1,2,3,4,5), 3,2, byrow=T))\n\nplot(poisson_gtrend_fit$residuals, main=\"\")\nhist(poisson_gtrend_fit$residuals, main=\"\")\nacf(poisson_gtrend_fit$residuals, main=\"\")\npacf(poisson_gtrend_fit$residuals, main=\"\")\n\nplot(as.vector(poisson_gtrend_fit$response), \n     as.vector(poisson_gtrend_fit$fitted.values),\n     xlab=\"response\", ylab=\"fitted\")\npit(poisson_gtrend_fit, ylim = c(0, 1.5), main = \"PIT Poisson\") \nsummary(poisson_gtrend_fit)## \n## Call:\n## tsglm(ts = qanon, model = list(past_obs = 1), xreg = reg, link = \"log\", \n##     distr = \"poisson\")\n## \n## Coefficients:\n##                 Estimate  Std.Error  CI(lower)  CI(upper)\n## (Intercept)      0.58373    0.18036   0.230232     0.9372\n## beta_1           0.83746    0.04834   0.742715     0.9322\n## fake_news_lag4   0.00892    0.00274   0.003546     0.0143\n## fake_news_lag5   0.00638    0.00354  -0.000554     0.0133\n## fake_news_lag6  -0.01617    0.00272  -0.021506    -0.0108\n## Standard errors and confidence intervals (level =  95 %) obtained\n## by normal approximation.\n## \n## Link function: log \n## Distribution family: poisson \n## Number of coefficients: 5 \n## Log-likelihood: -239.6582 \n## AIC: 489.3163 \n## BIC: 500.2646 \n## QIC: 489.3163"},{"path":"regression.html","id":"model-selection-aic-aicc-bic","chapter":"10 Regression","heading":"10.3 Model Selection (AIC, AICc, BIC)","text":"Statistical modeling , usually, recursive process requires fit several different models , eventually, select appropriate one. instance, Box Jenkins approach employed find appropriate ARIMA model time series (see previous chapter), requires fitting multiple models find suitable one based data. Similarly, “auto.arima” function library forecast, automatizes search appropriate ARIMA model, conducts search possible model.compare models select appropriate one, necessary use criteria. example employed AIC criterion. similar criteria AICc, BIC. can used find appropriate model, comparing goodness--fit different models fitted dataset. instance, documentation “auto.arima” function says function “returns best ARIMA model according either AIC, AICc BIC value”.AIC criterion acronym Akaike information criterion). lower AIC value, better fit (see also next paragraph).AICc criterion, , correction small sample size. sample small can used place AIC criterion. sample size increases, AICc converges AIC.BIC criterion Bayesian Information Criterion (Schwartz’s Bayesian Criterion) stronger penalty AIC overparametrized models (complex models, several predictors).criteria can also used searching appropriate regression model, compare several different models including different lags variables.comparing models using criteria, important models fitted dataset, otherwise results comparable. important aspect take account using lagged predictors. instance, may want try model including one lagged predictor \\(x_{t-1}\\) model including two lagged predictors \\(x_{t-1}\\) \\(x_{t-2}\\), compare order select best one according AIC, AICc BIC criterion. However, add lagged predictor loose data points.Thus, fit models different lags, fit dataset. case, instance, skip NA rows, use just rows 3 40.can compare model, instance, using AIC criterion, choose model smallest value.Finally, fit model using available data.Besides criteria, also strategies model selection.","code":"\nx_example <- ts(rnorm(40))\ny_example <- ts(rnorm(40))\n\nexample_data <- cbind(y = y_example,\n                      xLag0 = x_example,\n                      xLag1 = stats::lag(x_example, -1),\n                      xLag2 = stats::lag(x_example, -2))\n\nexample_data## Time Series:\n## Start = 1 \n## End = 42 \n## Frequency = 1 \n##               y       xLag0       xLag1       xLag2\n##  1  1.075501365  0.48505236          NA          NA\n##  2 -0.276740732 -1.20993027  0.48505236          NA\n##  3 -0.646751445  0.01724609 -1.20993027  0.48505236\n##  4  0.102585633  0.70085715  0.01724609 -1.20993027\n##  5 -0.072519610 -0.40170226  0.70085715  0.01724609\n##  6  1.619712305 -0.48727576 -0.40170226  0.70085715\n##  7  0.687751998 -0.76436248 -0.48727576 -0.40170226\n##  8  0.234929596  2.97223380 -0.76436248 -0.48727576\n##  9 -0.430528202 -0.43578545  2.97223380 -0.76436248\n## 10  0.638039146 -1.35197954 -0.43578545  2.97223380\n## 11 -0.217226469 -0.52868886 -1.35197954 -0.43578545\n## 12  0.020494309 -0.06027948 -0.52868886 -1.35197954\n## 13 -1.913357840 -0.03222441 -0.06027948 -0.52868886\n## 14 -0.526191144  0.55290101 -0.03222441 -0.06027948\n## 15  0.416282618 -0.25966064  0.55290101 -0.03222441\n## 16  0.457627862 -1.01400343 -0.25966064  0.55290101\n## 17 -0.275639625 -0.86452761 -1.01400343 -0.25966064\n## 18 -0.353882346  0.11040086 -0.86452761 -1.01400343\n## 19 -0.901762561 -0.02452574  0.11040086 -0.86452761\n## 20 -1.052145243 -2.51548495 -0.02452574  0.11040086\n## 21  0.832626641  0.44375488 -2.51548495 -0.02452574\n## 22  0.056411159  1.56218440  0.44375488 -2.51548495\n## 23  0.838708652  0.97180894  1.56218440  0.44375488\n## 24  0.344611778  1.58747428  0.97180894  1.56218440\n## 25 -1.088605832 -1.84530863  1.58747428  0.97180894\n## 26 -0.527820628 -1.27107477 -1.84530863  1.58747428\n## 27  3.061137014  2.03899229 -1.27107477 -1.84530863\n## 28 -0.520213835 -0.72914933  2.03899229 -1.27107477\n## 29  0.248244201  0.93447357 -0.72914933  2.03899229\n## 30  0.046969519 -0.75324932  0.93447357 -0.72914933\n## 31  0.249449596 -0.68448064 -0.75324932  0.93447357\n## 32  1.041843001 -0.59473197 -0.68448064 -0.75324932\n## 33 -0.338376495 -0.12412231 -0.59473197 -0.68448064\n## 34  0.201287727 -1.39014027 -0.12412231 -0.59473197\n## 35  0.602711731 -1.23556534 -1.39014027 -0.12412231\n## 36 -0.225018148 -1.28500792 -1.23556534 -1.39014027\n## 37 -0.010113590 -1.05225339 -1.28500792 -1.23556534\n## 38 -0.193597244  1.36496602 -1.05225339 -1.28500792\n## 39  0.009188137 -1.07721180  1.36496602 -1.05225339\n## 40  1.441899269  0.67538151 -1.07721180  1.36496602\n## 41           NA          NA  0.67538151 -1.07721180\n## 42           NA          NA          NA  0.67538151\n# Restrict data so models use same fitting period\nfit1 <- auto.arima(example_data[3:40,1], xreg=example_data[3:40,2])\nfit2 <- auto.arima(example_data[3:40,1], xreg=example_data[3:40,2:3])\nfit3 <- auto.arima(example_data[3:40,1], xreg=example_data[3:40,2:4])\nfit1$aic## [1] 95.11836\nfit2$aic## [1] 94.31932\nfit3$aic## [1] 96.07911\nfit1 <- auto.arima(example_data[,1], xreg=example_data[,2])\nfit1## Series: example_data[, 1] \n## Regression with ARIMA(0,0,0) errors \n## \n## Coefficients:\n##         xreg\n##       0.2447\n## s.e.  0.1120\n## \n## sigma^2 = 0.6511:  log likelihood = -47.67\n## AIC=99.34   AICc=99.66   BIC=102.72"},{"path":"regression.html","id":"some-examples-in-the-literature","chapter":"10 Regression","heading":"10.4 Some examples in the literature","text":"several examples use time series regression models literature field communication science.instance, Event-Centered Nature Global Public Spheres: UN Climate Change Conferences, Fridays Future, (Limited) Transnationalization Media Debates4, authors examined whether UN climate change conferences conducive emergence transnational public sphere triggering issue convergence increased transnational interconnectedness across national media debates. authors detail method follows way:[…] Given autoregressive nature properties time series, ordinary least squares regression analysis violate normality error independence observations assumption (Wells et al., 2019). Instead, applied dynamic regression approach (Gujarati & Porter, 2009; Hyndman & Athanasopoulos, 2018), assumes error term follows autoregressive integrated moving average (ARIMA) model (…). found best ARIMA structure error term using auto.arima function forecast R package (Hyndman & Khandakar, 2008). searches ARIMA structure can explain variance according Akaike information criterion (Akaike, 1973).case use term “dynamic regression” refer time series regression ARIMA errors, include lagged values variables, thus analyzing contemporary relationships variables.found, instance, events taking place supranational level governance (…) consistently led spikes media attention across countries. contrast, bottom-effort Fridays Future showed inconsistent relationship media attention across four countries.Online incivility, cyberbalkanization, dynamics opinion polarization mass protest event5, authors used standard regression regression ARIMA errors show “online incivility — operationalized use foul language — grew volume political discussions levels cyberbalkanization increased. Incivility led higher levels opinion polarization.”. Also case authors analyze “static process”, , focus contemporary relationships variables.Beyond cognitions: longitudinal study online search salience media coverage president6, authors used regression models ARIMA errors examine shifts newswire coverage search interest among Internet users President Obama first two years administration (2009-2010).case, authors analyze relationships variables taking account lagged values, thus adopting “dynamic process” perspective. instance, write:RQ2 sought determine time span linkages coverage volume search volume. (…) ARIMA models run gauge dynamics mutual influence two time series. first model examined effect coverage volume search volume time (.e., basic agenda setting) (…) presidential public relations, included additional input series. first model, search volume single dependent variable, identified close examination autocorrelation functions (ACFs) partial autocorrelation functions (PACFs). analysis revealed classic autoregressive model series (1, 0, 0). […] According results, shifts aggregate search volume two-year period significantly predicted coverage volume prior five weeks (p < .010)* presidential public relations efforts preceding two, three (p < .001), five weeks (p < .005). ARIMA model two predictors correctly specified (Ljung–Box Q = 18.132, p = .381) explained roughly 35% observed variation series.AIDS black white: influence newspaper coverage HIV/AIDS HIV/AIDS testing among African Americans White Americans, 1993–20077, authors examined effect newspaper coverage HIV/AIDS HIV testing behavior U.S. population., using lagged regression support causal order claims ensuring newspaper coverage precedes testing behavior inclusion 1-month lagged newspaper coverage variable model. Counterintuitively, found news media coverage negative effect testing behavior: every additional 100 HIV/AIDS risk related newspaper stories published group U.S. newspapers month, 1.7% decline HIV testing levels following month, higher negative effects African Americans.","code":""},{"path":"intervention-analysis.html","id":"intervention-analysis","chapter":"11 Intervention Analysis","heading":"11 Intervention Analysis","text":"chapter going learn intervention analysis (sometimes also called interrupted time-series analysis) see conduct intervention analysis.Intervention analysis typically conducted Box & Jenkins ARIMA framework traditionally uses method introduced Box Tiao (1975)8, provided framework assessing effect intervention time series study.summarized Box Tiao: Given known intervention, evidence change series kind expected actually occurred, , , can said nature magnitude change?. words Intervention analysis estimates effect external exogenous intervention time-series. conduct analysis, necessary know date intervention.Intervention analysis “quasi-experimental” design interesting approach test whether exogenous shocks, , instance, introduction new policy, impact time series process significant way, , changing mean function trend time series.Behind intervention analysis causal hypothesis observations treatment (“intervention”) different level slope intervention/interruption.Besides intervention interrupted time-series analysis, analysis can conducted segmented regression method. However, case traditional regression models applied time series data, approach take account autocorrelated structure time series. methods include complex computational approaches.","code":""},{"path":"intervention-analysis.html","id":"types-of-intervention","chapter":"11 Intervention Analysis","heading":"11.1 Types of intervention","text":"different types interventions. instance, intervention can abrupt impact determining permanent temporary change, sudden short-lived change due event, gradual yet permanent change.","code":""},{"path":"intervention-analysis.html","id":"intervention-analysis-with-arima","chapter":"11 Intervention Analysis","heading":"11.2 Intervention analysis with ARIMA","text":"exemplify intervention analysis going reproduce example paper Interrupted time series analysis using autoregressive integrated moving average (ARIMA) models: guide evaluating large-scale health interventions.data run analysis can downloaded .example evaulates impact health policy intervention (Australian health policy intervention restricted conditions particular medicine (quetiapine) subsidised). methodological process can applied evaluate intervention context.case study described follows:(…) due growing concerns inappropriate prescribing, January 1, 2014 new prescriptions tablet strength include refills. primary outcome number monthly dispensings 25 mg quetiapine, 48 months observations (January 2011 December 2014).Thus, data comprises 48 months observations, date intervention January 1, 2014.also seasonality process:Australia, medicine dispensing claims significant yearly seasonality. Medicines subsidised citizens eligible residents Pharmaceutical Benefits Scheme (PBS), people paying --pocket co-payment towards cost medicines, remainder subsidised. person’s (family’s) total --pocket costs reach “Safety Net threshold” calendar year, eligible reduced co-payment remainder year. Thus, incentive people reaching Safety Net refill medicines frequently towards end year. Hence, see increase prescriptions end year, followed decrease January.researchers hypothesize nature intervention follows (see picture ):(…) due nature intervention postulated immediate drop dispensings post-intervention (step change), well change slope (ramp). Thus, included variables representing types impacts model. impacts, h = 0 r = 0.sentence , h describes effect happens r represents decay pattern (see picture ).First load data, converting time series format, visualize time series along vertical lines representing date intervention.Next, create dummy variables representing intervention.\ncan tricky R. case, authors convert time ts object human-readable format .yearmon function (zoo function can use loading xts library).vectors dummy variable intervention. value equal zero date intervention, 1 .Next, specific case, also want create variable representing constant increasing change, capturing increasing effect intervention time.\nAlso case creation variable can little tricky. create two vectors using rep seq function, concatenate using c function.argument rep function two integers x times (rep(x, times)), function creates vectors repeat (“rep”) x values number times specified times. 36 months intervention, assign value zero.Instead, use seq function create vectors increasing values. part variable represent gradual increase intervention (12 months data intervention). function seq takes three arguments , , : starting end values sequence, increment sequence. case create sequence values increases 1 12 1.create variable need, concatenate function c function, follows:search appropriate ARIMA model data using auto.arima function (forecast package). include variables created external regressors.resulting model ARIMA(2,1,0)(0,1,1)[12].use information retrieved auto.arima function fit ARIMA model data, without including intervention (variables created), using just data date intervention (January 2014). , use window function order restrict set data consider, indicating December 2013 end series.Next, forecast 12 months didn’t include (starting January 2014 end period observation, December 2014) using forecast function (library forecast). logic behind operation see happened series absence intervention. words, use prediction counterfactual order describe possible effect intervention series, determining observed values diverge forecast.plotting data, can visualize predicted values absence intervention (red dashed line) well observed values (blue line). seems health policy considerably impacted analyzed prescriptions.Coming back initial ARIMA model including intervention variables, calculating also confidence intervals significance coefficients using coeftest confint function lmtest library, can quantify impact policy.estimated step change − 3285 dispensings (95% CI − 4465 − 2104) estimated change slope − 1397 dispensings per month (95% CI − 1606 − 1188). (figure, ndr) shows values predicted ARIMA model absence intervention (counterfactual) compared observed values. means change subsidy 25 mg quetiapine January 2014 associated immediate, sustained decrease 3285 dispensings, decrease 1397 dispensings every month. words, 4682 (3285 + 1397) fewer dispensings January 2014 predicted subsidy changes implemented. February 2014, 6079 fewer dispensings (3285 + 2*1397). Importantly, findings considered valid duration study period (.e. December 2014).","code":"\n# Load data\nquet <- read.csv(file = \"./data/12874_2021_1235_MOESM1_ESM.csv\")\n\n# Convert data to time series object\nquet.ts <- ts(quet[,2], frequency=12, start=c(2011, 1))\n\n# Plot data to visualize time series\nplot.ts(quet.ts, ylim=c(0, 40000), col = \"blue\", xlab = \"Month\", ylab = \"Dispensings\")\n# Add vertical line indicating date of intervention (January 1, 2014)\nabline(v=2014, col = \"gray\", lty = \"dashed\", lwd=2)\nlibrary(xts)\n# Create variable representing step change and view\nstep <- as.numeric(as.yearmon(time(quet.ts)) >= \"Jan 2014\")\nstep##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\nrep(0, 36)##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nseq(from = 1, to = 12, by = 1)##  [1]  1  2  3  4  5  6  7  8  9 10 11 12\n# Create variable representing ramp (change in slope) and view\nramp <- c(rep(0, 36), seq(1, 12, 1))\nramp ##  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n## [33]  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12\nlibrary(forecast)\n\n# Use automated algorithm to identify parameters\nmodel1 <- auto.arima(quet.ts, xreg = cbind(step, ramp), stepwise=FALSE)\n\n# Check residuals\ncheckresiduals(model1)## \n##  Ljung-Box test\n## \n## data:  Residuals from Regression with ARIMA(2,1,0)(0,1,1)[12] errors\n## Q* = 9.5692, df = 7, p-value = 0.2143\n## \n## Model df: 3.   Total lags used: 10\nmodel1## Series: quet.ts \n## Regression with ARIMA(2,1,0)(0,1,1)[12] errors \n## \n## Coefficients:\n##          ar1      ar2     sma1        step        ramp\n##       -0.873  -0.6731  -0.6069  -3284.7792  -1396.6523\n## s.e.   0.124   0.1259   0.3872    602.3362    106.6329\n## \n## sigma^2 = 648828:  log likelihood = -284.45\n## AIC=580.89   AICc=583.89   BIC=590.23\n# To forecast the counterfactual, model data excluding post-intervention time period\nmodel2 <- Arima(window(quet.ts, end = c(2013, 12)), order = c(2, 1, 0), \n                seasonal = list(order = c(0, 1, 1), period = 12))\n# Forecast 12 months post-intervention and convert to time series object\nfc <- forecast(model2, h = 12)\n\n# covert the average forecast (fc$mean) in a time series object\nfc.ts <- ts(as.numeric(fc$mean), start=c(2014, 1), frequency = 12)\n\n# Combine the observed and the forecast data\nquet.ts.2 <- ts.union(quet.ts, fc.ts)\nquet.ts.2##          quet.ts    fc.ts\n## Jan 2011   16831       NA\n## Feb 2011   17234       NA\n## Mar 2011   20546       NA\n## Apr 2011   19226       NA\n## May 2011   21136       NA\n## Jun 2011   20939       NA\n## Jul 2011   21103       NA\n## Aug 2011   22897       NA\n## Sep 2011   22162       NA\n## Oct 2011   22184       NA\n## Nov 2011   23108       NA\n## Dec 2011   25967       NA\n## Jan 2012   20123       NA\n## Feb 2012   21715       NA\n## Mar 2012   24497       NA\n## Apr 2012   21720       NA\n## May 2012   25053       NA\n## Jun 2012   23915       NA\n## Jul 2012   24972       NA\n## Aug 2012   26183       NA\n## Sep 2012   24163       NA\n## Oct 2012   26172       NA\n## Nov 2012   26642       NA\n## Dec 2012   29086       NA\n## Jan 2013   24002       NA\n## Feb 2013   24190       NA\n## Mar 2013   26052       NA\n## Apr 2013   26707       NA\n## May 2013   29077       NA\n## Jun 2013   26927       NA\n## Jul 2013   30300       NA\n## Aug 2013   29854       NA\n## Sep 2013   28824       NA\n## Oct 2013   31519       NA\n## Nov 2013   32084       NA\n## Dec 2013   33160       NA\n## Jan 2014   24827 29127.50\n## Feb 2014   23285 29671.28\n## Mar 2014   23884 31156.37\n## Apr 2014   21921 31339.65\n## May 2014   22715 33843.48\n## Jun 2014   19919 31809.61\n## Jul 2014   20560 34498.50\n## Aug 2014   18961 34774.18\n## Sep 2014   18780 33302.09\n## Oct 2014   17998 35641.85\n## Nov 2014   16624 36184.57\n## Dec 2014   18450 37792.03\n# Plot\nplot.ts(quet.ts.2, plot.type = \"single\", \n     col=c('blue','red'), xlab=\"Month\", ylab=\"Dispensings\", \n     lty=c(\"solid\", \"dashed\"), ylim=c(0,40000))\n\nabline(v=2014, lty=\"dashed\", col=\"gray\")\nlibrary(lmtest)\n\nmodel1## Series: quet.ts \n## Regression with ARIMA(2,1,0)(0,1,1)[12] errors \n## \n## Coefficients:\n##          ar1      ar2     sma1        step        ramp\n##       -0.873  -0.6731  -0.6069  -3284.7792  -1396.6523\n## s.e.   0.124   0.1259   0.3872    602.3362    106.6329\n## \n## sigma^2 = 648828:  log likelihood = -284.45\n## AIC=580.89   AICc=583.89   BIC=590.23\ncoeftest(model1)## \n## z test of coefficients:\n## \n##         Estimate  Std. Error  z value  Pr(>|z|)    \n## ar1     -0.87301     0.12396  -7.0427 1.885e-12 ***\n## ar2     -0.67314     0.12587  -5.3480 8.893e-08 ***\n## sma1    -0.60694     0.38722  -1.5674     0.117    \n## step -3284.77920   602.33616  -5.4534 4.942e-08 ***\n## ramp -1396.65226   106.63287 -13.0978 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nconfint(model1)##              2.5 %        97.5 %\n## ar1     -1.1159671    -0.6300565\n## ar2     -0.9198339    -0.4264433\n## sma1    -1.3658815     0.1520046\n## step -4465.3363731 -2104.2220250\n## ramp -1605.6488440 -1187.6556789"},{"path":"interrupted-time-series-analysis-using-segmented-regression.html","id":"interrupted-time-series-analysis-using-segmented-regression","chapter":"12 Interrupted time series analysis using segmented regression","heading":"12 Interrupted time series analysis using segmented regression","text":"Segmented regression another common way analyzing impact intervention. Two good papers explaining methods segmented regression , example:Bernal, J. L., Cummins, S., & Gasparrini, . (2017). Interrupted time series regression evaluation public health interventions: tutorial. International journal epidemiology, 46(1), 348-355.Wagner, . K., Soumerai, S. B., Zhang, F., & Ross‐Degnan, D. (2002). Segmented regression analysis interrupted time series studies medication use research. Journal clinical pharmacy therapeutics, 27(4), 299-309.explained latter, “Segmented regression analysis uses statistical models estimate level trend pre-intervention segment changes level trend intervention (interventions).”.exactly, segmented regression model structured follow:\\[Y = b_0 + b_1Time + b_2Intervention + b_3TimeSinceIntervention + e\\]includes least:outcome variable (Y);variable indicates time 1,2,…,t passed start series;dummy variable (0/1) observation collected (0) (1) intervention;variable measuring time 1,2,…,t passed since intervention occured, equal zero intervention.interpretation coefficients follows:\\(b_0\\) baseline level Time 0;Time (\\(b_1\\)) coefficient indicates trend (slope) intervention ( change outcome associated time unit increase).Intervention (\\(b_2\\)) coefficient indicates immediate effect (level change) induced intervention (last observation intervention first one ).Time Since Intervention (\\(b_3\\)) coefficient indicates “sustained effect”, .e., change trend intervention (effect time point passes intervention). measures difference slope line intervention. also possible calculate slope line intervention summing coefficients Time Time Since Treatment (\\(b_1 + b_3\\))good tutorial tecnique can found following link: https://ds4ps.org/pe4ps-textbook/docs/p-020-time-series.html.","code":""},{"path":"var.html","id":"var","chapter":"13 VAR","heading":"13 VAR","text":"VAR acronym stands Vector Autoregressive Model. common method analysis multivariate time series.can conceived way model system time series. VAR model, rigid distinction independent dependent variables, variable dependent independent. Besides endogenous variables dynamically interact, VAR model can include exogenous variables. Exogenous variables can impact endogenous variables, opposite true. make simple example, time series fans sold month may influenced quantity fans produced distributed industry, monthly temperature. purchase production fans can interact (endogenous variables), weather impacted processes, just impact external force (exogenous variable).make another example (paper Impact Politicization Health Online Misinformation Quality Information Vaccines), political debate certain topic - instance political debate led promulgation law mandatory vaccinations Italy - considered exogenous variable impacts news coverage topic spread problematic information Twitter. news media coverage Twitter discussions can considered part communication system dependent (news media set discussion agenda Twitter, also social media can stimulate news media coverage) assumed political debate led promulgation law mandatory vaccinations independent Twitter discussions topic.Stationary tests usually applied ascertain variables integrated (“” ARIMA model). case, variables differenced starting VAR analysis. preliminary analysis (instance testing cointegration) pre-processing can performed analysis. Next, number lags used selected. number can automatically identify automated methods (lag-length selection criteria methods). model evaluated. Results VAR model usually complicated, researchers relies statistical methods Granger causality test. Granger causality test, test developed nobel prize winner Clive Granger, applied study agenda setting processes. variable \\(X\\) said “Granger cause” another variable \\(Y\\) \\(Y\\) can better predicted past values \\(X\\) \\(Y\\) together past \\(Y\\) alone.Granger causality test statistical hypothesis test determining whether one time series useful forecasting another, first proposed 1969. Ordinarily, regressions reflect “mere” correlations, Clive Granger argued causality (…) tested measuring ability predict future values time series using prior values another time series. Since question “true causality” deeply philosophical, post hoc ergo propter hoc fallacy assuming one thing preceding another can used proof causation, (…) Granger test finds “predictive causality”.R package perform VAR modeling vars.Practical applications VAR modeling, including Granger causality, can found, instance, paper Assembling Networks Audiences Disinformation: Successful Russian IRA Twitter Accounts Built Followings,2015–2017 \nCoordinating Multi-Platform Disinformation Campaign: Internet Research Agency Activity Three U.S. Social Media Platforms, 2015 2017 link.","code":""},{"path":"var.html","id":"var-modeling-hands-on-tutorial","chapter":"13 VAR","heading":"13.1 VAR modeling hands-on tutorial","text":"","code":""},{"path":"var.html","id":"assumption-of-stationarity","chapter":"13 VAR","heading":"13.1.1 Assumption of stationarity","text":"Like time series techniques, VAR assumes series stationary. recap:stationary time series one whose properties depend time series observed. Thus, time series trends, seasonality, stationary — trend seasonality affect value time series different times. hand, white noise series stationary — matter observe , look much point time. […] general, stationary time series predictable patterns long-term. Time plots show series roughly horizontal (although cyclic behaviour possible), constant variance. Rob J. Hyndman George Athanasopoulos, “Forecasting: Principles Practice”.examples stationary non-stationary series.already learnt, pre-processing steps usually carried make stationary series stationary. instance, series linear trend can made stationary removing trend. seasonal series can seasonally adjusted. random-walk-like series can adjusted using differencing. Visual inspection statistical tests can performed support findings characteristics time series.pre-processing steps may unnecessary using vars library, function VAR (used fit VAR models) allows include trend seasonality components.","code":"\npar(mar=c(0,0,1,0))\nlayout(matrix(c(1,1,2,3,4,5), ncol = 2, nrow = 3, byrow = T))\n\nwn <- rnorm(n=1000)\n\nplot(ts(wn), main = \"Stationary Time Series (White Noise)\")\nabline(h = 0, col = \"red\", lwd = 2)\n\nnonvariance_stat <- rnorm(n=1000) * 1:1000\n\nplot(ts(nonvariance_stat), \n     main = \"Non stationary in variance\")\n\nlinear_trend <- rnorm(n=1000) + 0.05*1:1000\n\nplot(ts(linear_trend), \n     main = \"Non stationary in mean (Linear Trend)\")\n\nseasonality <- rnorm(n=24*30) + 0.2*rep(1:24, 30)\n\nplot(ts(seasonality), \n     main = \"Non stationary in mean (Seasonality)\")\n\nrandom_walk <- cumsum(sample(c(-100, 100), 1000, TRUE)) + rnorm(1000, sd = 44)\n\nplot(ts(random_walk), \n     main = \"Non stationary in mean (Random Walk)\")\npar(mar=c(0,0,1,0))\nlayout(matrix(c(1,1,2,3,4,5,6,7), ncol = 2, nrow = 4, byrow = T))\n\nplot(ts(wn), main = \"Stationary Time Series (White Noise)\")\nabline(h = 0, col = \"red\", lwd = 2)\n\nplot(ts(random_walk), \n     main = \"Non stationary in mean (Random Walk)\")\n\nplot(diff(ts(random_walk)), \n     main = \"Differenced\")\n\nplot(ts(linear_trend), \n     main = \"Non stationary in mean (Linear Trend)\")\n\nplot(ts(residuals(lm(linear_trend ~ seq(1, length(linear_trend), 1)))), \n     main = \"Detrended\")\n\nplot(ts(seasonality), \n     main = \"Non stationary in mean (Seasonality)\")\n\ndecomposed <- decompose(ts(seasonality, frequency = 24))\ndeseasonalized <- seasonality - decomposed$seasonal\n\nplot(ts(deseasonalized), \n     main = \"Deseasonalized\")"},{"path":"var.html","id":"other-assumptions-of-var-models-distribution-of-residuals","chapter":"13 VAR","heading":"13.1.2 Other assumptions of VAR models: distribution of residuals","text":"Besides stationarity linearity variables lagged values, VAR performed presence structural breaks. Structural breaks abrupt changes series’ mean overall process. can deal structural breaks different ways. example, might want split series different phases, using structural breaks breakpoints.crucial assumptions VAR residuals, required :Non serially correlatedNormal distributedHomoskedasticStatistical tests included vars package can used check assumptions.Another fundamental assumption VAR models absence cointegration series. Two series may cointegrated integrated order. Integrated series achieve stationarity differentiated. number differentiation needed achieve stationarity order integration (usually, one: (1) notation).Integrated series non-stationary, can happen linear combination integrated series stationary. case, said cointegrated. Although series follows seemingly random path, time, characterized equilibrium, driven common underlying process. Cointegration explained help story drunk dog:fact, drunk creature whose behavior follow random walk. Puppies, , wander aimlessly unleashed. new scent crosses puppy’s nose dictates direction pup’s next step. […] dog belongs drunk? drunk sets bar, wander aimlessly random-walk fashion. periodically intones “Oliver, ?”, Oliver interrupts aimless wandering bark. hears ; hears . thinks: “Oh can’t let get far ; ’ll lock ”. thinks, “Oh, can’t let get far ; ’ll wake middle night barking”. assessed far away moves partially close gap. Now neither drunk dog follows random walk; added formally call error-correction-mechanism steps. […] paths drunk dog still non-stationary. Significantly, despite nonstationarity paths, one might still say, “find , dog unlikely far away”. right, distance two paths stationary, walks woman dog said cointegrated (…). […] Notice cointegration probabilistic concept. dog leash, enforce fixed distance drunk dog. distance drunk dog instead random variable. stationary one, despite nonstationarity two paths. Murray, M. P. (1994). drunk dog: illustration cointegration error correction. American Statistician, 48(1), 37-39.Statistical tests used determine whether cointegration present. performed series integrated order integration (, cointegration can exists least two series integrated order). , cointegration tests performed. presence cointegration, Vector Error Correction Models (VECM) used instead VAR. VECM basically VAR model “error correction term”. topic (yet) covered tutorial.","code":""},{"path":"var.html","id":"var-fitting","chapter":"13 VAR","heading":"13.2 VAR fitting","text":"Let’s make simple example.\nUpload data:Let’s visually inspect series:variance look constant, can tentatively log-transform series work log-transformed data.check stationarity, can perform test Augmented Dickey-Fuller Test. Integration suggested “anti” series.series becomes stationary differencing.can recreate dataset including differentiated series. Using first difference transformation, lose first data point.Indeed, first point differentiated series obtained subtracting first point second one. second point differentiated series obtained subtracting second point third one, . Hence, differentiated dataset start second point time, first one original series. Therefore, need drop first data point dataset.fit VAR model use library vars.VAR models require researcher specify number lags include model. Roughly speaking, lags auto-regressive predictors. example, considering VAR model two variables \\(X\\) \\(Y\\), using 1 lag means use value \\(X_{t-1}\\) \\(Y_{t-1}\\) predict \\(X_t\\) \\(Y_t\\), 2 lags use \\(X_{t-1}\\), \\(X_{t-2}\\), \\(Y_{t-1}\\), \\(Y_{t-2}\\). choice lags important. function VARselect implements four different statistical tests find “best” number lags.case, criteria agree one lag.function fit model VAR, number lags specified using p parameter.can also include trend /seasonality, case doesn’t seem useful. also possible include exogenous variables. Exogenous variables usually defined variables external system: may affect system, affected system.VARselect function, use columns containing data (first column dataset contains dates).model looks like . can see, system linear regression models includes lagged predictors. analyzing results need confirm assumptions models met.serial.test test serially correlated errors. test okay significant. test significant.normality.test performs tests normality residuals. tests okay significant, like case.Another fundamental test performed function stability. test okay series stay within red bars. okay.arch.test assesses null hypothesis series residuals exhibits heteroscedasticity. Also test significant, good.assumptions met, can take look overall fit. doesn’t look amazing, assumptions met, let’s assume good enough let’s go ahead analysis.--good fit probably related fact variables VAR system highly correlated. coefficients statistically significant overall R-square small.Generally, coefficient table interpreted. Instead, VAR models interpreted using tools, Granger causality test.","code":"##             date_time anti   pro\n## 1 2021-03-12 23:59:00   37 14085\n## 2 2021-03-13 23:59:00  157  5219\n## 3 2021-03-14 23:59:00   40  5106\n## 4 2021-03-15 23:59:00   25  6898\n## 5 2021-03-16 23:59:00   24  2974\n## 6 2021-03-17 23:59:00   61  3230\nplot.ts(ts_tweets[, c(\"pro\", \"anti\")], \n        plot.type = \"multiple\",\n        main = \"tweets\")\nts_tweets$pro <- log(ts_tweets$pro)\nts_tweets$anti <- log(ts_tweets$anti)\n\nplot.ts(ts_tweets[, c(\"pro\", \"anti\")], \n        plot.type = \"multiple\",\n        main = \"tweets\")\ntseries::adf.test(ts_tweets$pro)## \n##  Augmented Dickey-Fuller Test\n## \n## data:  ts_tweets$pro\n## Dickey-Fuller = -3.9314, Lag order = 3, p-value = 0.02193\n## alternative hypothesis: stationary\ntseries::adf.test(ts_tweets$anti)## \n##  Augmented Dickey-Fuller Test\n## \n## data:  ts_tweets$anti\n## Dickey-Fuller = -2.101, Lag order = 3, p-value = 0.5338\n## alternative hypothesis: stationary\nanti <- diff(ts_tweets$anti, 1)\ntseries::adf.test(anti)## Warning in tseries::adf.test(anti): p-value smaller than printed p-value## \n##  Augmented Dickey-Fuller Test\n## \n## data:  anti\n## Dickey-Fuller = -4.7232, Lag order = 3, p-value = 0.01\n## alternative hypothesis: stationary\nlength(anti)## [1] 41\nlength(ts_tweets$pro)## [1] 42\n# drop the first data point\nts_tweets <- ts_tweets[-1,]\n\n# replance the anti series with the differentiated anti series\nts_tweets$anti <- anti\n# install.packages(\"vars\")\nlibrary(vars)## Loading required package: MASS## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select## Loading required package: strucchange## Loading required package: sandwich## \n## Attaching package: 'strucchange'## The following object is masked from 'package:stringr':\n## \n##     boundary## Loading required package: urca\nvars::VARselect(ts_tweets[,c(\"pro\", \"anti\")])## $selection\n## AIC(n)  HQ(n)  SC(n) FPE(n) \n##      1      1      1      1 \n## \n## $criteria\n##                  1           2           3           4          5           6           7\n## AIC(n) -2.92630038 -2.81403228 -2.64199556 -2.42923055 -2.5769659 -2.77801920 -2.68597480\n## HQ(n)  -2.83582733 -2.66324387 -2.43089178 -2.15781141 -2.2452314 -2.38596933 -2.23360957\n## SC(n)  -2.64875447 -2.35145577 -1.99438843 -1.59659281 -1.5592975 -1.57532025 -1.29824525\n## FPE(n)  0.05366012  0.06030449  0.07235601  0.09118664  0.0810697  0.06940455  0.08132081\n##                  8          9         10\n## AIC(n) -2.68018280 -2.5499629 -2.4424493\n## HQ(n)  -2.16750221 -1.9769669 -1.8091380\n## SC(n)  -1.10742264 -0.7921721 -0.4996280\n## FPE(n)  0.08986526  0.1167941  0.1564827\nvar_fit <- vars::VAR(ts_tweets[,c(\"pro\", \"anti\")], p = 1)\nsummary(var_fit)## \n## VAR Estimation Results:\n## ========================= \n## Endogenous variables: pro, anti \n## Deterministic variables: const \n## Sample size: 40 \n## Log Likelihood: -53.79 \n## Roots of the characteristic polynomial:\n## 0.4477 0.2503\n## Call:\n## vars::VAR(y = ts_tweets[, c(\"pro\", \"anti\")], p = 1)\n## \n## \n## Estimation results for equation pro: \n## ==================================== \n## pro = pro.l1 + anti.l1 + const \n## \n##         Estimate Std. Error t value Pr(>|t|)    \n## pro.l1   0.25131    0.16894   1.488    0.145    \n## anti.l1 -0.05992    0.06722  -0.891    0.379    \n## const    6.27985    1.41950   4.424 8.22e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Residual standard error: 0.3364 on 37 degrees of freedom\n## Multiple R-Squared: 0.06146, Adjusted R-squared: 0.01073 \n## F-statistic: 1.212 on 2 and 37 DF,  p-value: 0.3093 \n## \n## \n## Estimation results for equation anti: \n## ===================================== \n## anti = pro.l1 + anti.l1 + const \n## \n##         Estimate Std. Error t value Pr(>|t|)   \n## pro.l1   0.01161    0.38475   0.030  0.97610   \n## anti.l1 -0.44871    0.15309  -2.931  0.00576 **\n## const   -0.10750    3.23284  -0.033  0.97365   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Residual standard error: 0.7661 on 37 degrees of freedom\n## Multiple R-Squared: 0.2029,  Adjusted R-squared: 0.1598 \n## F-statistic: 4.709 on 2 and 37 DF,  p-value: 0.01507 \n## \n## \n## \n## Covariance matrix of residuals:\n##         pro   anti\n## pro  0.1132 0.0862\n## anti 0.0862 0.5870\n## \n## Correlation matrix of residuals:\n##         pro   anti\n## pro  1.0000 0.3345\n## anti 0.3345 1.0000\nvars::serial.test(var_fit)## \n##  Portmanteau Test (asymptotic)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 33.632, df = 60, p-value = 0.9977\nvars::normality.test(var_fit)## $JB\n## \n##  JB-Test (multivariate)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 3.9951, df = 4, p-value = 0.4067\n## \n## \n## $Skewness\n## \n##  Skewness only (multivariate)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 1.8704, df = 2, p-value = 0.3925\n## \n## \n## $Kurtosis\n## \n##  Kurtosis only (multivariate)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 2.1247, df = 2, p-value = 0.3456\nplot(vars::stability(var_fit))\nvars::arch.test(var_fit)## \n##  ARCH (multivariate)\n## \n## data:  Residuals of VAR object var_fit\n## Chi-squared = 44.188, df = 45, p-value = 0.5062\npar(mar=c(0,0,0,0))\nplot(var_fit)\nsummary(var_fit)## \n## VAR Estimation Results:\n## ========================= \n## Endogenous variables: pro, anti \n## Deterministic variables: const \n## Sample size: 40 \n## Log Likelihood: -53.79 \n## Roots of the characteristic polynomial:\n## 0.4477 0.2503\n## Call:\n## vars::VAR(y = ts_tweets[, c(\"pro\", \"anti\")], p = 1)\n## \n## \n## Estimation results for equation pro: \n## ==================================== \n## pro = pro.l1 + anti.l1 + const \n## \n##         Estimate Std. Error t value Pr(>|t|)    \n## pro.l1   0.25131    0.16894   1.488    0.145    \n## anti.l1 -0.05992    0.06722  -0.891    0.379    \n## const    6.27985    1.41950   4.424 8.22e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Residual standard error: 0.3364 on 37 degrees of freedom\n## Multiple R-Squared: 0.06146, Adjusted R-squared: 0.01073 \n## F-statistic: 1.212 on 2 and 37 DF,  p-value: 0.3093 \n## \n## \n## Estimation results for equation anti: \n## ===================================== \n## anti = pro.l1 + anti.l1 + const \n## \n##         Estimate Std. Error t value Pr(>|t|)   \n## pro.l1   0.01161    0.38475   0.030  0.97610   \n## anti.l1 -0.44871    0.15309  -2.931  0.00576 **\n## const   -0.10750    3.23284  -0.033  0.97365   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Residual standard error: 0.7661 on 37 degrees of freedom\n## Multiple R-Squared: 0.2029,  Adjusted R-squared: 0.1598 \n## F-statistic: 4.709 on 2 and 37 DF,  p-value: 0.01507 \n## \n## \n## \n## Covariance matrix of residuals:\n##         pro   anti\n## pro  0.1132 0.0862\n## anti 0.0862 0.5870\n## \n## Correlation matrix of residuals:\n##         pro   anti\n## pro  1.0000 0.3345\n## anti 0.3345 1.0000"},{"path":"var.html","id":"granger-causality-test","chapter":"13 VAR","heading":"13.3 Granger causality test","text":"use causality function perform Granger causality test. Granger causality test basic inferential tool available VAR analysis. time series X considered Granger cause another time series Y past values X Y predicts Y significantly better past values Y alone. analysis frequently performed communication studies focused agenda setting phenomena.Let’s see ’s Granger causality series. researcher needs specify “cause” variable (sometimes, researcher may hypothesis ). case, try . granger causality effect contemporaneous relationship (instant causation).","code":"\nvars::causality(var_fit, cause = \"pro\")## $Granger\n## \n##  Granger causality H0: pro do not Granger-cause anti\n## \n## data:  VAR object var_fit\n## F-Test = 0.00090982, df1 = 1, df2 = 74, p-value = 0.976\n## \n## \n## $Instant\n## \n##  H0: No instantaneous causality between: pro and anti\n## \n## data:  VAR object var_fit\n## Chi-squared = 4.0242, df = 1, p-value = 0.04485\nvars::causality(var_fit, cause = \"anti\")## $Granger\n## \n##  Granger causality H0: anti do not Granger-cause pro\n## \n## data:  VAR object var_fit\n## F-Test = 0.79448, df1 = 1, df2 = 74, p-value = 0.3756\n## \n## \n## $Instant\n## \n##  H0: No instantaneous causality between: anti and pro\n## \n## data:  VAR object var_fit\n## Chi-squared = 4.0242, df = 1, p-value = 0.04485"},{"path":"readings-and-bibliographical-references.html","id":"readings-and-bibliographical-references","chapter":"14 Readings and Bibliographical References","heading":"14 Readings and Bibliographical References","text":"","code":""},{"path":"readings-and-bibliographical-references.html","id":"mandatory","chapter":"14 Readings and Bibliographical References","heading":"14.1 Mandatory","text":"Shin, Y. (2017). Time series analysis social sciences: fundamentals. Univ California Press.","code":""},{"path":"readings-and-bibliographical-references.html","id":"other-readings","chapter":"14 Readings and Bibliographical References","heading":"14.2 Other readings","text":"Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, ., & Yang, J. (2019). Temporal Turn Communication Research: Time Series Analyses Using Computational Approaches. International Journal Communication (19328036), 13.Brodersen KH, Gallusser F, Koehler J, Remy N, Scott SL. Inferring causal impact using Bayesian structural time-series models. Annals Applied Statistics, 2015, Vol. 9, . 1, 247-274.Gaubatz, K. T. (2014). Survivor’s Guide R: Introduction Uninitiated Unnerved. SAGE Publications.Liboschik, T., Fokianos, K., & Fried, R. (2017). tscount: R package analysis count time series following generalized linear models. Journal Statistical Software, 82(1), 1-51.Schaffer, . L., Dobbins, T. ., & Pearson, S. . (2021). Interrupted time series analysis using autoregressive integrated moving average (ARIMA) models: guide evaluating large-scale health interventions. BMC medical research methodology, 21(1), 1-12.Zivot E., Wang J. (2003) Unit Root Tests. : Modeling Financial Time Series S-Plus®. Springer, New York, NY. https://doi.org/10.1007/978-0-387-21763-5_4","code":""},{"path":"readings-and-bibliographical-references.html","id":"useful-resources","chapter":"14 Readings and Bibliographical References","heading":"14.3 Useful resources","text":"Cross Validated statistics-related questionsStackoverflow coding-related questions","code":""}]
