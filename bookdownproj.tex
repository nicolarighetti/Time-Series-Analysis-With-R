% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Time Series Analysis With R},
  pdfauthor={Nicola Righetti},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

\title{Time Series Analysis With R}
\author{Nicola Righetti}
\date{2021-08-12}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{time-series-analysis-with-r}{%
\section{Time Series Analysis With R}\label{time-series-analysis-with-r}}

\emph{This book will be updated as the course goes on.}

\hypertarget{objectives}{%
\subsection{Objectives}\label{objectives}}

This course is a practical introduction to time series analysis with R.

It will introduce students to:

\begin{itemize}
\tightlist
\item
  The specificity of time series data;
\item
  The free statistical software R to conduct time series analysis;
\item
  Some of the main univariate and multivariate techniques to analyze time series data.
\end{itemize}

At the end of the course, the students are expected to know the specificity of time series data and to be able to use R to perform simple time series analysis by applying the techniques described during the course.

\hypertarget{lectures}{%
\subsection{Lectures}\label{lectures}}

\href{https://ufind.univie.ac.at/de/course.html?lv=220050\&semester=2021S}{12 lectures (Thursday 11:30-13:00)}.

Structure of the course:

\begin{itemize}
\tightlist
\item
  \textbf{Theoretical concepts}: this part of the course will introduce students to the main theoretical concepts of time series analysis;
\item
  \textbf{R Tutorial}: this part of the course consists in a hands-on tutorial on the R functions necessary to perform time series analysis. Every part of a time series analysis project will be taken into account, including data wrangling, visual representation, and statistical analysis;
\item
  \textbf{Individual/Group work}: this part of the course consists in individual and group work based on the application of the theoretical and practical knowledge described in the previous part of the course
\end{itemize}

\hypertarget{home-work}{%
\subsection{Home Work}\label{home-work}}

Theoretical concepts can be studied, but you have to practice in order to learn R.

\hypertarget{assessment}{%
\subsection{Assessment}\label{assessment}}

\begin{itemize}
\tightlist
\item
  Assignments distributed during the course, dealing with demonstrating the understanding of key concepts (30\%).
\item
  A final data analysis project where participants will apply the knowledge and techniques learned during the course (70\%).
\end{itemize}

\hypertarget{syllabus-and-readings}{%
\subsection{Syllabus and readings}\label{syllabus-and-readings}}

This open book is specifically created for the \emph{220050-1 SE SE Advanced Data Analysis 2 (2021S)} course. It includes both theoretical concepts and the R tutorial with the necessary code to perform all the operations we are going to learn.

The book also includes hyperlinks to additional free resources and readings.
The mandatory readings will be listed in the \emph{Readings} section of the book.

A new part of the book will be uploaded online every weeks, following the program of the lessons.

The link to this book is the following: \href{https://nicolarighetti.github.io/Time-Series-Analysis-With-R/}{Time-Series-Analysis-With-R}.

\hypertarget{further-information-and-support}{%
\subsection{Further information and support}\label{further-information-and-support}}

For any information, communication, or request for clarification, you can reach out to me at \href{https://ufind.univie.ac.at/de/person.html?id=113451}{my University of Vienna address}.

\hypertarget{getting-started-with-r}{%
\section{Getting started with R}\label{getting-started-with-r}}

\hypertarget{rstudio-interface-and-data}{%
\subsection{RStudio Interface and Data}\label{rstudio-interface-and-data}}

\hypertarget{download-and-install-rstudio}{%
\subsubsection{Download and Install RStudio}\label{download-and-install-rstudio}}

This course is based on the statistical software R. R is easier to use in the development environment RStudio (it works on both Windows, Apple, and other OS).

It is possible to \href{https://rstudio.com/products/rstudio/download/}{download a free version of RStudio Desktop} from the official websites.

You might also use a free \emph{online} version of RStudio by registering to the \href{https://rstudio.cloud}{RStudio Cloud free plan}. However, the free plan gives you just 15 hours per months. Our lessons take 4.5 hours per month, and since you also need to practice, the best choice is to install RStudio and R on your computer.

Now we are going to see how to get started with RStudio Desktop.

First, \href{https://rstudio.com/products/rstudio/download/}{download and install a free version of RStudio Desktop} and open the software.

\hypertarget{create-a-rstudio-project-and-import-data}{%
\subsubsection{Create a RStudio Project and Import data}\label{create-a-rstudio-project-and-import-data}}

When starting a data analysis project with RStudio, we create a new dedicated environment where we will keep all the scripts (files containing the code to perform the analysis), data sets, and outputs of the analysis (such as plots and tables). This dedicated work-space is simply called a \emph{project}.

To create a new project with RStudio, follows these steps:

\begin{itemize}
\tightlist
\item
  click on \emph{File} (on the top left);
\item
  then, click on \emph{New Project};
\item
  select \emph{New Directory}, and \emph{New Project};
\item
  choose a folder for the project, and give a name to your project. You can use the name \emph{Time-Series-Analysis-With-R};
\end{itemize}

In this way, it will be created a new folder for the project, in the main folder specified in the previous step. In this folder, you will find a file \textbf{.Rproj}, the name of which is the name you assigned to your project. To work on this project, you just need to open the \emph{.Rproj} file.

\includegraphics[width=33.57in]{images/r-studio-project-creation}

\hypertarget{create-a-script}{%
\subsubsection{Create a Script}\label{create-a-script}}

Once the project has been created, we can open a new \textbf{script} and save it.

A script is a file containing code. We can create a first script named \emph{basic-r-syntax}, where you will test the basic code we are going to see. The script will be saved with extension \emph{.r}.

You can open, change, and save the file every time you work on it.
To save your code is important, otherwise you would have to write the same code every time you work on the project!

\includegraphics[width=29.9in]{images/create-script}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{videos/create-script.mov}
\caption{Create and save a script}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{videos/update-script.mov}
\caption{Update a script and run code}
\end{figure}

\hypertarget{the-rstudio-user-interface}{%
\subsubsection{The RStudio User Interface}\label{the-rstudio-user-interface}}

The interface of RStudio is organized in four main quadrants:

\begin{itemize}
\tightlist
\item
  The top-left quadrant is the editor. Here you can create or open a script and compose the R commands.
\item
  The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. The bottom-right quadrant is a window for graphics output, but it also has tabs to manage your file directories, R packages, and the R Help facility.
\item
  On the bottom left is the R Console window, where the code gets executed and the output is produced. You can run the commands, sending the code from the editor to the console, by highlighting it and hitting the \emph{Run} button, or the Ctrl-Enter key combination. It is also possible to type and run commands directly into the console window (in this case, nothing will be saved).
\item
  The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. There is the \emph{file} tab, where you can navigate files and folders and find, for instance, the data sets you want to upload.
\item
  The bottom-right quadrant is a window for graphics output. Here you can visualize your plots. There is also a tab for the R packages, and the R Help facility.
\end{itemize}

\includegraphics[width=35.5in]{images/r-environment}

\hypertarget{load-and-save-data}{%
\subsubsection{Load and Save Data}\label{load-and-save-data}}

To load data into R you can click on the \textbf{file} window in the top-right quadrant, navigate your files/folders, and once you have found your data set file, you can just click it and follow the semi-automated import procedure.

\includegraphics[width=32.4in]{images/load-data}

\includegraphics[width=25.4in]{images/import-data-1}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{videos/import-data.mov}
\caption{Import Data}
\end{figure}

Otherwise, you can upload a data set by using a function. For instance, to import a \emph{csv} file, one of the most common format for data sets, it can be employed the function \textbf{read.csv}. The main argument of this function is the \emph{path} of the file you want to upload.
To specify the file path, consider that you are working within a specific environment, that is, your \emph{working directory} is the folder of the project (you can double check the working directory you are working in, by running the command \textbf{getwd()}). Thus, to indicate the path of the data set you want to upload, you can write a dot followed by a slash \textbf{./}, followed by the path of the data set \emph{inside the working directory}. For instance, in the case below, the data set is saved in a folder named \emph{data} inside the working directory. The name of the data set is \emph{tweets\_vienna} and its extension is \emph{.csv}. Therefore, the code to upload the file is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fake_news <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/fake-news-stories-over-time-20210111144200.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To save data there are a few options. Generally, if you want to save a data set, you can opt for the \emph{.csv} or the \emph{.rds} format. The \emph{.rds} format is only readable by R, while the \emph{.csv} is a ``universal'' format (you can read it with Excel, for instance).

To save a file as \emph{.csv} it can be used the function \textbf{write.csv}. The main arguments of this function are the name of the object that has to be saved, the path to the folder where the object will be saved, and the name we want to assign to the file.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(fake_news, }\DataTypeTok{file =} \StringTok{"./data/fake_news.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To save \emph{.rds} file the procedure is similar, but the \textbf{saveRDS} function has to be employed. Instead, to read an \emph{rds} file, the appropriate function is \textbf{readRDS}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{saveRDS}\NormalTok{(fake_news, }\DataTypeTok{file =} \StringTok{"./data/fake_news.rds"}\NormalTok{)}

\NormalTok{fake_news <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\StringTok{"./data/fake_news.rds"}\NormalTok{)   }\CommentTok{# read a .rds file}
\end{Highlighting}
\end{Shaded}

In the code above you can notice an hash mark sign followed by some text. It is a \textbf{comment}. Comments are textual content used to describe the code in order to make it easier to understand and reuse it. Comments are written after the \textbf{hash mark sign (\#)}, because the text written after the hash mark sign is ignored by R: you can read the comments, but R does not consider them as code.

\hypertarget{create-new-folders}{%
\subsubsection{Create new Folders}\label{create-new-folders}}

It is a good practice to create, in the main folder of the project, sub-folders dedicated to different type of files used in the project, such as a folder ``data'' for the data sets.

To create a new folder you can go to the \emph{Files} windows in the RStudio interface, click \textbf{New Folder}, and give it a name.

\includegraphics[width=23.99in]{images/new-folder}

\hypertarget{basic-r}{%
\subsection{Basic R}\label{basic-r}}

\hypertarget{objects}{%
\subsubsection{Objects}\label{objects}}

An \emph{object} is an R entity composed of a name and a value.

The \textbf{arrow (\textless-)} sign is used to \emph{create objects} and \emph{assign a value} to an object (or to change or ``update'' its previous value).

Example: create an object with name ``object\_consisting\_of\_a\_number'' and value equal 2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{object_consisting_of_a_number <-}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

Enter the name of the object in the console and run the command: the value assigned to the object will be printed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{object_consisting_of_a_number}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

The object is equal to its value. Therefore, for instance, an object with a numerical value can be used to perform arithmetical operations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{object_consisting_of_a_number }\OperatorTok{*}\StringTok{ }\DecValTok{10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20
\end{verbatim}

The value of an object can be transformed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{object_consisting_of_a_number <-}\StringTok{ }\NormalTok{object_consisting_of_a_number }\OperatorTok{*}\StringTok{ }\DecValTok{10}

\NormalTok{object_consisting_of_a_number}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20
\end{verbatim}

An object can also represent a \textbf{function}.

Example: create an object for the \emph{sum} (addition) function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{function_sum <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y)\{}
\NormalTok{  result <-}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{y}
  \KeywordTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function can now be applied to two numerical values:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{function_sum}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

Actually, we don't need this function, since mathematical functions are already implemented in R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{5} \OperatorTok{+}\StringTok{ }\DecValTok{7}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{*}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{3}\OperatorTok{^}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

The value of an object can be a number, a function, but also a \textbf{vector}.
Vectors are sequences of values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector_of_numbers <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector_of_numbers}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10
\end{verbatim}

A vector of numbers can be the argument of mathematical operations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector_of_numbers }\OperatorTok{*}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  2  4  6  8 10 12 14 16 18 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector_of_numbers }\OperatorTok{+}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  4  5  6  7  8  9 10 11 12 13
\end{verbatim}

Other R objects are \emph{matrix}, \emph{list}, and \emph{data.frame.}

A \textbf{matrix} is a table composed of rows and columns containing only numerical values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a_matrix <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{data =} \DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{10}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{5}\NormalTok{)}

\NormalTok{a_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1] [,2] [,3] [,4] [,5]
##  [1,]    1   11   21   31   41
##  [2,]    2   12   22   32   42
##  [3,]    3   13   23   33   43
##  [4,]    4   14   24   34   44
##  [5,]    5   15   25   35   45
##  [6,]    6   16   26   36   46
##  [7,]    7   17   27   37   47
##  [8,]    8   18   28   38   48
##  [9,]    9   19   29   39   49
## [10,]   10   20   30   40   50
\end{verbatim}

A \textbf{list} is just a list of other objects.
For instance, this list includes a numerical value, a vectors of numbers, and a matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(object_consisting_of_a_number, vector_of_numbers, a_matrix)}

\NormalTok{a_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 20
## 
## [[2]]
##  [1]  1  2  3  4  5  6  7  8  9 10
## 
## [[3]]
##       [,1] [,2] [,3] [,4] [,5]
##  [1,]    1   11   21   31   41
##  [2,]    2   12   22   32   42
##  [3,]    3   13   23   33   43
##  [4,]    4   14   24   34   44
##  [5,]    5   15   25   35   45
##  [6,]    6   16   26   36   46
##  [7,]    7   17   27   37   47
##  [8,]    8   18   28   38   48
##  [9,]    9   19   29   39   49
## [10,]   10   20   30   40   50
\end{verbatim}

A \textbf{data.frame} is like a matrix that can contain \emph{numbers} but also other types of data, such as \emph{characters} (a textual type of data), or \emph{factors} (unordered categorical variables, such as gender, or ordered categories, such as low, medium, high).

Data sets are usually stored in data.frame. For instance, if you import a \emph{csv} or an \emph{Excel} file in R, the corresponding R object is a \emph{data.frame}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is an object (vector) consisting of a series of numerical values}
\NormalTok{numerical_vector <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{14}\NormalTok{)}
\NormalTok{numerical_vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is another object (vector) consisting of a series of categorical values}
\NormalTok{categorical_vector <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Monday"}\NormalTok{, }\StringTok{"Tuesday"}\NormalTok{, }\StringTok{"Monday"}\NormalTok{, }\StringTok{"Tuesday"}\NormalTok{, }\StringTok{"Monday"}\NormalTok{, }\StringTok{"Wednesday"}\NormalTok{,}\StringTok{"Thursday"}\NormalTok{, }\StringTok{"Wednesday"}\NormalTok{, }\StringTok{"Thursday"}\NormalTok{, }\StringTok{"Saturday"}\NormalTok{, }\StringTok{"Sunday"}\NormalTok{, }\StringTok{"Friday"}\NormalTok{, }\StringTok{"Saturday"}\NormalTok{, }\StringTok{"Sunday"}\NormalTok{)}
\NormalTok{categorical_vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Monday"    "Tuesday"   "Monday"    "Tuesday"   "Monday"   
##  [6] "Wednesday" "Thursday"  "Wednesday" "Thursday"  "Saturday" 
## [11] "Sunday"    "Friday"    "Saturday"  "Sunday"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is an object consisting of a data.frame, created combining vectors through the function "data.frame"}
\NormalTok{a_dataframe <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"first_variable"}\NormalTok{ =}\StringTok{ }\NormalTok{numerical_vector,}
                          \StringTok{"second_variable"}\NormalTok{ =}\StringTok{ }\NormalTok{categorical_vector)}
\NormalTok{a_dataframe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    first_variable second_variable
## 1               1          Monday
## 2               2         Tuesday
## 3               3          Monday
## 4               4         Tuesday
## 5               5          Monday
## 6               6       Wednesday
## 7               7        Thursday
## 8               8       Wednesday
## 9               9        Thursday
## 10             10        Saturday
## 11             11          Sunday
## 12             12          Friday
## 13             13        Saturday
## 14             14          Sunday
\end{verbatim}

To access a specific column of a data.frame, you can use the name of the data.frame, the dollar symbol \textbf{\$}, and the name of the column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a_dataframe}\OperatorTok{$}\NormalTok{first_variable}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a_dataframe}\OperatorTok{$}\NormalTok{second_variable}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] Monday    Tuesday   Monday    Tuesday   Monday    Wednesday
##  [7] Thursday  Wednesday Thursday  Saturday  Sunday    Friday   
## [13] Saturday  Sunday   
## 7 Levels: Friday Monday Saturday Sunday Thursday ... Wednesday
\end{verbatim}

It is possible to add columns to a data.frame by writing:

\begin{itemize}
\tightlist
\item
  the name of the data.frame
\item
  the dollar sign
\item
  a name for the new column
\item
  the \emph{arrow sign \textless-}
\item
  a vector of values to be stored in the new column (it has to have length equal to the other vectors composing the data.frame)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a_dataframe}\OperatorTok{$}\NormalTok{a_new_variable <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{261}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{234}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{267}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a_dataframe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    first_variable second_variable a_new_variable
## 1               1          Monday             12
## 2               2         Tuesday            261
## 3               3          Monday             45
## 4               4         Tuesday             29
## 5               5          Monday             54
## 6               6       Wednesday            234
## 7               7        Thursday             45
## 8               8       Wednesday             42
## 9               9        Thursday              6
## 10             10        Saturday            267
## 11             11          Sunday             87
## 12             12          Friday              3
## 13             13        Saturday             12
## 14             14          Sunday              9
\end{verbatim}

It is possible to visualize the first few rows of a data.frame by using the function \emph{head}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(a_dataframe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   first_variable second_variable a_new_variable
## 1              1          Monday             12
## 2              2         Tuesday            261
## 3              3          Monday             45
## 4              4         Tuesday             29
## 5              5          Monday             54
## 6              6       Wednesday            234
\end{verbatim}

\includegraphics[width=14.04in]{images/data-frame-example}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{videos/data-frame-exercise-1.mov}
\caption{Exercise: visualize the first rows of a data.frame and access its columns}
\end{figure}

\hypertarget{functions}{%
\subsubsection{Functions}\label{functions}}

A \textbf{function} is a coded operation that applies to an object (e.g.: a number, a textual feature etc.) to transform it based on specific rules. A function has a name (the name of the function) and some \emph{arguments}. Among the arguments of a function there is always an \emph{object} or a value, for instance a numerical value, which is the content the function is applied to, and other possible arguments (either mandatory or optional).

Functions are operations applied to objects that give a certain output. E.g.: the arithmetical operation ``addition'' is a function that applies to two or more numbers to give, as its output, their sum. The \emph{arguments} of the ``sum'' function are the numbers that are added together.

The name of the function is written out of \emph{parentheses}, and the arguments of the function inside the parentheses:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8
\end{verbatim}

Arguments of functions can be numbers but also textual features. For instance, the function \emph{paste} creates a string composed of the strings that it takes as arguments.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\StringTok{"the"}\NormalTok{, }\StringTok{"cat"}\NormalTok{, }\StringTok{"is"}\NormalTok{, }\StringTok{"at"}\NormalTok{, }\StringTok{"home"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "the cat is at home"
\end{verbatim}

In R you can sometimes find a ``nested'' syntax, which can be confusing. The best practice is to keep things as simple as possible.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this comment, written after the hash mark, describe what is going on here: two "paste" function nested together have been used (improperly! because they make the code more complicated than necessary) to show how functions can be nested together. It would have been better to use the "paste" function just one time!}
\KeywordTok{paste}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"the"}\NormalTok{, }\StringTok{"cat"}\NormalTok{, }\StringTok{"is"}\NormalTok{, }\StringTok{"at"}\NormalTok{, }\StringTok{"home"}\NormalTok{), }\StringTok{"and"}\NormalTok{, }\StringTok{"sleeps"}\NormalTok{, }\StringTok{"on"}\NormalTok{, }\StringTok{"the"}\NormalTok{, }\StringTok{"sofa"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "the cat is at home and sleeps on the sofa"
\end{verbatim}

To sum up, functions manipulate and transform objects. Data wrangling, data visualization, as well as data analysis, are performed through functions.

\hypertarget{data-types}{%
\subsubsection{Data Types}\label{data-types}}

Variables can have different R \emph{formats}, such as:

\begin{itemize}
\tightlist
\item
  \textbf{double}: numbers that include decimals (0.1, 5.676, 121.67). This format is appropriate for \emph{continuous variables};
\item
  \textbf{integer}: such as 1, 2, 3, 10, 400. It is a format suitable to \emph{count data};
\item
  \textbf{factors}: for categorical variables. Factors can be ordered (e.g.: level of agreement: ``high'', ``medium'', ``low''), or not (e.g.: hair colors ``blond'', ``dark brown'', ``brown'');
\item
  \textbf{characters}: textual labels;
\item
  \textbf{logicals}: the format of logical values (i.e.: TRUE and FALSE)
\item
  \textbf{dates}: used to represent days;
\item
  \textbf{POSIX}: a class of R format to represent dates and times.
\end{itemize}

\begin{figure}
\includegraphics[width=33.54in]{images/r-data-format} \caption{R data formats. Tables from Gaubatz, K. T. (2014). [A Survivor's Guide to R: An Introduction for the Uninitiated and the Unnerved](https://us.sagepub.com/en-us/nam/a-survivors-guide-to-r/book242607). SAGE Publications.}\label{fig:unnamed-chunk-39}
\end{figure}

It is better to specify the appropriate type of data when importing a data set. In the example below, the data format are specified by using the import process of RStudio.

Notice that the data of type ``date'' requires users to specify the additional information regarding the format of the dates. Indeed, dates can be written in many different ways, and to read dates in R it is necessary to specify the structure of the date. In the example, dates are in the format Year-Month-Day, which is represented in R as ``\%Y-\%m-\%d'' (further details will be provided in another section of the book).

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{videos/import-data-types.mov}
\caption{Import data and specify data types}
\end{figure}

\hypertarget{excercise}{%
\subsubsection{Excercise}\label{excercise}}

\begin{itemize}
\tightlist
\item
  Upload the data set ``election news small'', using the appropriate data format;
\item
  Open the script ``basic-r-script'' and perform the following operations:

  \begin{itemize}
  \tightlist
  \item
    Check the first few rows of the data set;
  \item
    Access the single columns;
  \item
    Save the data frame with the name ``election\_news\_small\_test'' in the folder ``data'' by using the function ``write.csv'' (to review the procedure go to the section ``Load and Save Data'' on this book);
  \item
    Comment the code (the comments have to be written after the hash sign \emph{\#});
  \item
    Save the script.
  \end{itemize}
\end{itemize}

\hypertarget{basic-data-wrangling-with-tidyverse}{%
\section{Basic Data Wrangling with Tidyverse}\label{basic-data-wrangling-with-tidyverse}}

\href{https://en.wikipedia.org/wiki/Data_wrangling}{Data wrangling} \emph{is the process of transforming and mapping data from one ``raw'' data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.}

\href{https://link.springer.com/referenceworkentry/10.1007\%2F978-3-319-63962-8_9-1}{Another definition} is as follows: \emph{Data wrangling is the process of profiling and transforming datasets to ensure they are actionable for a set of analysis tasks. One central goal is to make data usable: to put data in a form that can be parsed and manipulated by analysis tools. Another goal is to ensure that data is responsive to the intended analyses: that the data contain the necessary information, at an acceptable level of description and correctness, to support successful modeling and decision-making.}

How to ``manipulate'' data sets in R:

\begin{itemize}
\tightlist
\item
  use basic R functions;
\item
  employ specific libraries such as \textbf{tidyverse}. Tidyverse is an R library composed of functions that allow users to perform basic and advanced data science operations. \url{https://www.tidyverse.org}.
\end{itemize}

In R, a \textbf{library (or ``package'')} is a coherent collection of functions, usually created for specific purposes.

To work with the \emph{tidyverse} library, it is necessary to install it first, by using the following command: \emph{install.packages(``tidyverse'')}.

After having installed tidyverse (or any other library), it is necessary to load it, so as we can work with its functions in the current R session:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# to load a library used the command library(NAME-OF-THE-LIBRARY)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -- Attaching packages ----------------------------- tidyverse 1.3.0 --
\end{verbatim}

\begin{verbatim}
## v ggplot2 3.3.2     v purrr   0.3.4
## v tibble  3.0.4     v dplyr   1.0.2
## v tidyr   1.1.2     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.0
\end{verbatim}

\begin{verbatim}
## -- Conflicts -------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
\end{verbatim}

Besides using the function \emph{install.packages(NAME-OF-THE-LIBRARY)} by using a line of code, it is also possible to use the RStudio interface.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{videos/install-load-library.mov}
\caption{Install and Load a Library}
\end{figure}

\hypertarget{the-pipe-operator}{%
\subsection{The Pipe Operator \%\textgreater\%}\label{the-pipe-operator}}

Tidyverse has a peculiar syntax that makes use of the so-called \href{https://style.tidyverse.org/pipes.html}{\textbf{pipe}} operator \textbf{\%\textgreater\%}, like in the following example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a_dataframe }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(second_variable) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(a_new_variable))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` ungrouping output (override with `.groups` argument)
\end{verbatim}

\begin{verbatim}
## # A tibble: 7 x 2
##   second_variable  mean
##   <fct>           <dbl>
## 1 Friday            3  
## 2 Monday           37  
## 3 Saturday        140. 
## 4 Sunday           48  
## 5 Thursday         25.5
## 6 Tuesday         145  
## 7 Wednesday       138
\end{verbatim}

To manipulate data sets we can rely on the functions included in \href{https://dplyr.tidyverse.org}{\textbf{dplyr}}: \emph{a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges}, such as \emph{mutate}, \emph{rename}, \emph{summarize}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}
\NormalTok{tweets <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/tweets_covid_small.csv"}\NormalTok{, }
    \DataTypeTok{col_types =} \KeywordTok{cols}\NormalTok{(}\DataTypeTok{created_at =} \KeywordTok{col_datetime}\NormalTok{(}\DataTypeTok{format =} \StringTok{"%Y-%m-%d %H:%M:%S"}\NormalTok{), }
        \DataTypeTok{retweet_count =} \KeywordTok{col_integer}\NormalTok{()))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(tweets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   created_at          screen_name   source             retweet_count
##   <dttm>              <chr>         <chr>                      <int>
## 1 2021-03-24 08:53:52 DoYourThingUK Twitter for iPhone             0
## 2 2021-03-24 08:53:25 DoYourThingUK Twitter for iPhone             0
## 3 2021-03-24 08:53:52 AlexS1595     Twitter for iPhone             3
## 4 2021-03-24 08:53:51 MakesworthAcc Twitter Web App                0
## 5 2021-03-24 08:53:39 MakesworthAcc Twitter Web App                4
## 6 2021-03-24 08:53:51 GGrahambute   Twitter for iPad              96
\end{verbatim}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{videos/upload-covid-tweets-small.mov}
\caption{Import data and specify data types (dates and times)}
\end{figure}

\hypertarget{mutate}{%
\subsection{Mutate}\label{mutate}}

The function \textbf{mutate} adds new variables to a data.frame or overwrites existing variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets <-}\StringTok{ }\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{log_retweet_count =} \KeywordTok{log}\NormalTok{(retweet_count))}
  
\KeywordTok{head}\NormalTok{(tweets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   created_at          screen_name source retweet_count
##   <dttm>              <chr>       <chr>          <int>
## 1 2021-03-24 08:53:52 DoYourThin~ Twitt~             0
## 2 2021-03-24 08:53:25 DoYourThin~ Twitt~             0
## 3 2021-03-24 08:53:52 AlexS1595   Twitt~             3
## 4 2021-03-24 08:53:51 Makesworth~ Twitt~             0
## 5 2021-03-24 08:53:39 Makesworth~ Twitt~             4
## 6 2021-03-24 08:53:51 GGrahambute Twitt~            96
## # ... with 1 more variable: log_retweet_count <dbl>
\end{verbatim}

\hypertarget{rename}{%
\subsection{Rename}\label{rename}}

Rename is a function to change the name of columns (sometimes it could be useful).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets <-}\StringTok{ }\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# rename (new_name = old_name)}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{device =}\NormalTok{ source)}

\KeywordTok{head}\NormalTok{(tweets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   created_at          screen_name device retweet_count
##   <dttm>              <chr>       <chr>          <int>
## 1 2021-03-24 08:53:52 DoYourThin~ Twitt~             0
## 2 2021-03-24 08:53:25 DoYourThin~ Twitt~             0
## 3 2021-03-24 08:53:52 AlexS1595   Twitt~             3
## 4 2021-03-24 08:53:51 Makesworth~ Twitt~             0
## 5 2021-03-24 08:53:39 Makesworth~ Twitt~             4
## 6 2021-03-24 08:53:51 GGrahambute Twitt~            96
## # ... with 1 more variable: log_retweet_count <dbl>
\end{verbatim}

The previous two steps can be performed at the same time, by concatenating the operations through the \emph{pipe \%\textgreater\%} operator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load again the data set}
\KeywordTok{library}\NormalTok{(readr)}
\NormalTok{tweets <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/tweets_covid_small.csv"}\NormalTok{, }
    \DataTypeTok{col_types =} \KeywordTok{cols}\NormalTok{(}\DataTypeTok{created_at =} \KeywordTok{col_datetime}\NormalTok{(}\DataTypeTok{format =} \StringTok{"%Y-%m-%d %H:%M:%S"}\NormalTok{), }
        \DataTypeTok{retweet_count =} \KeywordTok{col_integer}\NormalTok{()))}

\NormalTok{tweets <-}\StringTok{ }\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{log_retweet_count =} \KeywordTok{log}\NormalTok{(retweet_count}\OperatorTok{+}\DecValTok{1}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{device =}\NormalTok{ source)}

\KeywordTok{head}\NormalTok{(tweets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   created_at          screen_name device retweet_count
##   <dttm>              <chr>       <chr>          <int>
## 1 2021-03-24 08:53:52 DoYourThin~ Twitt~             0
## 2 2021-03-24 08:53:25 DoYourThin~ Twitt~             0
## 3 2021-03-24 08:53:52 AlexS1595   Twitt~             3
## 4 2021-03-24 08:53:51 Makesworth~ Twitt~             0
## 5 2021-03-24 08:53:39 Makesworth~ Twitt~             4
## 6 2021-03-24 08:53:51 GGrahambute Twitt~            96
## # ... with 1 more variable: log_retweet_count <dbl>
\end{verbatim}

To check the data format of the variables stored in the data.frame, it can be used the command \emph{str()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(tweets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## tibble [100 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ created_at       : POSIXct[1:100], format: "2021-03-24 08:53:52" ...
##  $ screen_name      : chr [1:100] "DoYourThingUK" "DoYourThingUK" "AlexS1595" "MakesworthAcc" ...
##  $ device           : chr [1:100] "Twitter for iPhone" "Twitter for iPhone" "Twitter for iPhone" "Twitter Web App" ...
##  $ retweet_count    : int [1:100] 0 0 3 0 4 96 0 1 0 3 ...
##  $ log_retweet_count: num [1:100] 0 0 1.39 0 1.61 ...
##  - attr(*, "spec")=
##   .. cols(
##   ..   created_at = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
##   ..   screen_name = col_character(),
##   ..   source = col_character(),
##   ..   retweet_count = col_integer()
##   .. )
\end{verbatim}

Sometimes variables are stored in the data.frame in the wrong format (see the paragraph ``data type''), so we want to convert them into a new format. For this purpose we can use, again, the function \textbf{mutate}, along with other functions such \emph{as.integer}, \emph{as.numeric}, \emph{as.character}, \emph{as.factors}, or \emph{as.logical}, \emph{as.Date}, or \emph{as.POSIXct()} based on the desired data format (it is possible and advisable to upload the data by paying attention to the type of data. If you upload the data in the right format, you can skip this step).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{device =} \KeywordTok{as.character}\NormalTok{(device)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   created_at          screen_name device retweet_count
##   <dttm>              <chr>       <chr>          <int>
## 1 2021-03-24 08:53:52 DoYourThin~ Twitt~             0
## 2 2021-03-24 08:53:25 DoYourThin~ Twitt~             0
## 3 2021-03-24 08:53:52 AlexS1595   Twitt~             3
## 4 2021-03-24 08:53:51 Makesworth~ Twitt~             0
## 5 2021-03-24 08:53:39 Makesworth~ Twitt~             4
## 6 2021-03-24 08:53:51 GGrahambute Twitt~            96
## # ... with 1 more variable: log_retweet_count <dbl>
\end{verbatim}

\hypertarget{summarize-and-group_by}{%
\subsection{Summarize and group\_by}\label{summarize-and-group_by}}

To aggregate data and calculate synthetic values (for instance, the average number of tweets \emph{by day}), it can be used the function \emph{group\_by} (to aggregate data, for instance by day), and \emph{summarize}, to calculate the summary values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets_summary <-}\StringTok{ }\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(screen_name) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{average_retweets =} \KeywordTok{mean}\NormalTok{(retweet_count))}

\KeywordTok{head}\NormalTok{(tweets_summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   screen_name     average_retweets
##   <chr>                      <dbl>
## 1 2EXvoZ6nublpw1F              164
## 2 AdilHaiderMD                  80
## 3 AlexS1595                      3
## 4 Andecave                      20
## 5 anshunandanpra4                2
## 6 ApKido                       150
\end{verbatim}

It is also possible to create more than one summary variables at once.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets_summary <-}\StringTok{ }\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(screen_name) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{average_retweets =} \KeywordTok{mean}\NormalTok{(retweet_count),}
            \DataTypeTok{average_log_retweets =} \KeywordTok{mean}\NormalTok{(log_retweet_count))}

\KeywordTok{head}\NormalTok{(tweets_summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   screen_name     average_retweets average_log_retweets
##   <chr>                      <dbl>                <dbl>
## 1 2EXvoZ6nublpw1F              164                 5.11
## 2 AdilHaiderMD                  80                 4.39
## 3 AlexS1595                      3                 1.39
## 4 Andecave                      20                 3.04
## 5 anshunandanpra4                2                 1.10
## 6 ApKido                       150                 5.02
\end{verbatim}

\hypertarget{count-occurrences}{%
\subsubsection{Count occurrences}\label{count-occurrences}}

A useful operation to perform when summarizing data, is to count the occurrences of a certain variable. For instance, to count the number of tweets sent by each user, it can be used the function \textbf{n()} inside the \emph{summarize} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets_summary <-}\StringTok{ }\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(screen_name) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{average_retweets =} \KeywordTok{mean}\NormalTok{(retweet_count),}
            \DataTypeTok{average_log_retweets =} \KeywordTok{mean}\NormalTok{(log_retweet_count),}
            \DataTypeTok{number_of_tweets =} \KeywordTok{n}\NormalTok{())}

\KeywordTok{head}\NormalTok{(tweets_summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   screen_name    average_retweets average_log_retwee~ number_of_tweets
##   <chr>                     <dbl>               <dbl>            <int>
## 1 2EXvoZ6nublpw~              164                5.11                1
## 2 AdilHaiderMD                 80                4.39                1
## 3 AlexS1595                     3                1.39                1
## 4 Andecave                     20                3.04                1
## 5 anshunandanpr~                2                1.10                1
## 6 ApKido                      150                5.02                1
\end{verbatim}

\hypertarget{arrange}{%
\subsection{Arrange}\label{arrange}}

To explore a data set it can be useful to sort the data (e.g.: from the lowest to the highest value of a variable). With tidyverse, we can order a data.frame by using the function \emph{arrange}.

To sort the data from the highest to the lowest value (descending order) the \emph{minus} sign (or the ``desc'' function) has to be added.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets_summary  }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{number_of_tweets) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   screen_name    average_retweets average_log_retwee~ number_of_tweets
##   <chr>                     <dbl>               <dbl>            <int>
## 1 iprdhzb                   0.667               0.462                3
## 2 benphillips76             3.5                 1.45                 2
## 3 DoYourThingUK             0                   0                    2
## 4 MakesworthAcc             2                   0.805                2
## 5 viralvideovlo~            3.5                 1.45                 2
## 6 2EXvoZ6nublpw~          164                   5.11                 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets_summary  }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(average_retweets)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   screen_name    average_retweets average_log_retwee~ number_of_tweets
##   <chr>                     <dbl>               <dbl>            <int>
## 1 Oliver_Miguel1             1988                7.60                1
## 2 Lil_3arbiii                1627                7.40                1
## 3 Kittyhawk681               1285                7.16                1
## 4 JulesFox12                 1091                7.00                1
## 5 rosaesaa26                  983                6.89                1
## 6 lewisabzueta                822                6.71                1
\end{verbatim}

Without the minus sign (or the ``desc'' command), data are sorted from the lowest to the highest value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets_summary  }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(number_of_tweets) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   screen_name    average_retweets average_log_retwee~ number_of_tweets
##   <chr>                     <dbl>               <dbl>            <int>
## 1 2EXvoZ6nublpw~              164                5.11                1
## 2 AdilHaiderMD                 80                4.39                1
## 3 AlexS1595                     3                1.39                1
## 4 Andecave                     20                3.04                1
## 5 anshunandanpr~                2                1.10                1
## 6 ApKido                      150                5.02                1
\end{verbatim}

\hypertarget{filter}{%
\subsection{Filter}\label{filter}}

The function \emph{filter} keeps only the cases (the ``rows'') we want to focus on. The arguments of this function are the conditions that have to be fulfilled to filter the data: a) the name of the column that we want to filter, b) the values to be kept.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(retweet_count }\OperatorTok{>=}\StringTok{ }\DecValTok{500}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{retweet_count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 5
##    created_at          screen_name device retweet_count
##    <dttm>              <chr>       <chr>          <int>
##  1 2021-03-24 08:53:31 Oliver_Mig~ Twitt~          1988
##  2 2021-03-24 08:53:48 Lil_3arbiii Twitt~          1627
##  3 2021-03-24 08:53:48 Kittyhawk6~ Twitt~          1285
##  4 2021-03-24 08:53:37 JulesFox12  Twitt~          1091
##  5 2021-03-24 08:53:42 rosaesaa26  Twitt~           983
##  6 2021-03-24 08:53:42 lewisabzue~ Twitt~           822
##  7 2021-03-24 08:53:42 jonvthvn08  Twitt~           768
##  8 2021-03-24 08:53:34 florent616~ Twitt~           768
##  9 2021-03-24 08:53:37 Ritu899039~ Twitt~           709
## 10 2021-03-24 08:53:33 Hurica3     Twitt~           575
## # ... with 1 more variable: log_retweet_count <dbl>
\end{verbatim}

In the examples below, notice the use of a double equal sign \emph{==}, and also of the quotation marks to indicate the modalities of a categorical variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(retweet_count }\OperatorTok{==}\StringTok{ }\DecValTok{1988}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 5
##   created_at          screen_name device retweet_count
##   <dttm>              <chr>       <chr>          <int>
## 1 2021-03-24 08:53:31 Oliver_Mig~ Twitt~          1988
## # ... with 1 more variable: log_retweet_count <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(device }\OperatorTok{==}\StringTok{ "Twitter for Android"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 33 x 5
##    created_at          screen_name device retweet_count
##    <dttm>              <chr>       <chr>          <int>
##  1 2021-03-24 08:53:49 marcin_luk~ Twitt~             1
##  2 2021-03-24 08:53:49 LebodyRanya Twitt~             0
##  3 2021-03-24 08:53:47 anshunanda~ Twitt~             2
##  4 2021-03-24 08:53:44 insoumise0~ Twitt~             5
##  5 2021-03-24 08:53:43 Metamorfop~ Twitt~             0
##  6 2021-03-24 08:53:43 keepsmilin~ Twitt~           164
##  7 2021-03-24 08:53:43 lovebresil~ Twitt~            81
##  8 2021-03-24 08:53:42 LightHeali~ Twitt~             1
##  9 2021-03-24 08:53:42 lewisabzue~ Twitt~           822
## 10 2021-03-24 08:53:42 rosaesaa26  Twitt~           983
## # ... with 23 more rows, and 1 more variable: log_retweet_count <dbl>
\end{verbatim}

It is also possible to use several conditions at the same time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(device }\OperatorTok{==}\StringTok{ "Twitter for Android"}\NormalTok{,}
\NormalTok{         retweet_count }\OperatorTok{>}\StringTok{ }\DecValTok{200}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{retweet_count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 5
##   created_at          screen_name device retweet_count
##   <dttm>              <chr>       <chr>          <int>
## 1 2021-03-24 08:53:31 Oliver_Mig~ Twitt~          1988
## 2 2021-03-24 08:53:37 JulesFox12  Twitt~          1091
## 3 2021-03-24 08:53:42 rosaesaa26  Twitt~           983
## 4 2021-03-24 08:53:42 lewisabzue~ Twitt~           822
## 5 2021-03-24 08:53:34 florent616~ Twitt~           768
## 6 2021-03-24 08:53:37 Ritu899039~ Twitt~           709
## 7 2021-03-24 08:53:27 aspeaker66  Twitt~           331
## 8 2021-03-24 08:53:42 JamesAn262~ Twitt~           201
## # ... with 1 more variable: log_retweet_count <dbl>
\end{verbatim}

\hypertarget{select}{%
\subsection{Select}\label{select}}

\textbf{Select} is used to keep just some of the columns of the original data.frame. For instance, we can apply the function in order to keep just the column ``device'' and ``retweet\_count''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(device, retweet_count) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   device             retweet_count
##   <chr>                      <int>
## 1 Twitter for iPhone             0
## 2 Twitter for iPhone             0
## 3 Twitter for iPhone             3
## 4 Twitter Web App                0
## 5 Twitter Web App                4
## 6 Twitter for iPad              96
\end{verbatim}

\hypertarget{exercise}{%
\subsection{Exercise}\label{exercise}}

Here are some exercises to consolidate the fundamental R skills learned during these first lessons:

\begin{itemize}
\item
  Download the csv file \emph{tweets\_vienna\_small.csv} in \href{https://drive.google.com/file/d/1RKPFOLsIAPLuACcrcB0iiq289CvDDgiT/view?usp=sharing}{this folder} and put it into the project folder ``data''
\item
  Upload the data set in R, setting the appropriate formats for the variables
\item
  Create a new script named ``data-manipulation'' with your surname and name as follows: \emph{``YOUR SURNAME-YOUR NAME-data-manipulation''}, and save it
\item
  In the script, write the code to perform the following operations:

  \begin{itemize}
  \item
    load the library ``Tidyverse''
  \item
    show the first rows of the data frame by using the function \emph{head}
  \item
    create a new data frame ``tweets\_vienna\_small\_updated'' by updating the dataframe ``tweets\_vienna\_small'' by using the function \emph{mutate} to create a new column ``log\_friends\_count'' whose values are the log of the values in the column ``friends\_count'' (you don't need to add 1 to the values in the column ``friends\_count'')
  \item
    save the updated dataframe ``tweets\_vienna\_small\_updated'', by using the following code to save a csv file (please change \emph{YOUR SURNAME-YOUR NAME} with your actual surname and name): \emph{write.csv(tweets\_vienna\_small\_updated, file = ``./data/YOUR SURNAME-YOUR NAME-tweets\_vienna\_small\_updated.csv'', row.names=F)} (we add row.names=F to avoid saving the number that indexes each row)
  \item
    create a new data frame named ``summary\_tweets\_vienna\_small'' aggregating the data by ``screen\_name'' (using the function \emph{group\_by}) and then summarizing the data (by using the function \emph{summarize}) as follows:

    \begin{itemize}
    \item
      in a column named ``average\_favorite\_count'', calculate the average of ``favorite\_count'' by ``screen\_name'' (that is, by user)
    \item
      in a column named ``average\_retweet\_count'', calculate the average of ``retweet\_count'' (by user, obviously, since the data are already aggregated by user' name)
    \item
      in a column named ``number\_of\_tweets'', calculate the number of tweets published by each users (by using the function \emph{n()})
    \item
      save the ``summary\_tweets\_vienna\_small'' in the data folder of the project, in csv format, and with the name \emph{``YOUR SURNAME-YOUR NAME-summary\_tweets\_vienna\_small.csv''} (remember to specify \emph{row.names=F} and to change \emph{YOUR SURNAME-YOUR NAME} with your actual surname and name)
    \item
      create a new data frame object called ``summary\_tweets\_vienna\_small\_filtered'', where you will save the data.frame summary\_tweets\_vienna\_small, after having filtered the rows with average\_retweet\_count higher than 10 (by using the function \emph{filter}), and after having selected the column ``screen\_name'' and ``average\_retweet\_count'' (so, you should end up with a data frame with just two columns, ``screen\_name'' and ``average\_retweet\_count'', and the rows with ``average\_retweet\_count'' higher than 10)
    \item
      save the data frame ``summary\_tweets\_vienna\_small\_filtered'' with the name \emph{``YOUR SURNAME-YOUR NAME-summary\_tweets\_vienna\_small\_filtered.csv''} in the folder data (remember to specify \emph{row.names=F} and to change \emph{YOUR SURNAME-YOUR NAME} with your actual surname and name).
    \end{itemize}
  \end{itemize}
\end{itemize}

Save the script ``YOUR SURNAME-YOUR NAME-data-manipulation'' with all the code you have used to perform these analysis. Write a comment in the script (using the hash mark \emph{\#}) if you are not able to do something.

Upload the script ``YOUR SURNAME-YOUR NAME-data-manipulation.r'' and the files ``YOUR SURNAME-YOUR NAME-tweets\_vienna\_small\_updated.csv'', ``YOUR SURNAME-YOUR NAME-summary\_tweets\_vienna\_small.csv'', and the file ``YOUR SURNAME-YOUR NAME-summary\_tweets\_vienna\_small\_filtered.csv'' on Moodle, in the folder ``HomeWork-1''. \emph{The deadline is Sunday 11 April}.

\hypertarget{basic-concepts}{%
\section{Basic Concepts}\label{basic-concepts}}

\hypertarget{time-series}{%
\subsection{Time Series}\label{time-series}}

A \textbf{time series} is a serially sequenced set of values representing a variable value at different points in time (VanLear, \href{https://us.sagepub.com/en-us/nam/the-sage-encyclopedia-of-communication-research-methods/book244974}{``Time Series Analysis''}). It consists in measures collected through time, at \textbf{regular time intervals}, about an unit of observation, resulting in a set of ordered values. This \emph{regularity} is the \textbf{frequency} of time series (which can be, for instance, hourly, weekly, monthly, quarterly, yearly etc.).

Time series data are different from \emph{cross-sectional data}, which are set of data observed on a sample of units taken at a \emph{given point in time}, or where the \emph{time dimension is not relevant} and can be ignored. Cross-sectional data are a \textbf{snapshot} of a population of interest at one particular point in time, while time series show the \textbf{dynamical} evolution of a variable over time. \emph{Panel data} combine cross-sectional and time series data by observing the same units over time.

\textbf{Time} is a fundamental variable in time series. It is often not relevant in other types of statistical analyses. Also from a \textbf{sociological perspective} (and psychological as well), we can see that \textbf{past events influence future} behaviors. Oftentimes, we can make reasonable \textbf{prediction} about future social behaviors just by observing past behaviors. Actually, social reproduction of behaviors over time and predictability of future social behaviors based on past experience and shared knowledge are essential to social order, and thus, a fundamental dimension of human society.

From a \textbf{statistical perspective}, the impact of time resulting from \textbf{repeated measurements} over time on a single subject or unit, introduce a \textbf{dependency among data points} which \emph{prevents the use of some of the most common statistical techniques}. In cross-sectional data, observations are assumed to be independent: values observed on one unit has no influence on values observed on other units. Time series observations have a different nature: a time series is not a collection of independent observations, or observations taken on independent units, but a collection of successive observations on the same unit. Observations are not taken across units at the same time (or without regards to time), but across time on the same unit.

When dealing with time series data, time is an important factor to be taken into account. It introduces a new dimension to the data. For instance, we can calculate how a variable increases or decreases over time, if it peaks at a given moment in time, or at regular intervals. We consider not just if, and how much, a variable is correlated with another variable, but if there is a correlation over time among them, if the peaks in one variable precedes the peaks in the other one, or how much time it requires for a variable to have an impact on another one, and how much this impact changes over time.

Importantly, when dealing with time series data, we have to to acknowledge that sampling adjacent points in time introduces a \textbf{correlation in the data}. This \textbf{serial dependency} creates \emph{correlated errors} which violates the assumptions of many traditional statistical analyses and can \emph{bias} the estimation of error for confidence intervals or significance tests. This characteristic of time series data, in general, \emph{precludes the use of common statistical approaches} such as linear regression and correlation analysis, \emph{which assume the observations to be independent}.

The application of ``standard'' statistical techniques to time series data might lead to foolish, and totally unreliable results. For instance, the statisticians \href{https://en.wikipedia.org/wiki/Udny_Yule}{George Udny Yule} wrote:

\begin{quote}
It is fairly familiar knowledge that we sometimes obtain between quantities varying with the time (time-variables) quite high correlations to which we cannot attach any physical significance whatever, although under the ordinary test the correlation would be held to be certainly ``significant.'' (\ldots) the occurrence of such ``nonsense-correlations'' makes one mistrust the serious arguments that are sometimes put forward on the basis of correlations between time-series. {[}\ldots{]} When the successive x's and y's in a sample no longer form a random series, but a series in which successive terms are closely related to one another, the usual conceptions (of correlation, ed.) to which we are accustomed fail totally and entirely to apply (Yule, G.U. (1926). \href{https://www.jstor.org/stable/pdf/2341482.pdf}{Why do we sometimes get nonsense-correlations between Time-Series? A study in sampling and the nature of time-series}. \emph{Journal of the royal statistical society}, 89(1), 1-63.)
\end{quote}

A funny website reporting spurious time series correlation is \href{https://www.tylervigen.com/spurious-correlations}{tylervigen.com}.

\includegraphics[width=28.89in]{images/chart}
\includegraphics[width=28.89in]{images/chart-2}

Despite it can be funny to see these improbable correlations, we have to keep in mind that adopting the right approach to analyze data is a serious issue when doing research. In a \href{https://gking.harvard.edu/files/gking/files/epr.pdf}{paper on the American Journal of Political Science} we can read, for instance:

\begin{quote}
The results of the analysis below strongly suggest that the way event counts have been analyzed in hundreds of important political science studies have produced statistically and substantively unreliable results. Misspecification, inefficiency, bias, inconsistency, insufficiency, and other problems result from the unknowing application of two common methods that are without theoretical justification or empirical utility in this type of data.
\end{quote}

Due to the peculiarity of time series data, \textbf{time series analysis} has been developed as a specific statistical methodology appropriate for the analysis of \textbf{time-dependent data}. Time series analysis aims at providing an understanding of the \emph{underlying processes and patterns of change over time} of a unit of observation and the relations between variables observed over time, handling the time structure of the data in a proper way.

\hypertarget{time-series-analysis}{%
\subsection{Time Series Analysis}\label{time-series-analysis}}

Time series analysis is an approach employed in many disciplines. Almost every field of study has data characterized by a time development, and every phenomenon with a temporal dimension can be conceived as a time series, and can be analyzed through time series analysis methods. Time series analysis are an important part of data analysis in disciplines such as economics, to analyze, for instance, \emph{inflation trends}, marketing to analyze the \emph{number of clients} of a store or number of accesses to an e-commerce website, in demography to study the \emph{growth of national population} overtime or \emph{trends in population ageing}, in engineering to analyze \emph{radio frequencies}, in neurology to analyze \emph{brain waves} detected through electroencephalograms. Political science can be interested in studying patterns in \emph{alternation of political parties in government}, and digital communication can be interested in using time series analysis to study series of tweets using an hashtag, the \emph{news media coverage} on a certain topic, or the trends in \emph{users searches on search engines}, such as those provided by Google Trends.

About the use of time series analysis in communication science, it can be observed that:

\begin{quote}
``Many of the major theories and models in our field contain time as a central player: the two-step flow, cultivation, spiral-of-silence, agenda-setting, framing, and communication mediation models, to name a few (Nabi \& Oliver, 2009). Each articulates a set of processes that play out in time: Messages work their way through media systems and networks, citizens perceive the world around them and decide to communicate, or not, and they make choices about participation, presumably as a product of a process that includes communication exposure. Indeed, the words that animate our field---effect, flow, influence, dynamic, cycle---reveal our understanding of communication as a process, and processes have temporal dimensions (Box-Steffensmeier, Freeman, Hitt, \& Pevehouse, 2014). The perspective of time series analysis can help expand our notions of time's role in these dynamics. We see several ways in which we can become more attentive to time in our field''. \href{https://ijoc.org/index.php/ijoc/article/view/10635}{Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, A., \& Yang, J. (2019). The Temporal Turn in Communication Research: Time Series Analyses Using Computational Approaches. International Journal of Communication (19328036), 13}.
\end{quote}

\begin{quote}
``One of the most common applications of time series analyses in mass communication is in agenda-setting research. The approach is to correlate the national news coverage on a topic over time with public opinion or public policy on that topic, often to estimate lagged effects or the decay of effects over time. Likewise, both trends and cycles of television programming, viewing, and advertising, have been explored through time series analyses. In the interpersonal literature, the most popular and one of the most important applications of time series analysis has been the investigation of mutual adaptation in the form of patterns of reciprocity or compensation between conversational partners over the course of an interaction.'' (C. Arthur VanLear, ``Time Series Analysis'', in Allen, M. (Ed.). (2017). \href{https://us.sagepub.com/en-us/nam/the-sage-encyclopedia-of-communication-research-methods/book244974}{The SAGE encyclopedia of communication research methods}. Sage Publications).
\end{quote}

In general, we can distinguish at least the following objectives of a time series analysis study:

\begin{itemize}
\tightlist
\item
  \textbf{DESCRIPTION}: Description of a process characterized by an intrinsic temporal dimension. Simple examples of related questions are: is there an upward trend? Is there a peak at a certain point in time? Is there a regular pattern recurring every year, in a particular moment in time? Descriptive questions like these can be answered via descriptive time series analysis.
\item
  \textbf{EVALUATION}: Evaluation of the impact of a certain event, occurring in a particular point in time, on a process. For instance: did a change in social media moderation policy, such as those that led to ban accounts linked to conspiracy theories, impacted on the quantity of fake news shared online by users? Specific time series techniques can be used to perform this kind of analysis.
\item
  \textbf{EXPLANATION}: Explanation of a phenomenon characterized by a time series structure on the basis of related variables. For instance: does the quantity of news shared on Facebook help explaining the polarization of the debate online? Does the volume of news media articles on a topic help explaining the growth of the debate online on the same topic? Inferential statistical techniques, such as regression models developed for time series, are used to answer questions like these.
\item
  \textbf{FORECASTING}: Prediction of the future values of a process. For instance: can we expect that news media coverage on a certain topic keep growing in the near future? This is the subject of time series forecasting.
\end{itemize}

We can also distinguish between \textbf{univariate} and \textbf{multivariate} time series analysis. Time series analysis can be used to explain the temporal dependencies \textbf{within} and \textbf{between} processes. By temporal dependency within a social process, we mean that the current value of a variable is, in part, a function of previous values of that same variable. To analyze univariate structure of time series, univariate techniques are used. Temporal dependency between social processes, conversely, indicates that the current value of a variable is in part a function of the previous values of other variables. Multivariate time series analysis are used to explain the relations between time series.

\hypertarget{stochastic-and-deterministic-processes}{%
\subsection{Stochastic and Deterministic Processes}\label{stochastic-and-deterministic-processes}}

A general distinction can be made between time series, based on their deterministic or non-deterministic nature.

A \textbf{deterministic time series} is one which can be explicitly expressed by an analytic expression. It has no random or probabilistic parts. It is always possible to exactly predict its future behavior, and state how it behaved in the past. Deterministic processes are pretty rare when dealing with individual and social behaviors! Predicting future behaviors of a crowd, of a person, of a social group, can be reasonably possible, sometimes, based on past behaviors and other contextual information, since human behavior is partly \emph{influenced} by the past. However, it is not totally \emph{determined} by the past. There is always a certain degree of uncertainty in the prediction; human behaviors are, generally speaking, not fully predictable.

Social and individual behaviors, therefore, are non-deterministic. A \textbf{non-deterministic time series} cannot be fully described by an analytic expression. It has some \textbf{random}, or probabilistic component, that prevents its behavior from being explicitly described. It could be possible to say, in probabilistic terms, what its future behavior might be. However, there is always a \emph{residual}, unpredictable, component. A time series may be considered non-deterministic also because all the information necessary to describe it explicitly is not available, although it might be in principle, or because the nature of the generating process, or part of it, is inherently random. We can say that the time series analyzed in social science have always, at least, a stochastic component that makes them not totally deterministic.

Since non-deterministic time series have a random component, they follow probabilistic rather than deterministic laws. Random data are not defined by explicit mathematical relations, but rather in statistical terms, that is, by probability distributions and parameters such as mean and variance. Non-deterministic time series can be analyzed by assuming that they are manifestations of probabilistic or \emph{stochastic processes}.

\hypertarget{time-series-objects}{%
\section{Time Series Objects}\label{time-series-objects}}

\hypertarget{time-series-objects-1}{%
\subsection{Time Series Objects}\label{time-series-objects-1}}

Every \emph{object} we manipulate in R is characterized by a specific structure. Objects' structures vary depending on the type of object: a \emph{list}, a \emph{matrix}, or a \emph{data.frame}, are different objects with different structures. Every structure has its own manipulation methods. For instance, it can be accessed and analyzed by using different functions and strings of code.

In R there are many different types of object. To get an overview you can refer to this handbook, to the \href{https://cran.r-project.org/doc/manuals/r-release/R-intro.html}{R manual}, or the chapter 5 of \href{https://rc2e.com/datastructures}{this free online book}. In this course we are going to learn more about the data structures we have to deal with when conducting time series analysis in R, that is, the structure of time series objects and data sets.

Time series data sets in R can be represented by different objects. Specific \emph{libraries} (coherent collections of functions) can give different structures to time series data sets.

There are many R libraries for handling and working with time series objects. Some of them are more general and other are useful to perform very specific analysis. \href{https://cran.r-project.org/web/views/TimeSeries.html}{On this page} you can find a comprehensive list of the R libraries for time series analysis.

For now, we just need to know that different libraries can create time series objects with different structures which can be manipulated through different functions. This means that \textbf{not all the objects can be analyzed with all the functions}, as well that there is \textbf{not always compatibility} between R libraries.
Many functions have been developed with reference to specific libraries and objects, or require a particular object structure. As a consequence, creating a time series object with a certain structure or a certain library can imply that we can use some functions and perform only a certain type of analysis. In other words, specific type of objects could introduce specific \textbf{constrains to data analysis} (and visualization), so it could be wise to plan in advance the necessary analyses, so as to select the necessary libraries and data structure.

We now introduce three types of objects that are commonly used to store and analyze time series data:

\begin{itemize}
\tightlist
\item
  The \textbf{data.frame} (in base R)
\item
  The \textbf{ts} object (in base R)
\item
  The \textbf{xts} object (created through the library \emph{xts}).
\end{itemize}

We analyze the structures of these objects and their strengths and limitations. In the next chapter, we'll also learn the methods available to visualize them.

\hypertarget{time-series-as-data-frames}{%
\subsubsection{Time Series as Data Frames}\label{time-series-as-data-frames}}

Data frames (\emph{data.frame}) are the most common data set structure in R. A data.frame is simply a table \emph{cases by variables} (each row is a case and each column is a variable).

To see an example of data.frame containing time series data, we can upload a data set containing the number of news articles mentioning the keyword ``elections'' published by USA news media. I retrieved this data set from \href{https://mediacloud.org}{MediaCloud}, a free and open source platform for studying media ecosystem that tracks millions of stories published online. You can download the data set \href{https://drive.google.com/file/d/1vzpdPFb_ihBlqqbHxJS7mEvzQ3tNhmEm/view?usp=sharing}{at this link}.

We can upload the \emph{.csv} file by using the function \textbf{read.csv}. The main argument of the read.csv function is the path of the file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections_news <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/elections-stories-over-time-20210111144254.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By using the function \textbf{class} we can see that this is a data.frame.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(elections_news)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

We can check the first few rows of the data.frame by using the function \textbf{head}, which shows the first few rows of the data set, so as to get an idea of the structure of this simple data.frame.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(elections_news)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         date count total_count      ratio
## 1 2015-01-01   373       25611 0.01456405
## 2 2015-01-02   387       31932 0.01211950
## 3 2015-01-03   289       24646 0.01172604
## 4 2015-01-04   322       25513 0.01262102
## 5 2015-01-05   567       39982 0.01418138
## 6 2015-01-06   626       42366 0.01477600
\end{verbatim}

This data.frame contains time series data: the first column contains dates, and the other columns contain the values of the observations. We can also see that the data.frame seems to contain daily data, where each row corresponds to a specific day. The data frame also includes, in the column ``count'', the number of news articles mentioning the keyword ``elections'', in the column ``total\_count'', the total number of news articles on \emph{all the topics}, and in the column ``ratio'' the proportion of news articles mentioning the keyword (count/total\_count).

The function \emph{head} (and \emph{tail}) can be impractical with data.frame including a lot of columns, so it could be better to use the function \textbf{str} to check the structure of the data.frame.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(elections_news)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    2192 obs. of  4 variables:
##  $ date       : Factor w/ 2192 levels "2015-01-01","2015-01-02",..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ count      : int  373 387 289 322 567 626 507 521 531 346 ...
##  $ total_count: int  25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ...
##  $ ratio      : num  0.0146 0.0121 0.0117 0.0126 0.0142 ...
\end{verbatim}

As you can see in the output of the function \emph{str}, the format of the column date is \emph{Factor}. The format is, in this case, automatically attributed by R, but (as we have already said) it can be specified before importing the data.

\emph{Factor} is an appropriate format for categorical variables, but R includes a specific format for dates and times. In this case we have just a date, so we can convert it to a variable of type \emph{date}. We can change the format of the variable by using the function \textbf{as.Date}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections_news}\OperatorTok{$}\NormalTok{date <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(elections_news}\OperatorTok{$}\NormalTok{date)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(elections_news)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    2192 obs. of  4 variables:
##  $ date       : Date, format: "2015-01-01" ...
##  $ count      : int  373 387 289 322 567 626 507 521 531 346 ...
##  $ total_count: int  25611 31932 24646 25513 39982 42366 45163 44928 44041 29093 ...
##  $ ratio      : num  0.0146 0.0121 0.0117 0.0126 0.0142 ...
\end{verbatim}

We can also perform the same operation with tidyverse, by using the function \textbf{mutate}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{elections_news <-}\StringTok{ }\NormalTok{elections_news }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{date =} \KeywordTok{as.Date}\NormalTok{(date))}
\end{Highlighting}
\end{Shaded}

A data.frame is the common format for data sets, including time series data sets. We can do many things with data stored in this format, such as creating plots and performing various types of analysis. However, to handle time series in R there are more specific data formats.

\hypertarget{time-series-as-ts-objects}{%
\subsubsection{Time Series as TS objects}\label{time-series-as-ts-objects}}

The basic object created to handle time series in R is the object of class \emph{ts}. The name stands for ``Time Series''.

An example of ts object is already present in R under the name of ``AirPassengers'', a time series data set in ts format. We can load this data set with the function \textbf{data}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(AirPassengers)}
\end{Highlighting}
\end{Shaded}

By applying the function \emph{class} we can see that this is an object of class ts.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(AirPassengers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "ts"
\end{verbatim}

AirPassengers is a small data set so we can print all the data set to see its structure, which is an example of the standard structure of a ts object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949 112 118 132 129 121 135 148 148 136 119 104 118
## 1950 115 126 141 135 125 149 170 170 158 133 114 140
## 1951 145 150 178 163 172 178 199 199 184 162 146 166
## 1952 171 180 193 181 183 218 230 242 209 191 172 194
## 1953 196 196 236 235 229 243 264 272 237 211 180 201
## 1954 204 188 235 227 234 264 302 293 259 229 203 229
## 1955 242 233 267 269 270 315 364 347 312 274 237 278
## 1956 284 277 317 313 318 374 413 405 355 306 271 306
## 1957 315 301 356 348 355 422 465 467 404 347 305 336
## 1958 340 318 362 348 363 435 491 505 404 359 310 337
## 1959 360 342 406 396 420 472 548 559 463 407 362 405
## 1960 417 391 419 461 472 535 622 606 508 461 390 432
\end{verbatim}

By calling the \emph{str} function we get synthetic information on the object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(AirPassengers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...
\end{verbatim}

The AirPassengers data set is a univariate time series representing monthly totals of international airline passengers from 1949 to 1960. As every time series, it has a \textbf{start date} and an \textbf{end date}. It also has a \textbf{frequency}, which is the frequency at which the observations were taken.

All this characteristics differentiate a ts object from a data.frame. The structure of a data.frame lacks the start and the end date, and the frequency value.

The functions \textbf{start}, \textbf{end}, and \textbf{frequency}, can be applied to a \emph{ts} object to check their values. We started by saying that some functions work with some objects but not with other types of objects. This is an example. These functions, indeed, work with \emph{ts} objects just because they are part of their structures, and are arguments usually specified when this kind of object is created. They do not work if applied to a data.frame object, since a data.frame structure does not include the start and end date, nor the frequency of observations. It can be seen that the ts structure is much more specific for time series data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{start}\NormalTok{(AirPassengers)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1949
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{end}\NormalTok{(AirPassengers)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1960
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{frequency}\NormalTok{(AirPassengers)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12
\end{verbatim}

Importantly, the frequency of a time series is assumed to be \textbf{regular} over time. This applies to time series in general, and not just to ts objects. In this case, the time series starts on January 1949 and ends on December 1969, and has \emph{monthly} frequency. Monthly frequency is indicated in \emph{ts} as ``12'', meaning 12 months. Indeed, the reference unit of a ts object is a year. So, quarterly data, for instance, have frequency equal to 4.

To create a ts object is necessary to follow specific steps and use specific functions. To exemplify the process of creation of a ts object we take the example of the data contained in the AirPassengers data set, and store them in a data.frame (you don't need to learn how to do that, just copy and paste the code). A data.frame is the data set format you will probably start with, so it can be useful to see how to create a ts object starting from a data.frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{date <-}\StringTok{ }\KeywordTok{seq.Date}\NormalTok{(}\DataTypeTok{from =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"1949-01-01"}\NormalTok{), }
                 \DataTypeTok{to =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"1960-12-01"}\NormalTok{), }\DataTypeTok{by=}\StringTok{"month"}\NormalTok{)}
\NormalTok{passengers <-}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(AirPassengers)}

\NormalTok{data_frame_format <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"Date"}\NormalTok{ =}\StringTok{ }\NormalTok{date, }
                                \StringTok{"Passengers"}\NormalTok{ =}\StringTok{ }\NormalTok{passengers)}
\end{Highlighting}
\end{Shaded}

To create a ``ts'' time series object starting from a data.frame, we need:

\begin{itemize}
\tightlist
\item
  To specify which column contains the observations. In this case, the column name is ``Passengers''.
\item
  We then need to specify the start and end date, which in this case are in the format year/month, but can be just years in case of yearly data. The ts format for the start/end date is the following: \emph{c=(YEAR, MONTH)}. The \emph{c} represents the \emph{concatenate} function, and it concatenates the year and the month in a single vector.
\item
  Finally, we indicate the frequency of the time series observations. The frequency is specified based on the time period of a year, so in this case we have a frequency equal to 12, because we have monthly observation, meaning that we have 12 observation per year.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ts_format <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data_frame_format}\OperatorTok{$}\NormalTok{Passengers, }
                \DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{1949}\NormalTok{, }\DecValTok{01}\NormalTok{), }
                \DataTypeTok{end=}\KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{, }\DecValTok{12}\NormalTok{), }
                \DataTypeTok{frequency =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ts_format}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949 112 118 132 129 121 135 148 148 136 119 104 118
## 1950 115 126 141 135 125 149 170 170 158 133 114 140
## 1951 145 150 178 163 172 178 199 199 184 162 146 166
## 1952 171 180 193 181 183 218 230 242 209 191 172 194
## 1953 196 196 236 235 229 243 264 272 237 211 180 201
## 1954 204 188 235 227 234 264 302 293 259 229 203 229
## 1955 242 233 267 269 270 315 364 347 312 274 237 278
## 1956 284 277 317 313 318 374 413 405 355 306 271 306
## 1957 315 301 356 348 355 422 465 467 404 347 305 336
## 1958 340 318 362 348 363 435 491 505 404 359 310 337
## 1959 360 342 406 396 420 472 548 559 463 407 362 405
## 1960 417 391 419 461 472 535 622 606 508 461 390 432
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(ts_format)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "ts"
\end{verbatim}

It's important to notice that yearly, quarterly, and monthly data work fine with the \emph{ts} structure, but \emph{more fine grained data create complications and are not totally suitable for a ts structure}.

This is due to the fact that time series objects require the frequency of observations to be regular and in \emph{ts} the observations have to be regular with reference to a year. Unfortunately, a time series that spans over many years cannot be composed of a constant number of days, since the number of days will be sometimes 365 and other time 366, in case of leap years. This is a limitation of \emph{ts} objects. However, when dealing with monthly data or data with frequency lower than one month (such as quarterly data), \emph{ts} works great.

\hypertarget{time-series-as-xtszoo-objects}{%
\subsubsection{Time Series as XTS/ZOO objects}\label{time-series-as-xtszoo-objects}}

Time series can be stored in object of class xts/zoo. This class of objects is created with the library \emph{xts}, which is related and an extension of the package \emph{zoo} (another package to deal with time series data). As other libraries, it requires to be installed and loaded.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("xts")}
\KeywordTok{library}\NormalTok{(xts)}
\end{Highlighting}
\end{Shaded}

The \emph{xts} object is more flexible that the \emph{ts} one.

We can create an \emph{xts} time series by starting from the data.frame we have just created. Similarly to what required by \emph{ts}, we need to specify:

\begin{itemize}
\tightlist
\item
  the column of the data.frame (or the vector) containing the data;
\item
  the column of the data.frame (or the vector) containing the dates/times (which has to be in a date/time format);
\item
  the frequency of observations.
\end{itemize}

We can use the data.frame already created with the AirPassengers data to create a new \emph{xts} object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xts_format <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ data_frame_format}\OperatorTok{$}\NormalTok{Passengers, }
                  \DataTypeTok{order.by =}\NormalTok{ data_frame_format}\OperatorTok{$}\NormalTok{Date, }
                  \DataTypeTok{frequency =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(xts_format)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "xts" "zoo"
\end{verbatim}

Also the structure of this object, like the \emph{ts} one, includes the range of dates of the time series, with it starting and ending date.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(xts_format)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## An 'xts' object on 1949-01-01/1960-12-01 containing:
##   Data: num [1:144, 1] 112 118 132 129 121 135 148 148 136 119 ...
##   Indexed by objects of class: [Date] TZ: UTC
##   xts Attributes:  
##  NULL
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(xts_format)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            [,1]
## 1949-01-01  112
## 1949-02-01  118
## 1949-03-01  132
## 1949-04-01  129
## 1949-05-01  121
## 1949-06-01  135
\end{verbatim}

\hypertarget{exercise-1}{%
\subsection{Exercise}\label{exercise-1}}

\begin{itemize}
\tightlist
\item
  Download the data set \href{https://drive.google.com/file/d/1RKPFOLsIAPLuACcrcB0iiq289CvDDgiT/view?usp=sharing}{Austrian\_Local\_Media}, a data set including metrics from Facebook pages of Austrian Local Media (monthly observations from January 2015 to December 2020 on quantity of posts and interactions)
\item
  Set the ``beginning\_of\_interval'' to the appropriate ``Date'' format
\item
  Create a ts object, called ``ts\_object\_1'', using as data values the post\_count values
\item
  Create a xts object, called ``xts\_object\_1'', using as data values the post\_count values
\item
  Use the Tidyverse function ``mutate'' to create a new column with the number of interaction by post (total\_interactions/post\_count) and call it ``interaction\_ratio''
\item
  Create a ts object, called ``ts\_object\_2'', using as data values the interaction\_ratio values
\item
  Create a xts object, called ``xts\_object\_2'', using as data values the interaction\_ratio values
\item
  Try to plot these objects, by using the command \emph{plot.ts(ts\_object\_1)}, \emph{plot.ts(ts\_object\_2)}, \emph{plot.xts(xts\_object\_1)}, \emph{plot.xts(xts\_object\_2)}
\end{itemize}

\hypertarget{plot-time-series}{%
\section{Plot Time Series}\label{plot-time-series}}

\hypertarget{plot-time-series-objects}{%
\subsection{Plot Time Series Objects}\label{plot-time-series-objects}}

In this lecture we are going to learn how to plot time series data.

We will take into account three main functions: \textbf{ggplot} from the tidyverse library, \textbf{plot.ts} from base R, and \textbf{plot.xts} from the xts library. Ggplot is probably the most versatile function from the perspective of the graphical results that can be obtained, but also the more complex, while for ordinary visualization, plot.ts is probably the easiest tool.

Plotting time series is an important part of the analysis because it permits to visualize and \textbf{explore the data}, both from a \textbf{univariate} perspective (focusing on the characteristics of a single time series) and from a \textbf{multivariate} perspective (focusing on the characteristics of many time series, and on the relations between them). To visualize and explore the relations between time series, we'll learn to plot a single time series as well as many different time series at once.

\hypertarget{plot.ts}{%
\subsection{plot.ts}\label{plot.ts}}

You can visualize a time series by using the function plot.ts() applied to a time series data in a ts format.

An example of ts time series is provided by the AirPassengers dataset, already included in R (you can load the data by running \emph{data(``AirPassengers'')}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"AirPassengers"}\NormalTok{)}
\KeywordTok{plot.ts}\NormalTok{(AirPassengers)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-81-1.pdf}

You can add many details to your plot, such as a title, a label for the y axis and for the x axis, change the colors of the plot (use \emph{colors()} to see the list of standard colors in R), and the size of the line.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.ts}\NormalTok{(AirPassengers, }
     \DataTypeTok{main =} \StringTok{"PASSENGERS"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"1949-1960 (monthly data)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"violetred3"}\NormalTok{, }
     \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-82-1.pdf}

You can use plot.ts to plot two (or more) time series together, a useful operation to take a look at their relations. The different time series must have the same structure (the same starting date, the same ending date, and the same frequency), and should be stored in the same ts object.

To merge in one ``ts'' object two or more time series already in the ts format, you can employ the function \textbf{ts.union()}. You can plot both the time series in the same plot, or create two different plots, by using the option \textbf{``plot.type''} and specifying \textbf{single} or \textbf{multiple}.

With \textbf{lwd} you control the line width (or the line size):

\begin{itemize}
\tightlist
\item
  Line types can either be specified as an integer (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) or as one of the character strings ``blank'', ``solid'', ``dashed'', ``dotted'', ``dotdash'', ``longdash'', or ``twodash'', where ``blank'' uses `invisible lines' (i.e., does not draw them).
\end{itemize}

With \textbf{lty} you control the line type:

\begin{itemize}
\tightlist
\item
  The line width, a positive number, defaulting to 1. The interpretation is device-specific, and some devices do not implement line widths less than one.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a "toy" time series with the same lenght of the AirPassenger one}
\NormalTok{AirPassengers_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{AirPassengers }\OperatorTok{+}\StringTok{ }\DecValTok{100}
\NormalTok{AirPassengers_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\NormalTok{AirPassengers }\OperatorTok{+}\StringTok{ }\DecValTok{300}

\NormalTok{AirPassengers_multi <-}\StringTok{ }\KeywordTok{ts.union}\NormalTok{(AirPassengers, AirPassengers_}\DecValTok{2}\NormalTok{, AirPassengers_}\DecValTok{3}\NormalTok{)}

\KeywordTok{plot.ts}\NormalTok{(AirPassengers_multi, }
        \DataTypeTok{main =} \StringTok{"Three time series"}\NormalTok{,}
        \DataTypeTok{xlab =} \StringTok{"TIME"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"VALUES"}\NormalTok{,}
        \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{), }
        \DataTypeTok{lwd=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{lty=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{),}
        \DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-83-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.ts}\NormalTok{(AirPassengers_multi, }
        \DataTypeTok{main =} \StringTok{"Three time series"}\NormalTok{,}
        \DataTypeTok{xlab =} \StringTok{"TIME"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"VALUES"}\NormalTok{,}
        \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }
        \DataTypeTok{lwd=}\DecValTok{4}\NormalTok{,}
        \DataTypeTok{plot.type =} \StringTok{"multiple"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-84-1.pdf}

With the parameter \textbf{nc} you can control the number of columns used to display the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.ts}\NormalTok{(AirPassengers_multi, }
        \DataTypeTok{main =} \StringTok{"Three time series"}\NormalTok{,}
        \DataTypeTok{xlab =} \StringTok{"TIME"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"VALUES"}\NormalTok{,}
        \DataTypeTok{col =} \StringTok{"orange"}\NormalTok{,}
        \DataTypeTok{lwd=}\DecValTok{4}\NormalTok{,}
        \DataTypeTok{plot.type =} \StringTok{"multiple"}\NormalTok{,}
        \DataTypeTok{nc=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-85-1.pdf}

To explore long time series it can be useful to focus on a limited time window. To do that, you can subset the data by using the function \textbf{window}. It is a function which extracts the subset of data observed between the specified start and end time. You can use the function \emph{window} in the plot.ts function (alternatively, you can create a new object by applying the function \emph{window} first, and then plot the new object).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.ts}\NormalTok{(}\KeywordTok{window}\NormalTok{(AirPassengers, }\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{1950}\NormalTok{, }\DecValTok{01}\NormalTok{), }\DataTypeTok{end=}\KeywordTok{c}\NormalTok{(}\DecValTok{1954}\NormalTok{, }\DecValTok{12}\NormalTok{)), }
     \DataTypeTok{main =} \StringTok{"PASSENGERS (1950-1955)"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"1950-1955 (monthly data)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"violetred3"}\NormalTok{, }
     \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-86-1.pdf}

In the \emph{window} function, you can also specify a \textbf{frequency}, and the series is then re-sampled at the new frequency. For instance, by re-sampling at quarterly frequency, the function keeps the observations made on January, April, July, and October, and by re-sampling at a six-month frequency, it keeps the observations made on January and July.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.ts}\NormalTok{(}\KeywordTok{window}\NormalTok{(AirPassengers, }\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{1950}\NormalTok{, }\DecValTok{01}\NormalTok{), }\DataTypeTok{end=}\KeywordTok{c}\NormalTok{(}\DecValTok{1954}\NormalTok{,}\DecValTok{12}\NormalTok{), }\DataTypeTok{frequency =} \DecValTok{4}\NormalTok{), }
        \DataTypeTok{main =} \StringTok{"PASSENGERS (1950-1955) - QUARTERLY DATA"}\NormalTok{,}
        \DataTypeTok{xlab =} \StringTok{"1950-1955 (Quarterly data)"}\NormalTok{,}
        \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{,}
        \DataTypeTok{col =} \StringTok{"violetred3"}\NormalTok{, }
        \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-87-1.pdf}

You can use the \emph{window} function also with more than one time series.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.ts}\NormalTok{(}\KeywordTok{window}\NormalTok{(AirPassengers_multi, }\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{1950}\NormalTok{, }\DecValTok{01}\NormalTok{), }\DataTypeTok{end=}\KeywordTok{c}\NormalTok{(}\DecValTok{1954}\NormalTok{,}\DecValTok{12}\NormalTok{), }\DataTypeTok{frequency =} \DecValTok{4}\NormalTok{), }
        \DataTypeTok{main =} \StringTok{"PASSENGERS (1950-1955) - QUARTERLY DATA"}\NormalTok{,}
        \DataTypeTok{xlab =} \StringTok{"1950-1955 (Quarterly data)"}\NormalTok{,}
        \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{,}
        \DataTypeTok{col =} \StringTok{"violetred3"}\NormalTok{, }
        \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-88-1.pdf}

To learn something more about the graphical options of plot.ts, you can open and read the help page by using \textbf{?plot.ts}. The question mark followed by the name of a function opens the help page of that function.

\hypertarget{plot.xts}{%
\subsection{plot.xts}\label{plot.xts}}

To plot a \emph{xts} object we can similarly use the \textbf{plot.xts} function.

You can create a xts object with the \emph{xts} function (see the previous chapter), but if you already have a \emph{ts} object, you can also convert it to a \emph{xts} object by using the function \textbf{as.xts}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers_xts <-}\StringTok{ }\KeywordTok{as.xts}\NormalTok{(AirPassengers)}

\KeywordTok{plot.xts}\NormalTok{(AirPassengers_xts,}
         \DataTypeTok{main =} \StringTok{"PASSENGERS"}\NormalTok{,}
         \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{, }
         \DataTypeTok{col =} \StringTok{"steelblue2"}\NormalTok{, }
         \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-89-1.pdf}

By using \textbf{multi.panel=TRUE}, or \textbf{multi.panel=FALSE} you can plot all the time series in the same panel or using different panels.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers_multi_xts <-}\StringTok{ }\KeywordTok{as.xts}\NormalTok{(AirPassengers_multi)}

\KeywordTok{plot.xts}\NormalTok{(AirPassengers_multi_xts,}
         \DataTypeTok{main =} \StringTok{"PASSENGERS"}\NormalTok{,}
         \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{, }
         \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{1}\NormalTok{,}
         \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"black"}\NormalTok{),}
         \DataTypeTok{multi.panel =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-90-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.xts}\NormalTok{(AirPassengers_multi_xts,}
         \DataTypeTok{main =} \StringTok{"PASSENGERS"}\NormalTok{,}
         \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{, }
         \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{1}\NormalTok{,}
         \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"black"}\NormalTok{),}
         \DataTypeTok{multi.panel =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-91-1.pdf}

To \textbf{subset} the data, in order to visualize and focus on just one part of the series, instead of the function \emph{window}, you have to write the dates into \emph{squared brackets} as in the examples below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.xts}\NormalTok{(AirPassengers_multi_xts[}\StringTok{"1950-01/1954-12"}\NormalTok{], }
         \DataTypeTok{main =} \StringTok{"PASSENGERS"}\NormalTok{,}
         \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{, }
         \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{1}\NormalTok{,}
         \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"black"}\NormalTok{),}
         \DataTypeTok{multi.panel =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-92-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.xts}\NormalTok{(AirPassengers_xts[}\StringTok{"1950-01/1956-06"}\NormalTok{], }
         \DataTypeTok{main =} \StringTok{"PASSENGERS"}\NormalTok{,}
         \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{, }
         \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{1}\NormalTok{,}
         \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"black"}\NormalTok{),}
         \DataTypeTok{multi.panel =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-93-1.pdf}

You can also change the frequency of the observations by using specific functions in the \emph{xts} library.

By using the function \textbf{periodicity} you can find the frequency of the time series.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{periodicity}\NormalTok{(AirPassengers_xts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Monthly periodicity from Jan 1949 to Dec 1960
\end{verbatim}

With the function \textbf{to.period} you can re-sample the data to ``seconds'', ``minutes'', ``hours'', ``days'', ``weeks'', ``months'', ``quarters'', and ``years''. You can only re-sample the data from a higher to a lower frequency, but not from a lower to a higher one. For instance, if you have monthly data, you can aggregate the data in quarterly or yearly data, but you cannot create a weekly or hourly time series.

The result of the \emph{to.period} function will contain the open (first) and close (last) value for the given period, as well as the maximum and minimum over the new period, reflected in the new high and low, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{to.period}\NormalTok{(AirPassengers_xts, }\DataTypeTok{period=}\StringTok{"years"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          AirPassengers_xts.Open AirPassengers_xts.High
## Dec 1949                    112                    148
## Dec 1950                    115                    170
## Dec 1951                    145                    199
## Dec 1952                    171                    242
## Dec 1953                    196                    272
## Dec 1954                    204                    302
## Dec 1955                    242                    364
## Dec 1956                    284                    413
## Dec 1957                    315                    467
## Dec 1958                    340                    505
## Dec 1959                    360                    559
## Dec 1960                    417                    622
##          AirPassengers_xts.Low AirPassengers_xts.Close
## Dec 1949                   104                     118
## Dec 1950                   114                     140
## Dec 1951                   145                     166
## Dec 1952                   171                     194
## Dec 1953                   180                     201
## Dec 1954                   188                     229
## Dec 1955                   233                     278
## Dec 1956                   271                     306
## Dec 1957                   301                     336
## Dec 1958                   310                     337
## Dec 1959                   342                     405
## Dec 1960                   390                     432
\end{verbatim}

You can plot all the values, or select a value by using the \emph{square brackets} with a comma, followed by the number of the column you want to plot (see the table above, with 4 columns). This notation is a way to access the columns (and the rows) of a data.frame or matrix. You write the name of the data.frame or the matrix, and the squared brackets indicate the index of the rows, in the first position before the comma, and the index of the columns, in the second position, after the comma. So, for instance, to access the value in the second column and the second row of the data.set ``data'', you can write data{[}2,2{]}, and to access the values in the third column and first row, you can write data{[}1,3{]}. If you leave a blank space in the column or row space, you get all the values in that column or rows. Therefore, by writing data{[},2{]}, or data{[},3{]}, you get all the values in the column 2 and 3, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.xts}\NormalTok{(}\KeywordTok{to.period}\NormalTok{(AirPassengers_xts, }\DataTypeTok{period=}\StringTok{"years"}\NormalTok{)[,}\DecValTok{2}\NormalTok{],}
         \DataTypeTok{main =} \StringTok{"PASSENGERS"}\NormalTok{,}
         \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{, }
         \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{1}\NormalTok{,}
         \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"black"}\NormalTok{),}
         \DataTypeTok{multi.panel =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-96-1.pdf}

You can also \textbf{re-sample and calculate the average} (or another statistics) for the new period. For instance, in the example we re-sample the data by year and calculate the average. It is also possible to calculate other statistics such as, for instance, the median, just by writing \emph{``median''} instead of ``mean''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index_years <-}\StringTok{ }\KeywordTok{endpoints}\NormalTok{(AirPassengers_xts, }\DataTypeTok{on =} \StringTok{"year"}\NormalTok{)}
\NormalTok{AirPassengers_xts_year_avg <-}\StringTok{ }\KeywordTok{period.apply}\NormalTok{(AirPassengers_xts, }\DataTypeTok{INDEX=}\NormalTok{index_years, }\DataTypeTok{FUN=}\NormalTok{mean)}

\KeywordTok{plot.xts}\NormalTok{(AirPassengers_xts_year_avg,}
         \DataTypeTok{main =} \StringTok{"PASSENGERS (Year Average)"}\NormalTok{,}
         \DataTypeTok{ylab =} \StringTok{"Passengers (1000's)"}\NormalTok{, }
         \DataTypeTok{lwd=}\DecValTok{5}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{1}\NormalTok{,}
         \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
         \DataTypeTok{multi.panel =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-97-1.pdf}
You can find additional details on \emph{xts} and \emph{plot.xts} by reading the help functions. \href{https://s3.amazonaws.com/assets.datacamp.com/blog_assets/xts_Cheat_Sheet_R.pdf}{At this link} you can find a synthetic presentation of the functions of the xts library.

\hypertarget{ggplot}{%
\subsection{ggplot}\label{ggplot}}

Ggplot2 is the tidyverse library for data visualization. We can use it to create time series plots and many other types of plot.

We upload a data set, first, and we set the appropriate time format for the date.

We place our data set (``elections\_news'') inside the ggplot function. Notice that the ggplot syntax is similar to the tidyverse one, but uses the plus sign instead of the pipe one (\%\textgreater\%).

To create a line plot with ggplot it is necessary to use the \textbf{geom\_line} function. This function requires two parameters: the data for the x-axis and the data for the y-axis. These parameters have to be written inside the \textbf{aes} function. You can also specify the colors and the size of the line. By using additional function, after the plus sign, you can also set the labels for the x- and y-axes, and the title, the subtitle, and the caption of the plot. You can also change the overall aspect of the plot by using one of the \textbf{themes} included in the library.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(elections_news) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ ratio), }\DataTypeTok{color =} \StringTok{"snow4"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"News Articles"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Date"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Time Series of News Articles on Elections"}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"Data from MediaCloud"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Data Analysis II"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-100-1.pdf}

You can also plot more than one series. For instance, you can create two plots, and then use the function \textbf{grid.arrange}, from the library \textbf{gridExtra} to combine the plots together.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("gridExtra")}
\KeywordTok{library}\NormalTok{(gridExtra)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'gridExtra'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     combine
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\NormalTok{elections_news }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ ratio), }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"News Articles (ratio)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Date"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"MediaCloud Data on Elections (Daily)"}\NormalTok{) }

\NormalTok{p2 <-}\StringTok{ }\NormalTok{elections_news }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ count), }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{size=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"News Articles (count)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Date"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"MediaCloud Data on Elections (Daily)"}\NormalTok{) }

\KeywordTok{grid.arrange}\NormalTok{(p1,p2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-101-1.pdf}

You can also plot two or more than two series in the same plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections_news <-}\StringTok{ }\NormalTok{elections_news }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{time_series_data_2 =}\NormalTok{ count}\OperatorTok{*}\DecValTok{2}\NormalTok{,}
         \DataTypeTok{time_series_data_3 =}\NormalTok{ count}\OperatorTok{*}\DecValTok{4}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(elections_news) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ count), }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ time_series_data_}\DecValTok{2}\NormalTok{), }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ time_series_data_}\DecValTok{3}\NormalTok{), }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{size=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Date"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"MediaCloud Data on Elections (Daily)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-102-1.pdf}

To focus on a shorter time window, we can use the \textbf{dplyr} function \textbf{filter}. Besides filtering the data, we add a function ``scale\_x\_datetime'', which control the labels on the x-axis, specifying we want to use monthly labels.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections_news }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{>=}\StringTok{ "2016-01-01"} \OperatorTok{&}\StringTok{ }\NormalTok{date }\OperatorTok{<}\StringTok{ "2017-01-01"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ count), }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_date}\NormalTok{(}\DataTypeTok{breaks=}\StringTok{"month"}\NormalTok{, }\DataTypeTok{date_labels =}\StringTok{"%Y-%m"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{45}\NormalTok{, }\DataTypeTok{hjust=}\DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"News Articles (Ratio)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Day"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"MediaCloud Data on Elections (Monthly) - 2016"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-103-1.pdf}

You can also use ggplot to annotate the date. To create annotations in ggplot you can use the function ``annotate'', to label the data point, and ``geom\_segment'', to trace lines for connecting data points to labels.

You can learn more about annotation in ggplot here: \url{https://ggplot2-book.org/annotations.html}
And about lines here: \url{https://ggplot2.tidyverse.org/reference/geom_segment.html}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(elections_news) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ count), }\DataTypeTok{col =} \StringTok{"grey50"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.25}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylim}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15000}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\CommentTok{# 1 EVENT}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"label"}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2018-11-01"}\NormalTok{), }\DataTypeTok{y =} \DecValTok{14500}\NormalTok{, }
           \DataTypeTok{label =} \StringTok{"Midterm Elections}\CharTok{\textbackslash{}n}\StringTok{November 2018"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"orange"}\NormalTok{, }\DataTypeTok{fontface=}\StringTok{"bold"}\NormalTok{, }\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{# add a line. You can also use an arrow by adding in geom_segment: }
\StringTok{  }\CommentTok{# arrow = line(length = unit(0.2, "cm"), ends = "last") }
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2018-11-01"}\NormalTok{), }\DataTypeTok{xend =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2018-11-01"}\NormalTok{), }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{yend =} \DecValTok{14500}\NormalTok{), }
               \DataTypeTok{color =} \StringTok{"orange"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{# 2 EVENT}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"label"}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2019-05-01"}\NormalTok{), }\DataTypeTok{y =} \DecValTok{12000}\NormalTok{, }
           \DataTypeTok{label =} \StringTok{"Pennsylvania Elections}\CharTok{\textbackslash{}n}\StringTok{May 2019"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"orange"}\NormalTok{, }\DataTypeTok{fontface=}\StringTok{"bold"}\NormalTok{, }\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2019-05-01"}\NormalTok{), }\DataTypeTok{xend =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2019-05-01"}\NormalTok{), }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{yend =} \DecValTok{12000}\NormalTok{), }
               \DataTypeTok{color =} \StringTok{"orange"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{# 3 EVENT}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"label"}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2016-11-01"}\NormalTok{), }\DataTypeTok{y =} \DecValTok{12000}\NormalTok{, }
           \DataTypeTok{label =} \StringTok{"Presidential Elections}\CharTok{\textbackslash{}n}\StringTok{November 2016"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"orange"}\NormalTok{, }\DataTypeTok{fontface=}\StringTok{"bold"}\NormalTok{, }\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2016-11-01"}\NormalTok{), }\DataTypeTok{xend =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2016-11-01"}\NormalTok{), }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{yend =} \DecValTok{12000}\NormalTok{), }
               \DataTypeTok{color =} \StringTok{"orange"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{# 4 EVENT}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"label"}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2020-11-01"}\NormalTok{), }\DataTypeTok{y =} \DecValTok{12000}\NormalTok{, }
           \DataTypeTok{label =} \StringTok{"Presidential}\CharTok{\textbackslash{}n}\StringTok{Elections}\CharTok{\textbackslash{}n}\StringTok{November}\CharTok{\textbackslash{}n}\StringTok{2020"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"orange"}\NormalTok{, }\DataTypeTok{fontface=}\StringTok{"bold"}\NormalTok{, }\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2020-11-01"}\NormalTok{), }\DataTypeTok{xend =} \KeywordTok{as.Date}\NormalTok{(}\StringTok{"2020-11-01"}\NormalTok{), }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{yend =} \DecValTok{12000}\NormalTok{), }
               \DataTypeTok{color =} \StringTok{"orange"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"News Articles"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Date"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"MediaCloud Data on Elections"}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"Peaks annotated with relevant political events"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Advanced Data Analysis}
\StringTok{                  University of Vienna"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{),}
        \DataTypeTok{plot.subtitle =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
        \DataTypeTok{plot.caption =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"italic"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_gray}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-104-1.pdf}

\hypertarget{structural-decomposition}{%
\section{Structural Decomposition}\label{structural-decomposition}}

\hypertarget{components-of-a-time-series}{%
\subsection{Components of a time series}\label{components-of-a-time-series}}

A time series can be considered composed of 4 main parts: \textbf{trend}, \textbf{cycle}, \textbf{seasonality}, and the \textbf{irregular} or remainder/residual part.

\includegraphics[width=32.25in]{images/Structure}

\hypertarget{trend-and-cycle}{%
\subsubsection{Trend and Cycle}\label{trend-and-cycle}}

The \textbf{Trend} component is the longest-term behavior of a time series. The simplest model for a trend is a linear increase or decrease, but the trend has not to be linear. In the AirPassengers time series there is a clear upward, linear trend.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(xts)}
\KeywordTok{data}\NormalTok{(}\StringTok{"AirPassengers"}\NormalTok{)}
\NormalTok{AirPassengers_xts <-}\StringTok{ }\KeywordTok{as.xts}\NormalTok{(AirPassengers)}
\KeywordTok{plot.xts}\NormalTok{(AirPassengers_xts)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-106-1.pdf}

\hypertarget{stochastic-and-deterministic-trend}{%
\subsubsection{Stochastic and Deterministic Trend}\label{stochastic-and-deterministic-trend}}

There is a distinction between \textbf{deterministic} and \textbf{stochastic} trends.

A \textbf{deterministic trend} is a \emph{fixed function of time}. If a series has a deterministic trend, the increase (or decrease) in the value of the series is a function of time. For instance, it may appear to grow or decline steadily over time. A deterministic trend can be linear, as well as non linear. Deterministic trends have plausible explanations (for example, a deterministic increasing trend in the data may be related to an increasing population). A series with deterministic trend is also called \emph{trend stationary}.

A \textbf{stochastic trend} wanders up and down or shows change of direction at unpredictable times. Time series with a stochastic trend are also said to be \emph{difference stationary}. An example of stochastic trend is provided by the so-called \emph{random walk} process.

\textbf{Random Walk} is a particular time series process in which the current values are combinations of the previous ones (\(x_t = x_{t-1} + w_t\), where \(x_{t-1}\) is the value immediately before \(x\), and \(w_t\) is a random component). The resulting time series is characterized by a discernible pattern over time which is not exactly predictable (\emph{stochastic trend}). Starting from the same initial point, the same process can generate different time series.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{111}\NormalTok{)}
\NormalTok{Random_Walk <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)))}
\KeywordTok{plot.ts}\NormalTok{(Random_Walk, }\DataTypeTok{ylab =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{italic}\NormalTok{(x)[}\KeywordTok{italic}\NormalTok{(t)]), }\DataTypeTok{main =} \StringTok{"Random Walk"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-107-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{555}\NormalTok{)}
\NormalTok{Random_Walk <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)))}
\KeywordTok{plot.ts}\NormalTok{(Random_Walk, }\DataTypeTok{ylab =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{italic}\NormalTok{(x)[}\KeywordTok{italic}\NormalTok{(t)]), }\DataTypeTok{main =} \StringTok{"Random Walk"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-107-2.pdf}

A paper on \href{https://www.tandfonline.com/doi/full/10.1080/00036846.2020.1789061}{The effectiveness of social distancing in containing Covid-19} shows an example of stochastic trend and complex, deterministic nonlinear trends represented by polynomials.

\begin{figure}
\includegraphics[width=16.67in]{images/covid-trends} \caption{Figure 5 shows the actual number of Covid-19 cases recorded in the UK up to 17 June 2020. The stochastic trend estimated earlier is superimposed on the actual observations and so are two deterministic nonlinear trends represented by polynomials of degrees 5 and 6. We can see that the stochastic trend captures the slow growth at the beginning of the sample period whereas the two deterministic trends do not. The stochastic trend is better also at capturing the sharp increase represented by observation number 72.. (original caption)}\label{fig:unnamed-chunk-108}
\end{figure}

The trend component of the series is often considered along with the \textbf{cyclic} one (\emph{trend-cycle}). The \textbf{cyclical} component is represented by fluctuations (rises and falls) not occurring at a fixed frequency. The cycle component is therefore different from the seasonal variation (see below) in that it does not follow a fixed calendar frequency.

\hypertarget{seasonality}{%
\subsubsection{Seasonality}\label{seasonality}}

The \textbf{Seasonal} component is a repeated pattern occurring at a fixed time period such as the time of the year or the day of the week (the frequency of seasonality, which is always a fixed and known frequency). There is a clear seasonal variation in the AirPassenger time series: bookings were highest during the summer months of June, July, and August and lowest during the autumn/winter months.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.xts}\NormalTok{(AirPassengers_xts[}\StringTok{"1954-01/1955-12"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-109-1.pdf}

It is possible to plot the distributions of data by months by using the function \emph{boxplot} and \emph{cycle}, to visualize the increasing number of passengers during the summer months. In this case, \emph{cycle} is used to refer to the positions of each observation in the (yearly, in this case) cycle of observations (every year is considered to be a cycle of 12 observations).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(AirPassengers }\OperatorTok{~}\StringTok{ }\KeywordTok{cycle}\NormalTok{(AirPassengers))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-110-1.pdf}

The library \emph{forecast}, an R package that provides methods and tools for displaying and analysing time series forecasts, includes a function to create a ``polar'' seasonal plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("forecast")}
\KeywordTok{library}\NormalTok{(forecast)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Registered S3 method overwritten by 'quantmod':
##   method            from
##   as.zoo.data.frame zoo
\end{verbatim}

\begin{verbatim}
## This is forecast 8.13 
##   Want to meet other forecasters? Join the International Institute of Forecasters:
##   http://forecasters.org/
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggseasonplot}\NormalTok{(AirPassengers, }\DataTypeTok{polar=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-111-1.pdf}

An example of weekly seasonality can be found in the COVID-19 statistics.

\begin{figure}
\includegraphics[width=18.46in]{images/covid-italy} \caption{Covid statistics (Google)}\label{fig:unnamed-chunk-112}
\end{figure}

Cyclic and seasonal variations can look similar. Both cyclic and seasonal variations have `peak-and-trough' patterns. The main difference is that in seasonal patterns the period between successive peaks (or troughs) is constant, while in cyclical patterns the distance between successive peaks is irregular.

\hypertarget{residuals}{%
\subsubsection{Residuals}\label{residuals}}

The \textbf{irregular} or remainder/residual component is the random-like part of the series.

In general, when we fit mathematical models to time series data, the \emph{residual} error series represents the discrepancies between the fitted values, calculated from the model, and the data. A good model encapsulates most of the deterministic features of the time series, and the residual error series should therefore appear to be a realization of independent random variables from some probability distribution.

The analysis of residuals is thus important to judge the fit of a model. In this case, its residual error series appears to be a realization of \emph{independent random variables}. Often the random variable is conceived as a Gaussian random variable. We'll return to these topics in the last part of the chapter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers_Random <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(AirPassengers, }\DataTypeTok{type=}\StringTok{"multiplicative"}\NormalTok{)}\OperatorTok{$}\NormalTok{random}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(AirPassengers_Random, }\DataTypeTok{xlab=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(AirPassengers_Random), }\DataTypeTok{main =} \StringTok{""}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-114-1.pdf}

\hypertarget{structural-decomposition-1}{%
\subsection{Structural decomposition}\label{structural-decomposition-1}}

Along with the analysis of the peaks (see previous chapter), analyzing a time series based on these structural parts can be an important exploratory step. It helps understanding the likely causes of the series features and formulate an appropriate time series model. For instance, in the case of the AirPassengers series, we could hypothesize that the \emph{increasing trend} is due to the rising prosperity in the aftermath of the Second World War, greater availability of aircraft, cheaper flights due to competition between airlines, and an increasing population. The \emph{seasonal} variation, instead, seems to coincide with vacation periods.

\textbf{Decomposition methods} try to identify and separate the above mentioned parts of a time series. Usually they consider together the trend and cycle (\emph{trend-cycle}) - the longer-term changes in the series - and the \emph{seasonal} factors - periodic fluctuations of constant length happening at a specific calendar frequency).

There are two main ways through which these elements can be combined together: in the \textbf{additive} and the \textbf{multiplicative} form:

\begin{itemize}
\tightlist
\item
  The \textbf{additive model} (\(x_{t} = m_{t} + s_{t} + z_{t}\), where \(x_{t}\) is the observed time series, \(m_{t}\) is the trend-cycle component, \(s_{t}\) is the seasonal component and \(z_{t}\) is the residual) is useful when the seasonal variation is relatively constant over time
\item
  The \textbf{multiplicative model} (\(x_{t} = m_{t} * s_{t} * z_{t}\)) is useful when the the seasonal effects tends to increase as the trend increases.
\end{itemize}

There are different methods to decompose a time series. Here we consider the function \textbf{decompose}. This function is defined as \emph{Classical Seasonal Decomposition by Moving Averages}. The function \emph{decompose} uses a \textbf{moving average (MA)} approach to filter the data. Moving average is a classical approach to extract the trend from a time series by averaging out the seasonal effects.

\hypertarget{moving-average}{%
\subsubsection{Moving Average}\label{moving-average}}

Moving average is a process that replaces each value \(x_{t}\) with an average of its current value \(x_{t}\) and its immediate neighbors in the past and future. For instance, it is possible to calculate a simple moving average by using the closest neighbors of a point, as follows: \(x_{t} = \frac{1}{3} (x_{t-1} + x_{t} + x_{t+1})\). This is called \emph{Centered Moving Average}.

The number of neighbors in the past and future is determined by the analyst and is also called \emph{width of the window}. The time window for the moving average is chosen by considering the frequency of the data and their seasonal effects. For instance, monthly data, which are supposed to show monthly seasonality (for instance, in the AirPassengers data there are more passengers during the summer months), can be averaged by using a period of 12 months (six months before and after each point. Since we have an even number of months, some other calculation are necessary. For instance, the moving average value for July, is calculated by averaging the average of January up to December, and the average of February up to January. R functions do this for you).

The centered moving average is an example of a \textbf{smoothing} procedure that is applied retrospectively to a time series with the objective of identifying an underlying signal or trend. Smoothing procedures usually use points before and after the time at which the smoothed estimate is to be calculated. A consequence is that the smoothed series will have \emph{some points missing at the beginning and the end} unless the smoothing algorithm is adapted for the end points. In the case of monthly data, for instance, the moving average filter determines the lost of the first and last six months of data.

Smoothing procedures like moving average, allows the main underlying trend to emerge by filtering out seasonality and noise, so they are used to get an idea of the long-term underlying process of a time series.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections_news <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/elections-stories-over-time-20210111144254.csv"}\NormalTok{, }
                           \DataTypeTok{col_types =} \KeywordTok{cols}\NormalTok{(}\DataTypeTok{date =} \KeywordTok{col_date}\NormalTok{(}\DataTypeTok{format =} \StringTok{"%Y-%m-%d"}\NormalTok{)))}

\NormalTok{en <-}\StringTok{ }\KeywordTok{as.xts}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ elections_news}\OperatorTok{$}\NormalTok{ratio, }\DataTypeTok{order.by =}\NormalTok{ elections_news}\OperatorTok{$}\NormalTok{date)}

\NormalTok{en2 <-}\StringTok{ }\KeywordTok{rollmean}\NormalTok{(en, }\DataTypeTok{k =} \DecValTok{2}\NormalTok{)}
\NormalTok{en4 <-}\StringTok{ }\KeywordTok{rollmean}\NormalTok{(en, }\DataTypeTok{k =} \DecValTok{4}\NormalTok{)}
\NormalTok{en8 <-}\StringTok{ }\KeywordTok{rollmean}\NormalTok{(en, }\DataTypeTok{k =} \DecValTok{8}\NormalTok{)}
\NormalTok{en16 <-}\StringTok{ }\KeywordTok{rollmean}\NormalTok{(en, }\DataTypeTok{k =} \DecValTok{16}\NormalTok{)}
\NormalTok{en32 <-}\StringTok{ }\KeywordTok{rollmean}\NormalTok{(en, }\DataTypeTok{k =} \DecValTok{32}\NormalTok{)}

\NormalTok{enALL <-}\StringTok{ }\KeywordTok{merge.xts}\NormalTok{(en, en2, en4, en8, en16, en32)}

\CommentTok{# notice the NA elements increasing as the width of the moving average increase}
\KeywordTok{head}\NormalTok{(enALL, }\DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    en        en2        en4        en8       en16
## 2015-01-01 0.01456405 0.01334178         NA         NA         NA
## 2015-01-02 0.01211950 0.01192277 0.01275765         NA         NA
## 2015-01-03 0.01172604 0.01217353 0.01266199         NA         NA
## 2015-01-04 0.01262102 0.01340120 0.01332611 0.01285129         NA
## 2015-01-05 0.01418138 0.01447869 0.01320110 0.01253790         NA
## 2015-01-06 0.01477600 0.01300100 0.01294493 0.01250958         NA
## 2015-01-07 0.01122600 0.01141117 0.01241382 0.01267144         NA
## 2015-01-08 0.01159633 0.01182664 0.01169304 0.01285992 0.01257820
## 2015-01-09 0.01205695 0.01197492 0.01214178 0.01262867 0.01239793
## 2015-01-10 0.01189290 0.01245691 0.01277492 0.01226887 0.01234396
##            en32
## 2015-01-01   NA
## 2015-01-02   NA
## 2015-01-03   NA
## 2015-01-04   NA
## 2015-01-05   NA
## 2015-01-06   NA
## 2015-01-07   NA
## 2015-01-08   NA
## 2015-01-09   NA
## 2015-01-10   NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.xts}\NormalTok{(enALL[}\StringTok{"2015-01-01/2016-01-01"}\NormalTok{], }\DataTypeTok{multi.panel =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-116-1.pdf}

\hypertarget{decompose}{%
\subsubsection{Decompose}\label{decompose}}

To apply the function \emph{decompose}, we need a \textbf{ts} object.

Considering the AirPassengers time series, since the seasonal effect tends to increase as the trend increases, we can use a multiplicative model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers_dec <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(AirPassengers, }\DataTypeTok{type=}\StringTok{"multiplicative"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(AirPassengers_dec)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-117-1.pdf}

As an example of \emph{additive model} we can use data from the ``Seatbels'' data set.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"Seatbelts"}\NormalTok{)}
\NormalTok{seatbelts <-}\StringTok{ }\NormalTok{Seatbelts[,}\DecValTok{5}\NormalTok{]}
\KeywordTok{plot.ts}\NormalTok{(seatbelts)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-118-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seatbelts_dec <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(seatbelts, }\DataTypeTok{type=}\StringTok{"additive"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(seatbelts_dec)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-119-1.pdf}

The \textbf{residual} part of the model should be (approximately) \textbf{random}, which indicates that the model explained (most of) the significant patterns in the data (the \emph{``signal''}), leaving out the \emph{``noise''}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot.ts}\NormalTok{(seatbelts_dec}\OperatorTok{$}\NormalTok{random, }\DataTypeTok{main=}\StringTok{"Residuals"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(seatbelts_dec}\OperatorTok{$}\NormalTok{random, }\DataTypeTok{breaks =} \DecValTok{25}\NormalTok{, }\DataTypeTok{freq =}\NormalTok{ F, }\DataTypeTok{main =} \StringTok{"Histogram"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(seatbelts_dec}\OperatorTok{$}\NormalTok{random, }\DataTypeTok{na.rm =}\NormalTok{ T), }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-120-1.pdf}

We can re-create the original time series starting from its elements (we don't actually need to do that, it is just for illustrative purposes).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(AirPassengers_dec}\OperatorTok{$}\NormalTok{trend }\OperatorTok{*}\StringTok{ }\NormalTok{AirPassengers_dec}\OperatorTok{$}\NormalTok{seasonal }\OperatorTok{*}\StringTok{ }\NormalTok{AirPassengers_dec}\OperatorTok{$}\NormalTok{random,  }
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{1950}\NormalTok{, }\DecValTok{1960}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{600}\NormalTok{), }\DataTypeTok{main =} \StringTok{"'Re-composed' series"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(AirPassengers, }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{1950}\NormalTok{, }\DecValTok{1960}\NormalTok{),}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{600}\NormalTok{),  }\DataTypeTok{main =} \StringTok{"Original series"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-121-1.pdf}

\hypertarget{compare-additive-and-multiplicative-models}{%
\subsubsection{Compare Additive and Multiplicative Models}\label{compare-additive-and-multiplicative-models}}

Sometimes it could be hard to choose between additive or multiplicative models. In general, when seasonality or the variation around the trend-cycle component change proportionally to the level of the series (the trend, or the average), the multiplicative model works better. However, it might be difficult to assess the variability of the series from a time series plot.

Exploratory methods, such as representing the variability in the data through \href{https://en.wikipedia.org/wiki/Box_plot}{box plots} can help. The following ``custom'' function (which I called \emph{ts.year.boxplot}) takes as argument a time series \emph{ts}, and shows the \href{https://en.wikipedia.org/wiki/Statistical_dispersion}{spread} of the data by year.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run the code to "create" the function }
\NormalTok{ts.year.boxplot <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(ts) \{}
  
\NormalTok{  ts }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{fortify}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{substring}\NormalTok{(x, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(x, }\StringTok{"%Y"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =} \KeywordTok{format}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\KeywordTok{cut}\NormalTok{(x, }\DataTypeTok{breaks =}\StringTok{"year"}\NormalTok{)), }\StringTok{"%Y"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_boxplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{year, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{group=}\NormalTok{year)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_linedraw}\NormalTok{()}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

A multiplicative time series like the \emph{AirPassengers} shows a clear increasing spread as the level of the series goes up:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply the function}
\KeywordTok{ts.year.boxplot}\NormalTok{(AirPassengers)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-123-1.pdf}

The box plots of an additive time series looks more regular. There is no evident systematic

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ts.year.boxplot}\NormalTok{(seatbelts)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-124-1.pdf}

Below you can find another function (I called it \emph{compare.decomposition.methods}) that compares the additive and multiplicative decomposition models applied to the same \emph{ts} series. It creates plots of residuals (the time series plot, histogram, acf and pacf plots) and (roughly) measures the total residuals and residuals' autocorrelation (lower values are better). It also create a plot showing the different fit of the adittive and multiplicative model to the data, and includes the boxplot introduced above. We can try looking at these plots and the total autocorrelation measure to get further hints into the most appropriate method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{compare.decomposition.methods <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(ts)\{}

\NormalTok{    boxplot_ts <-}\StringTok{ }\NormalTok{ts }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{fortify}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{substring}\NormalTok{(x, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.Date}\NormalTok{(x, }\StringTok{"%Y"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =} \KeywordTok{format}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\KeywordTok{cut}\NormalTok{(x, }\DataTypeTok{breaks =}\StringTok{"year"}\NormalTok{)), }\StringTok{"%Y"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_boxplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{year, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{group=}\NormalTok{year)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_linedraw}\NormalTok{()}
  
  \CommentTok{# decompose the series with both the methods}
\NormalTok{  xad <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(ts, }\DataTypeTok{type =} \StringTok{"additive"}\NormalTok{)}
\NormalTok{  ymu <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(ts, }\DataTypeTok{type =} \StringTok{"multiplicative"}\NormalTok{)}
  
  \CommentTok{# plots}
  \KeywordTok{print}\NormalTok{(boxplot_ts)}
  
  \KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
  \KeywordTok{plot}\NormalTok{(ts, }\DataTypeTok{main=}\StringTok{"ADDITIVE (BLUE) - MULTIPLICATIVE (RED)"}\NormalTok{)}
  \KeywordTok{lines}\NormalTok{(xad}\OperatorTok{$}\NormalTok{seasonal}\OperatorTok{+}\NormalTok{xad}\OperatorTok{$}\NormalTok{trend, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
  \KeywordTok{lines}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{seasonal}\OperatorTok{*}\NormalTok{ymu}\OperatorTok{$}\NormalTok{trend, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
  
  \KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
  
\NormalTok{  am <-}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(xad}\OperatorTok{$}\NormalTok{seasonal)}\OperatorTok{+}\KeywordTok{as.vector}\NormalTok{(xad}\OperatorTok{$}\NormalTok{trend)}
\NormalTok{  am <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(am, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\KeywordTok{start}\NormalTok{(ts)[}\DecValTok{1}\NormalTok{], }\KeywordTok{start}\NormalTok{(ts)[}\DecValTok{2}\NormalTok{]), }
         \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\KeywordTok{end}\NormalTok{(ts)[}\DecValTok{1}\NormalTok{], }\KeywordTok{end}\NormalTok{(ts)[}\DecValTok{2}\NormalTok{]),}
         \DataTypeTok{frequency =} \KeywordTok{frequency}\NormalTok{(ts))}
\NormalTok{  am <-}\StringTok{ }\KeywordTok{ts.union}\NormalTok{(am, ts)}
\NormalTok{  am <-}\StringTok{ }\NormalTok{am[}\KeywordTok{complete.cases}\NormalTok{(am),]}
  
\NormalTok{  mm <-}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{seasonal)}\OperatorTok{*}\KeywordTok{as.vector}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{trend)}
\NormalTok{  mm <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(mm, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\KeywordTok{start}\NormalTok{(ts)[}\DecValTok{1}\NormalTok{], }\KeywordTok{start}\NormalTok{(ts)[}\DecValTok{2}\NormalTok{]), }
         \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\KeywordTok{end}\NormalTok{(ts)[}\DecValTok{1}\NormalTok{], }\KeywordTok{end}\NormalTok{(ts)[}\DecValTok{2}\NormalTok{]),}
         \DataTypeTok{frequency =} \KeywordTok{frequency}\NormalTok{(ts))}
\NormalTok{  mm <-}\StringTok{ }\KeywordTok{ts.union}\NormalTok{(mm, ts)}
\NormalTok{  mm <-}\StringTok{ }\NormalTok{mm[}\KeywordTok{complete.cases}\NormalTok{(mm),]}
  
  \KeywordTok{plot}\NormalTok{(}\KeywordTok{as.vector}\NormalTok{(am[,}\DecValTok{1}\NormalTok{]), }\KeywordTok{as.vector}\NormalTok{(am[,}\DecValTok{2}\NormalTok{]), }\DataTypeTok{xlab=}\StringTok{"ADDITIVE"}\NormalTok{)}
  \KeywordTok{plot}\NormalTok{(}\KeywordTok{as.vector}\NormalTok{(mm[,}\DecValTok{1}\NormalTok{]), }\KeywordTok{as.vector}\NormalTok{(mm[,}\DecValTok{2}\NormalTok{]), }\DataTypeTok{xlab=}\StringTok{"MULTIPLICATIVE"}\NormalTok{)}

  \KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
  \KeywordTok{plot}\NormalTok{(xad}\OperatorTok{$}\NormalTok{random, }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"ADDITIVE"}\NormalTok{)}
  \KeywordTok{hist}\NormalTok{(xad}\OperatorTok{$}\NormalTok{random, }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
  \KeywordTok{acf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(xad}\OperatorTok{$}\NormalTok{random), }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
  \KeywordTok{pacf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(xad}\OperatorTok{$}\NormalTok{random), }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}

  \KeywordTok{plot}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{random, }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"MULTIPLICATIVE"}\NormalTok{)}
  \KeywordTok{hist}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{random, }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
  \KeywordTok{acf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{random), }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
  \KeywordTok{pacf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{random), }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}


  \CommentTok{# Sum of squares of residual auto-correlation (acf)}
  \KeywordTok{cat}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}
    \StringTok{"###################################}\CharTok{\textbackslash{}n}\StringTok{TOTAL AUTOCORRELATION (ABSOLUTE VALUES)}\CharTok{\textbackslash{}n}\StringTok{###################################"}\NormalTok{,}
            \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{ADITTIVE MODEL = "}\NormalTok{, }
    \KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\KeywordTok{acf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(xad}\OperatorTok{$}\NormalTok{random), }\DataTypeTok{plot =}\NormalTok{ F)}\OperatorTok{$}\NormalTok{acf)),}\DecValTok{2}\NormalTok{),}
    \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{MULTIPLICATIVE MODEL = "}\NormalTok{, }
    \KeywordTok{round}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{acf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{random), }\DataTypeTok{plot =}\NormalTok{ F)}\OperatorTok{$}\NormalTok{acf)),}\DecValTok{2}\NormalTok{),}
        \StringTok{"}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{###################################}\CharTok{\textbackslash{}n}\StringTok{SUM OF RESIDUALS (ABSOLUTE VALUES)}\CharTok{\textbackslash{}n}\StringTok{###################################"}\NormalTok{,}
    \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{ADITTIVE MODEL = "}\NormalTok{, }
    \KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\KeywordTok{scale}\NormalTok{(xad}\OperatorTok{$}\NormalTok{random)), }\DataTypeTok{na.rm=}\NormalTok{T),}\DecValTok{2}\NormalTok{),}
    \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{MULTIPLICATIVE = "}\NormalTok{, }
    \KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\KeywordTok{scale}\NormalTok{(ymu}\OperatorTok{$}\NormalTok{random)), }\DataTypeTok{na.rm=}\NormalTok{T),}\DecValTok{2}\NormalTok{)}
\NormalTok{    ))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In this case, the multiplicative model looks better.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{compare.decomposition.methods}\NormalTok{(AirPassengers)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-126-1.pdf} \includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-126-2.pdf} \includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-126-3.pdf} \includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-126-4.pdf}

\begin{verbatim}
## ###################################
## TOTAL AUTOCORRELATION (ABSOLUTE VALUES)
## ################################### 
## ADITTIVE MODEL =  8.19 
## MULTIPLICATIVE MODEL =  0.16 
## 
## ###################################
## SUM OF RESIDUALS (ABSOLUTE VALUES)
## ################################### 
## ADITTIVE MODEL =  98.35 
## MULTIPLICATIVE =  95.97
\end{verbatim}

\hypertarget{adjust-time-series}{%
\subsection{Adjust time series}\label{adjust-time-series}}

Sometimes the analyst is not interested in the trend or in the seasonal variation in the data, and might want to remove them, in order to let other underlying process to emerge more clearly. Other times, some components of time series can be misleading, leading to inflated or spurious correlations, and can be preferable to remove them before proceding with the analysis.

\hypertarget{seasonal-adjusted-data}{%
\subsubsection{Seasonal adjusted data}\label{seasonal-adjusted-data}}

It is common to find seasonally adjusted data, that is time series from which the seasonal component has been removed. This happen quite often in economics, for instance (but in other disciplines as well), where certain growing trends can be considered trivial, and explained based on solid theory. Other parts of the series are instead considered more important, and removing the seasonal component allow them to emerge more clearly, as summarized by \href{https://www.nber.org/system/files/chapters/c4321/c4321.pdf}{Granger, C. W. (1978), \emph{Seasonality: causation, interpretation, and implications}. In Seasonal analysis of economic time series}:

\begin{quote}
Presumably, the seasonal is treated in this fashion, because it is economically uninportant, being dull, superficially easily explained, and easy to forecast but, at the same time, being statistically important in that it is a major contributor to the total variance of many series. The presence of the seasonal could be said to obscure movements in other components of greater economic significance. (\ldots) It can be certainly be stated that, when considering the level of an economic variable, the low frequency components (the trend-cycle, ed.) are usually both statistically and economically important. (\ldots) Because of their dual importance, it is desirable to view this component as clearly as possible and, thus, the interference from the season should be removed. (\ldots) the preference for seasonally adjusted data is so that they can more clearly see the position of local trends or the place on the business cycle. It is certainly true that for any series containing a strong season, it is very difficult to observe these local trends without seasonal adjustment.
\end{quote}

Moreover, seasonality can lead to spurious correlations:

\begin{quote}
(\ldots) if the relationship between a pair of economic variables is to be analyzed, it is obviously possible to obtain a spurious relationship if the two series contain important seasonals. By adjusting series, one possible source of spurious relationship is removed.
\end{quote}

However, adjust for seasonality a series is application specific, and sometimes this part of the series can be of interest:

\begin{quote}
Firms having seasonal fluctuations in demand for their products, for example, may need to make decisions based largely on the seasonal component (\ldots) and a local government may try to partially control seasonal fluctuations in unemployment. (\ldots) Only by having both the the adjusted and the unadjusted data available can these potential users gain the maximum benefit from all of the effort that goes into collecting the information.
\end{quote}

There are many different methods to adjust data for seasonality. A simple approach is based on the results of the decomposition process, and consists in substracting (in the case of an addittive decomposition model) the seasonal component from the original series, or dividing the original series by the seasonal component (in the case of a multiplicative model).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"Seatbelts"}\NormalTok{)}
\NormalTok{seatbelts <-}\StringTok{ }\NormalTok{Seatbelts[,}\DecValTok{5}\NormalTok{]}
\NormalTok{seatbelts_dec <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(seatbelts, }\DataTypeTok{type=}\StringTok{"additive"}\NormalTok{)}

\NormalTok{seatbelts_deseason <-}\StringTok{ }\NormalTok{seatbelts }\OperatorTok{-}\StringTok{ }\NormalTok{seatbelts_dec}\OperatorTok{$}\NormalTok{seasonal}

\NormalTok{seat <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(seatbelts, seatbelts_deseason)}

\KeywordTok{plot.ts}\NormalTok{(seat, }
        \DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{,}
        \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
        \DataTypeTok{main =} \StringTok{"Original (red) and Seasonally Adjusted Series (blue)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-127-1.pdf}

Adjust for seasonal variations makes it possible to observe potentially noteworthy fluctuations. In the case of the AirPassengers data, for instance, the seasonally adjusted plot shows more clearly an anomaly in the year 1960 that was not noticeable in the raw data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers_decompose <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(AirPassengers, }\DataTypeTok{type=}\StringTok{"multiplicative"}\NormalTok{)}
\NormalTok{AirPassengers_seasonal <-}\StringTok{ }\NormalTok{AirPassengers_decompose}\OperatorTok{$}\NormalTok{seasonal}
\NormalTok{AirPassengers_deseasonal <-}\StringTok{ }\NormalTok{(AirPassengers}\OperatorTok{/}\NormalTok{AirPassengers_seasonal)}

\NormalTok{AirPass <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(AirPassengers, AirPassengers_deseasonal)}

\KeywordTok{plot.ts}\NormalTok{(AirPass, }
        \DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{,}
        \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
        \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{),}
        \DataTypeTok{main =} \StringTok{"Original (red) and Seasonally Adjusted Series (blue)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-128-1.pdf}

Below you can find another example with data from social media (you can download them here \href{https://drive.google.com/file/d/16NyBB1YOICSQ8lmZpcq62ahkJff9Tx6E/view?usp=sharing}{art1}, and \href{https://drive.google.com/file/d/1UWodHEAAphThP-jkALA5u1qEbTBPqHiE/view?usp=sharing}{art2}), consisting in posts published by pages of news media.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{art1 <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/art1.csv"}\NormalTok{)}
\NormalTok{art2 <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/art2.csv"}\NormalTok{)}

\NormalTok{art1_summary <-}\StringTok{ }\NormalTok{art1 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{post_created_date =} \KeywordTok{as.Date}\NormalTok{(post_created_date)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{complete}\NormalTok{(}\DataTypeTok{post_created_date =} \KeywordTok{seq.Date}\NormalTok{(}\KeywordTok{min}\NormalTok{(post_created_date), }
           \KeywordTok{max}\NormalTok{(post_created_date), }\DataTypeTok{by =} \StringTok{"day"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(post_created_date) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{posts =} \KeywordTok{n}\NormalTok{())}

\NormalTok{art2_summary <-}\StringTok{ }\NormalTok{art2 }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{post_created_date =} \KeywordTok{as.Date}\NormalTok{(post_created_date)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{complete}\NormalTok{(}\DataTypeTok{post_created_date =} \KeywordTok{seq.Date}\NormalTok{(}\KeywordTok{min}\NormalTok{(post_created_date), }
           \KeywordTok{max}\NormalTok{(post_created_date), }\DataTypeTok{by =} \StringTok{"day"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(post_created_date) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{posts =} \KeywordTok{n}\NormalTok{())}

\NormalTok{art1_ts <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ art1_summary}\OperatorTok{$}\NormalTok{posts, }\DataTypeTok{frequency =} \DecValTok{7}\NormalTok{)}
\NormalTok{art2_ts <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ art2_summary}\OperatorTok{$}\NormalTok{posts, }\DataTypeTok{frequency =} \DecValTok{7}\NormalTok{)}

\NormalTok{art <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(art1_ts, art2_ts)}

\NormalTok{art1_dec <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(art1_ts)}
\NormalTok{art2_dec <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(art2_ts)}

\NormalTok{art1_seas <-}\StringTok{ }\NormalTok{art1_dec}\OperatorTok{$}\NormalTok{seasonal}
\NormalTok{art2_seas <-}\StringTok{ }\NormalTok{art2_dec}\OperatorTok{$}\NormalTok{seasonal}

\NormalTok{art1_deseas <-}\StringTok{ }\NormalTok{art1_ts }\OperatorTok{-}\StringTok{ }\NormalTok{art1_seas}
\NormalTok{art2_deseas <-}\StringTok{ }\NormalTok{art2_ts }\OperatorTok{-}\StringTok{ }\NormalTok{art2_seas}

\NormalTok{art_des <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(art1_deseas, art2_deseas)}

\NormalTok{art_all <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(art, art_des)}

\KeywordTok{plot.ts}\NormalTok{(art_all, }
        \DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{,}
        \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
        \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
        \DataTypeTok{main =} \StringTok{"Original (red) and Seasonally Adjusted Series (blue)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-129-1.pdf}

By calculating a simple correlation between the original series and the de-seasonalized series, it can be observed that the correlation coefficients changes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(art)[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9641655
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(art_des)[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7209823
\end{verbatim}

\hypertarget{detrended-series}{%
\subsubsection{Detrended series}\label{detrended-series}}

As the series can be asjusted for seasonality, they can be also adjusted for trend based on the same reasons. A similar process can also be used to remove the trend from the data, in particular in the case of a \emph{deterministic} trend.

In the case of a \emph{stochastic} trend, instead, the usual practice is to detrend the data through \textbf{differencing}. Differecing means taking the first difference of consecutive points in time \(x_t\) - \(x_{t-1}\). In this way, the resulting series represents the \emph{relative change from one point in time to another}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Random_Walk <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)))}

\NormalTok{Random_Walk_diff <-}\StringTok{ }\KeywordTok{diff}\NormalTok{(Random_Walk)}

\KeywordTok{plot.ts}\NormalTok{(Random_Walk,}
        \DataTypeTok{main =} \StringTok{"Random Walk"}\NormalTok{, }
        \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-131-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.ts}\NormalTok{(Random_Walk_diff, }
        \DataTypeTok{main =} \StringTok{"Differenced Random Walk"}\NormalTok{, }
        \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-131-2.pdf}

Detrending a time series can be important before applying some statistical techniques, for instance before calculating the correlation between two time series. Time series with a trend component can reveal spurious correlations, since correlations may exist just because two variables are trending up or down at the same time. By detrending the time series, it can be more appropriately measured if the change in one time series over time is related to the change in another time series.

Detrending time series is also used when researchers consider irrelevant the trend. This is the case when the trend is considered an obvious characteristic of the process. For instance, economists can take for granted that there is an increasing trend in GDP due to inflation, and thus they may want to ``clean'' the data to eliminate this trivial trend. They are more interested in deviations from the growth, than in the growth that they consider a ``normal'' characteristic of the process.

There are also statistical tests to ascertain the presence of a trend.

A \emph{monotonic} trend can be detected with the \textbf{Mann--Kendall trend test}. The null hypothesis is that the data come from a population with independent realizations and are identically distributed. For the two sided test, the alternative hypothesis is that the data follow a monotonic trend (read the help: \emph{?mk.test}). The function to calculate this test is included in the package ``trend''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("trend")}
\KeywordTok{library}\NormalTok{(trend)}
\KeywordTok{mk.test}\NormalTok{(AirPassengers, }\DataTypeTok{alternative =} \StringTok{"greater"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Mann-Kendall trend test
## 
## data:  AirPassengers
## z = 14.382, n = 144, p-value < 2.2e-16
## alternative hypothesis: true S is greater than 0
## sample estimates:
##            S         varS          tau 
## 8.327000e+03 3.351643e+05 8.098232e-01
\end{verbatim}

\hypertarget{other-decomposition-methods-in-r}{%
\subsubsection{Other Decomposition Methods in R}\label{other-decomposition-methods-in-r}}

There are many different methods to decompose (and adjust) time series. Besides the classic \emph{decompose} function, the following can be mentioned:

\href{https://stat.ethz.ch/R-manual/R-devel/library/stats/html/stl.html}{\textbf{STL}} (in the base-R \emph{stats} library): Decompose a time series into seasonal, trend and irregular components using loess

{[}\textbf{X11}{]}, a method for decomposing quarterly and monthly data developed by the US Census Bureau and Statistics Canada, and the {[}\textbf{SEATS}{]} methods, implemented in the \href{http://www.seasonal.website/seasonal.html}{\emph{seasonal}} package.

\hypertarget{white-noise-and-stationarity}{%
\subsection{White Noise and Stationarity}\label{white-noise-and-stationarity}}

We said that the \textbf{residual} part of the model should be (approximately) \textbf{random}, which indicates that the model explained most of the significant patterns in the data (the \emph{``signal''}), leaving out the \emph{``noise''}.

The standard model of independent random variation in time series analysis is known as \textbf{white noise} (a term coined in an article published in Nature in 1922, where it was used to refer to series that contained all frequencies in equal proportions, analogous to white light). The charts below show how a white noise process looks like.

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-133-1.pdf} \includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-133-2.pdf}

When we introduced the concept of deterministic and stochastic trend, we said that the series showing the first type of trend are also called \textbf{trend stationary}, and the series showing the second type of trend are also called \textbf{difference stationary}. Both these names refer to the concept of \textbf{stationarity}.

A process is \emph{stationary} if it is homogeneous, that is, if it has no distinguished points in times or, in other words, its statistical qualities are the same for any point in time. There are more or less stringent definition of stationarity (\emph{strict and weak stationarity}), and the most used for practical purposes is the so-called \textbf{weak-stationarity}. In this sense, a time series is said to be stationary if there is:

\begin{itemize}
\tightlist
\item
  \textbf{no trend} (no systematic change in mean, that is, time invariant mean), and no seasonality (no periodic variations);
\item
  \textbf{no change in variance} over time (time invariant variance);
\item
  \textbf{no auto-correlation} (we'll return to this topic in the next chapters)
\end{itemize}

\emph{White noise is an example of stationary time series}. As you can see in the chart above, white noise time series is pretty regular, the mean is always the same (0) and there are no changes in variance over time: the plot looks much the same at any point in time!

Also through differencing, the series can achive a stationary form. This is the case, in particular, of the series with a stochastic trend, that are also called \emph{difference-stationary}, exactly because through differencing they become stationary.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.ts}\NormalTok{(Random_Walk_diff, }
        \DataTypeTok{main =} \StringTok{"Differenced Random Walk"}\NormalTok{, }
        \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-134-1.pdf}

The process through which stationarity is reached is also called \emph{Pre-Whitening}, and it can be used as a pre-processing phase before conducting correlation and regression analysis:

\begin{quote}
Once the form(s) of serial dependency that best account for the series are identified, they are removed from the series. This is called \emph{prewhitening} and is used to produce a series that is a ``white noise'' process (i.e., a process that is free of serial dependency with each value statistically independent of other values in the series). Once pre-whitening is accomplished the values of that series can be correlated with, used to predict, or predicted from, the values in other contemporaneous time series (usually also pre-whitened) representing other variables of interest. By removing serial dependency, the pre-whitening process makes these analyses free of correlated errors. It also removes the possibility that a common temporal trend or pattern is a confounding explanation for the observed association between the two variable series (VanLear, \href{https://us.sagepub.com/en-us/nam/the-sage-encyclopedia-of-communication-research-methods/book244974}{``Time Series Analysis''})
\end{quote}

The logic behind the process and the importance of white noise is also well explained in these sentences:

\begin{quote}
This ``residual'' part of the data, indeed, can be used as a dependent variable, giving the analyst confidence that any time series properties in the data will not account for any observed correlation between the covariates and the dependent variable. In the univariate context, the white noise process is important because it is what we would like to ``recover'' from our data -- after stripping away the ways in which a univariate
series can essentially explain itself. By removing the time series properties of our data, leaving only white noise, we have a series that can then be explained by other sources of variation. Another way to conceptualize white noise is as the exogenous portion of the data-generating process. Each of our data series is a function of those forces that cause the series to rise or fall (the independent variables we normally include to test our hypotheses) and of time series properties that lead those forces to
be more or less ``sticky''. After we filter away those time series properties, we are left with the forces driving the data higher or lower. We often refer to these forces as \textbf{shocks} -- and these shocks then reverberate in our data, sometimes for a short spell or a long spell or even infinitely. The goal of time series analysis is to separately model the time series properties (the reverberations) so the shocks (i.e., the white noise) can be captured. (Box-Steffensmeier, J. M., Freeman, J. R., Hitt, M. P., \& Pevehouse, J. C. (2014). Time series analysis for the social sciences. Cambridge University Press.)
\end{quote}

\hypertarget{correlations-and-arima}{%
\section{Correlations and ARIMA}\label{correlations-and-arima}}

\hypertarget{auto-correlation-acf-and-pacf}{%
\subsection{Auto-Correlation (ACF and PACF)}\label{auto-correlation-acf-and-pacf}}

In the previous chapter we said that a time series is said to be stationary if there is:

\begin{itemize}
\tightlist
\item
  \textbf{no trend} (no systematic change in mean, that is, time invariant mean), and no seasonality (no periodic variations);
\item
  \textbf{no change in variance} over time (time invariant variance);
\item
  \textbf{no auto-correlation} (we'll return to this topic in the next chapters)
\end{itemize}

Auto-correlation or serial correlation is an important characteristic of time series data and can be defined as the \emph{correlation of a variable with itself at different time points}.

Autocorrelation has many consequences. It prevents us to use traditional statistical methods such as linear regression, which assume that the observations are independent from each other. In presence of autocorrelation, the estimated standard errors of the parameter estimates will tend to be less than their true value. This will lead to erroneously high statistical significance being attributed to statistical tests (the \emph{p} values will be smaller than they should be).

In this section we introduce an important tool for the diagnosis of the properties of a time series, including autocorrelation: the \textbf{correlogram}. The accurate study of correlogram is a common step in many time series analysis procedures.

\hypertarget{correlogram-acf-and-pacf}{%
\subsubsection{Correlogram: ACF and PACF}\label{correlogram-acf-and-pacf}}

The correlogram is a chart that presents one of two statistics:

\begin{itemize}
\tightlist
\item
  \textbf{the autocorrelation function (ACF)}.The ACF statistic measures the correlation between \(x_t\) and \(x_{t+k}\) where \emph{k} is the number of lead periods into the future. It measures the correlation between any two points based on a given interval. It is not strictly equivalent to the Pearson product moment correlation. In R, ACF is calculated and visualized with the function ``acf'';
\item
  \textbf{the partial autocorrelation function (PACF)}. The PACF(k) is a measure of correlation between times series observations that are k units apart, after the correlation at intermediate lags has been controlled for or ``partialed'' out. In other words, the PACF measures the correlation between \(x_t\) and \(x_{t+k}\) after it has stripped out the effect of the intermediate \emph{x}'s. In R, the PACF is calculated and visualized with the function ``pacf''. It is useful to detect correlations that are not evident in ACF.
\end{itemize}

Let's consider, as an example, the correlogram of a random walk process. We know that this is a particular time series process in which the current values are combinations of the previous ones (\(x_t = x_{t-1} + w_t\), where \(x_{t-1}\) is the value immediately before x, and \(w_t\) is a random component). The resulting time series is characterized by a discernible pattern over time which is not exactly predictable (\emph{stochastic trend}). The ACF of a random walk time series, indeed, shows a correlation between values in the series: even values not so close are notwithstanding correlated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Random_Walk <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{499}\NormalTok{, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)))}

\KeywordTok{acf}\NormalTok{(Random_Walk)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-135-1.pdf}

Instead, the PACF, which removes the correlations between intermediate values, shows a correlation at lag 1, that is, it shows that the overall correlation depends on consequent values. The dotted blue lines signal the boundaries of statistical significance.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pacf}\NormalTok{(Random_Walk)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-136-1.pdf}

We can clearly visualize the auto-correlation by using a simple scatterplot, by plotting two consecutive lines of points.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(Random_Walk[}\DecValTok{1}\OperatorTok{:}\DecValTok{499}\NormalTok{], Random_Walk[}\DecValTok{2}\OperatorTok{:}\DecValTok{500}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-137-1.pdf}

The ACF or PACF of a white noise process is very different. We know that white noise is a stationary process, without distinguishable points in time and no correlation between points. Indeed, the ACF of white noise shows no correlation (the only line above statistical significance is at zero, which is nothing to be worried about, since it just means that each point is correlated with itself).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{White_Noise <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)))}

\KeywordTok{acf}\NormalTok{(White_Noise)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-138-1.pdf}

In the PACF we can see that there is nothing above the dotted line (which means that there is nothing statistically significant).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pacf}\NormalTok{(White_Noise)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-139-1.pdf}

If we plot two consecutive lists of points by using a scatterplot, we can see there is no serial correlation (no pattern is visible):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(White_Noise[}\DecValTok{1}\OperatorTok{:}\DecValTok{499}\NormalTok{], White_Noise[}\DecValTok{2}\OperatorTok{:}\DecValTok{500}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-140-1.pdf}

\hypertarget{arima-models}{%
\subsection{ARIMA models}\label{arima-models}}

The ACF and PACF plots can be used to diagnose the main characteristics of a time series and find a proper statistical model. We talk about univariate models, since they are models to describe a single time series. Univariate time series can be modeled as \textbf{Auto Regressive (AR), Integrated (I), and Moving Average (MA) processes}. These models are synthesized using the acronym \textbf{ARIMA}. When a seasonal (S) component is also taken into account, we also use the acronym \emph{SARIMA}.

\hypertarget{auto-regressive-ar-models}{%
\subsubsection{Auto Regressive (AR) models}\label{auto-regressive-ar-models}}

We just said that a time series is often characterized by auto-correlation, so we can clearly deduce that we can model it by using a regression model, that is, by regressing the time series on its past values. In this way we have an \textbf{auto-regressive model}: a regression of \(x_{t}\) on past terms \(x_{t-k}\) from the same series.

In time series analysis, past terms \(x_{t-k}\) from a same series are called \textbf{lags}. The lagged values of a time series are its delayed values, where the delay can be of an arbitrary amount of time \(k\). For instance, considering a simple series of 4 data points distributed from time \({t+0}\) (first data point) to time \({t+3}\) (last data point) \({x_{t+0}, x_{t+1}, x_{t+2}, x_{t+3}}\), the corresponding lagged series, assuming \(k=1\), is \({NA, x_{t+0}, x_{t+1}, x_{t+2}}\). Notice that the first data point is missing since there is no data point behind it, and the other data points are shifted one time point ahead.

An auto-regressive (AR) model can be described as follows (the \(\alpha\) are coefficients, \(t\) are time points, \(w\) is a random component or white noise):

\[
{x_t} = \alpha x_{t-1} + \alpha x_{t-2} + \alpha x_{t-k} + {w_t}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.90}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(AR_}\DecValTok{1}\NormalTok{, }\DataTypeTok{main =} \StringTok{"AR(1)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-141-1.pdf}

The ACF of an autoregressive process typically shows a slow and gradual decay in autocorrelation over time.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acf}\NormalTok{(AR_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-142-1.pdf}

The PACF of an autoregressive process shows a peak in correspondence with the order of the model. In the case of an AR(1) the peak is at time 1.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pacf}\NormalTok{(AR_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-143-1.pdf}

In the case of an AR(3) the peak is at time 1, 2, and 3.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{)))}
\KeywordTok{plot}\NormalTok{(AR_}\DecValTok{3}\NormalTok{, }\DataTypeTok{main =} \StringTok{"AR(3)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-144-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pacf}\NormalTok{(AR_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-144-2.pdf}

Now we can see that the \textbf{random walk} process we have seen above is a particular case of auto-regressive model. In a random walk process, each value is the previous one plus a random part:

\[
x_t = x_{t-1} + w_t
\]

Thus each point \(x_t\) is correlated with the previous one \(x_{t-k}\) where the lag value \(k\) is equal to 1 (\({k=1}\)). Therefore, a random walk process is an auto-regressive model of \textbf{order 1}, since just 1 lag is taken into consideration in the auto-regressive model (and with \(\alpha = 1\)). The order of an auto-regressive model is indicated by parenthesis, e.g.: \textbf{AR(1)}.

The AR process can have different characteristics (and different ACF and PACF) based on the parameters.

\hypertarget{moving-average-ma-models}{%
\subsubsection{Moving Average (MA) models}\label{moving-average-ma-models}}

We already know Moving Average as a method to smooth time series and detect a trend. When referring to Moving Average as a process (MA), we refer to a process in which the values of the series are a function of a \textbf{weighted average of past errors}. In other terms, a moving average (MA) process is a linear combination of the current white noise term and the \(q\) most recent past white noise terms:

\[
{x_t} = w_t + \beta w_{t-1} + ... + \beta w_{t-q}
\]

The order of a MA process indicates the lags of white noise taken into account in the model (e.g: \emph{MA(3)}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MA_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{ma =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{)))}
\KeywordTok{plot}\NormalTok{(MA_}\DecValTok{3}\NormalTok{, }\DataTypeTok{main =} \StringTok{"MA(3)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-145-1.pdf}
The ACF plot of a MA process shows a more clear cut-off after the term corresponding to the order ot the process. It is different from the ACF of an AR process, which shows a more gradual decay.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acf}\NormalTok{(MA_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-146-1.pdf}

The PACF of a MA process shows an up-and-down movement and does not shut off, but instead tapers toward 0 in some manner.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pacf}\NormalTok{(MA_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-147-1.pdf}

\hypertarget{integrated-i-process}{%
\subsubsection{Integrated (I) process}\label{integrated-i-process}}

An integrated process is a non-stationary time series process that becomes stationary when transformed by \textbf{differencing}. In other words, an integrated process is a difference-stationary process, that is a process with a stochastic trends (see the previous chapter).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{I_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)))}
\KeywordTok{plot}\NormalTok{(I_}\DecValTok{1}\NormalTok{, }\DataTypeTok{main =} \StringTok{"I(1)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-148-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{diff}\NormalTok{(I_}\DecValTok{1}\NormalTok{), }\DataTypeTok{main =} \StringTok{"I(1) after 'differencing'"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-149-1.pdf}

\hypertarget{seasonal-s-models}{%
\subsubsection{Seasonal (S) models}\label{seasonal-s-models}}

We already introduced the seasonal model. For instance, a dataset showing a seasonal component is AirPassengers. The seasonality appears in the yearly fluctuations in the ACF and in the spikes occurring at 12 months from each other in the PACF.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"AirPassengers"}\NormalTok{)}

\CommentTok{# this function par(mfrow=c(..., ...))}
\CommentTok{# is used to combine more than one plot}
\CommentTok{# in the same frame. The two numerical values}
\CommentTok{# indicates number of rows and columns}
\CommentTok{# the frame is made of}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\KeywordTok{acf}\NormalTok{(AirPassengers, }\DataTypeTok{lag.max =} \DecValTok{48}\NormalTok{)}
\KeywordTok{pacf}\NormalTok{(AirPassengers, }\DataTypeTok{lag.max =} \DecValTok{48}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-150-1.pdf}

\hypertarget{fit-sarima-models}{%
\subsubsection{Fit (S)ARIMA models}\label{fit-sarima-models}}

The above examples represent simple processes, but real time series are often the result of more complex \textbf{mixtures of different types of process}, and therefore it is more complex to identify an appropriate model for the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7623}\NormalTok{)}
\NormalTok{arima_}\DecValTok{112}\NormalTok{ <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{ma =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.2}\NormalTok{)))}
\KeywordTok{plot}\NormalTok{(arima_}\DecValTok{112}\NormalTok{, }\DataTypeTok{main =} \StringTok{"ARIMA(1,1,2)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-151-1.pdf}

A popular methods to find the appropriate model is the \href{https://en.wikipedia.org/wiki/BoxJenkins_method}{Box-Jenkins method}, a recursive process involving the analysis of a time series, the guess of possible (S)ARIMA models, the fit of the hypothesized models, and a meta-analysis to determine the best specification. Once a best-fitting model has been
found, the correlogram of the \textbf{residuals} should be verified as \textbf{white noise}.

The Box-Jenkins method could be time-consuming and requires some expertise. ACF/PACF can also become difficult to read in case of complex models, and their appropriate interpretation could require a lot of expertise as well. Fortunately, experts have developed \textbf{automated methods} that allow us to automatically found and fit an ARIMA model. This is the case of the \textbf{auto.arima} function implemented in the \textbf{forecast} package (a package for time series analysis and especially for forecasting, developed by \href{https://scholar.google.com/citations?user=vamErfkAAAAJ\&hl=en\&oi=ao}{Rob J. Hyndman}, professor of statistics and time series analysis expert).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Install the package if you haven't installed it yet}
\CommentTok{# install.packages("forecast")}
\KeywordTok{library}\NormalTok{(forecast)}

\NormalTok{arima_fit <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(arima_}\DecValTok{112}\NormalTok{)}

\NormalTok{arima_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: arima_112 
## ARIMA(1,1,2) 
## 
## Coefficients:
##          ar1     ma1     ma2
##       0.7649  0.7387  0.2484
## s.e.  0.0356  0.0528  0.0534
## 
## sigma^2 estimated as 1.035:  log likelihood=-717.9
## AIC=1443.8   AICc=1443.88   BIC=1460.65
\end{verbatim}

As said above, to evaluate the fit of a model we should analyze the \textbf{residuals}, and ascertain they behave as white noise. The object resulting from the function \emph{auto.arima} has a slot including the residuals. To ascertain that residuals are white noise we can plot its \textbf{ACF and PACF} (no spike should be significant) and also its \textbf{histogram}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{layout}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{))}
\KeywordTok{acf}\NormalTok{(arima_fit}\OperatorTok{$}\NormalTok{residuals)}
\KeywordTok{pacf}\NormalTok{(arima_fit}\OperatorTok{$}\NormalTok{residuals)}
\KeywordTok{hist}\NormalTok{(arima_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main =} \StringTok{"Histogram of residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-153-1.pdf}

The \emph{forecast} package also implements the function \textbf{checkresiduals} to create nice and complete plots of residual diagnostics by using a simple function.

Besides creating the plots, the function calulate the \textbf{Ljung-Box test} (default), or the \textbf{Breusch-Godfrey test} (if you specify \emph{test=``BG''} inside the function):

The \emph{Ljung-Box test} (and also the Breusch--Godfrey test) is a diagnostic tool, applied to the residuals of a time series after fitting an ARIMA model, to test the lack of fit. The test examines the autocorrelations of the residuals. If there are no significant autocorrelations, it can be concluded that the model does not exhibit significant lack of fit. To pass the test, the p-value has to be above the significance level (usually 0.05)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{checkresiduals}\NormalTok{(arima_fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-154-1.pdf}

\begin{verbatim}
## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(1,1,2)
## Q* = 5.8977, df = 7, p-value = 0.5517
## 
## Model df: 3.   Total lags used: 10
\end{verbatim}

\hypertarget{sarima}{%
\paragraph{SARIMA}\label{sarima}}

If we fit a model to the \emph{AirPassengers} dataset, which has a seasonal component, we find a Seasonal Autoregressive Integrated Moving Average model (\textbf{SARIMA}). The seasonal component of the AirPassenger dataset is evident in the plot of the series and its ACF and PACF. The \emph{forecast} package has a useful function \textbf{ggtsdisplay} to plot a time series along with its ACF and PACF.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggtsdisplay}\NormalTok{(AirPassengers)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-155-1.pdf}

The seasonal part of the ARIMA model consists of terms that are similar to the non-seasonal components of the model, but involves lagged values of the seasonal period.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers_sarima <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(}\KeywordTok{window}\NormalTok{(AirPassengers))}
\NormalTok{AirPassengers_sarima}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: window(AirPassengers) 
## ARIMA(2,1,1)(0,1,0)[12] 
## 
## Coefficients:
##          ar1     ar2      ma1
##       0.5960  0.2143  -0.9819
## s.e.  0.0888  0.0880   0.0292
## 
## sigma^2 estimated as 132.3:  log likelihood=-504.92
## AIC=1017.85   AICc=1018.17   BIC=1029.35
\end{verbatim}

\hypertarget{forecasting}{%
\paragraph{Forecasting}\label{forecasting}}

Based on the ARIMA models we found, we can also try to \textbf{forecast} future values. We can use the function \textbf{forecast} of the homonym library. For instance, we could try to forecast the values of the AirPassenger dataset in the next four years.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers_forecast <-}\StringTok{ }\KeywordTok{forecast}\NormalTok{(AirPassengers_sarima, }\DataTypeTok{h=}\DecValTok{48}\NormalTok{, }\DataTypeTok{level =} \DecValTok{90}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(AirPassengers_forecast, }\DataTypeTok{main =} \StringTok{"AirPassengers forecast"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-157-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AirPassengers_sarima <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(AirPassengers)}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-correlation}{%
\subsection{Cross-correlation}\label{cross-correlation}}

\emph{Cross-correlation }is the correlation between the (lagged) values of a time series and the values of another series. Similarly to ACF and PACF, there is a specific plot that shows the cross-correlation between two time series, and a specific R function: \textbf{ccf}.

The cross-correlation can be useful to understand wich lagged values of a \emph{X} series can be used to predict the values of a \emph{Y} series, and thus used, for instance, in a time series regression model.

Unfourtunately, the problem with the cross-correlation function is that, as we said in the preceding sections, with autocorrelated data it is difficult to assess the dependence between two processes, and it is possible to find spurious correlations.

Thus, it is pertinent to disentangle the linear association between \emph{X} and \emph{Y} from their autocorrelation. A useful device for doing this is \textbf{prewhitening}. The prewhitening method works as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  determine an ARIMA \emph{time series model} for the X-variable, and store the residuals from this model;
\item
  fit the ARIMA X-model to the Y-variable, and keep the residuals;
\item
  examines the CCF between the X and Y model residuals.
\end{enumerate}

We can implement this procedure writing all the necessary code, or by using the library \emph{forecast}, or also, alternatively, the library \emph{TSA}.

To make an example, we apply the method to two simulated two series. The Y-variable is created in such a way that it is correlated with the lagged values at time \(x_{t-3}\) and \(x_{t-4}\). Therefore, we should find a correlation at those lags.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x_series <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{200}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{1}\NormalTok{))}
\NormalTok{z <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(x_series, stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x_series, }\DecValTok{-3}\NormalTok{), stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x_series, }\DecValTok{-4}\NormalTok{)) }
\NormalTok{y_series <-}\StringTok{ }\DecValTok{15} \OperatorTok{+}\StringTok{ }\FloatTok{0.8}\OperatorTok{*}\NormalTok{z[,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\FloatTok{1.5}\OperatorTok{*}\NormalTok{z[,}\DecValTok{3}\NormalTok{] }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{197}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The cross-correlation applied to the original series results in a plot where everything seems to be correlated. The ``real'' correlation at \(x_{t-3}\) and \(x_{t-4}\) is not discernible at all.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ccf}\NormalTok{(x_series, y_series, }\DataTypeTok{na.action =}\NormalTok{ na.omit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-159-1.pdf}

By using the \emph{forecast} library, we can calculate the pre-withened \emph{ccf} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit an ARIMA model}
\NormalTok{x_model <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(x_series)}
\CommentTok{# keep the residuals ("white noise")}
\NormalTok{x_residuals <-}\StringTok{ }\NormalTok{x_model}\OperatorTok{$}\NormalTok{residuals}

\CommentTok{# fit the same ARIMA model to the Y-series}
\CommentTok{# by using the "Arima" function in forecast}
\NormalTok{y_model <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(y_series, }\DataTypeTok{model=}\NormalTok{x_model)}
\CommentTok{# keep the residuals}
\NormalTok{y_filtered <-}\StringTok{ }\KeywordTok{residuals}\NormalTok{(y_model)}

\CommentTok{# apply the ccf to the residuals}
\KeywordTok{ccf}\NormalTok{(x_residuals, y_filtered)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-160-1.pdf}

Now, it's clear that the X-variable is correlated with the Y-variable at \(x_{t-3}\) and \(x_{t-4}\).

The previous steps show in some detail the steps involved in the pre-whitening strategy, but it is possible to use the original series with the \textbf{prewhiten} function of the \textbf{TSA} library. Although the function can take, as an argument, a pre-fitted ARIMA model, its greater advantage is that it can take care of all the necessary steps to prewithen the series. In particular, if no model is specified, the library automatically applies a simple AR model. Although this model can be just an approximation of the ``true'' model (which can be more complex), an approximation can be enough to pre-whiten the series and find a proper cross-correlation (that is, also a simpler and approximate model can do the job).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("TSA")}
\KeywordTok{library}\NormalTok{(TSA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Registered S3 methods overwritten by 'TSA':
##   method       from    
##   fitted.Arima forecast
##   plot.Arima   forecast
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'TSA'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:readr':
## 
##     spec
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     acf, arima
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:utils':
## 
##     tar
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prewhiten}\NormalTok{(x_series, y_series)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-161-1.pdf}

\hypertarget{examples-in-literature}{%
\subsection{Examples in literature}\label{examples-in-literature}}

A few examples to exemplify the use of ARIMA and Cross-Correlation in the scientific literature, with specific reference to communication science.

In \href{https://academic.oup.com/joc/article/61/1/48/4098436?casa_token=b3Cd_02sN9IAAAAA:MM9rnMjb14X1ZSqxrcswoO3CDEYHry97L9EG9vL4dt5kpkZryx8VSlR8F_wXaOmBBu9VZvqlrLCZ0w}{Scheufele, B., Haas, A., \& Brosius, H. B. (2011). Mirror or molder? A study of media coverage, stock prices, and trading volumes in Germany. Journal of Communication, 61(1), 48-70}, the authors investigate \emph{``the short-term relationship between media coverage, stock prices, and trading volumes of eight listed German companies''}, by using ARIMA and cross-correlation, in particular asking:

\begin{quote}
RQ2: Do cross-lagged correlations between media coverage and stock prices or trading volumes differ according to the amount and the valence of coverage?
RQ3: Do cross-lagged correlations between media coverage and stock prices or trading volumes differ according to the type of media (Financial Web sites, daily newspapers, and stock market TV shows) which reports on the company or stock?
\end{quote}

To answer these questions, the authors made use of time series analysis. In particular, they:

\begin{quote}
estimated cross-lagged correlations between media coverage and stock prices or trading volumes, respectively. Basically, two steps of time-series analysis can be distinguished: (a) In the first step, each media time-series and each time-series of trading volumes was adjusted by ARIMA (Autoregressive Integrated Moving Average) modeling separately. The differences between the original time-series and its ARIMA model are called residuals and were used for analysis. Like with ordinary least squares regression, these residuals should not be auto-correlated. If the residuals are not auto-correlated, time-series analysis speaks of White Noise. This modeling technique called prewhitening was necessary to avoid spurious correlations. (\ldots) (b) In the next step, cross-correlations between each adjusted media time-series and each adjusted stock series were calculated. (\ldots) The coefficient expresses the strength of correlation, whereas the lags offer an insight into dynamics: Correlations at positive (negative) lags indicate that changes in media coverage proceeded (succeeded) shifts in stock prices or trading volumes.
\end{quote}

In \href{https://ijoc.org/index.php/ijoc/article/viewFile/495/392}{Groshek, J. (2010). A time-series, multinational analysis of democratic forecasts and Internet diffusion. International Journal of Communication, 4, 33}, the author \emph{examines the democratic effects that the Internet has shown using macro- level, cross-national data in a sequence of time--series statistical tests}:

\begin{quote}
this study relies principally on macro-level time--series democracy data from an historical sample that includes 72 countries, reaching back as far as 1946 in some cases, but at least from 1954 to 2003. From this sample, a sequence of ARIMA (autoregressive integrated moving average) time--series regressions were modeled for each country for at least 40 years prior to 1994. These models were then used to generate statistically-forecasted democracy values for each country, in each year from 1994 to 2003. A 95\% confidence interval with an upper and lower democracy score was then constructed around each of the forecasted values using dynamic mean squared errors. The actual democracy scores of each country for each year from 1994 to 2003 were then compared to the upper and lower values of the confidence interval.
In the event that the actual democracy level of any country was greater than the upper value of the forecasted democracy score during the time period of 1994 to 2003, Internet diffusion was investigated in case studies as a possible causal mechanism.
\end{quote}

In other terms, the author used a forecasting approach to predict the values of the series from 1994 to 2003, in order to find statistically significant differences between the predicted and the actual values. These discrepancies were interpreted as caused by factors that were not present in the past, and possibly by the introduction of the Internet.

The study found that, \emph{based on the results of the 72 countries reported here, the diffusion of the Internet should not be considered a democratic panacea, but rather a component of contemporary democratization processes}

\hypertarget{regression}{%
\section{Regression}\label{regression}}

In this chapter we are going to see how to conduct a regression analysis with time series data.

\emph{Regression analysis} is a used for estimating the relationships between a \emph{dependent variable (DV)} (also called \emph{outcome} or \emph{response}) and one or more \emph{independent variables (IV)} (also called \emph{predictors} or \emph{explanatory variables}).

A standard regression model \(Y\) = \(\beta\) + \(\beta x\) + \(\epsilon\) has no time component. Differently, a time series regression model includes a time dimension and can be written, in a simple and general formulation, using just one explanatory variable, as follows:

\[
y_t = \beta_0 + \beta_1x_t + \epsilon_t
\]

In this equation, \(y_t\) is the time series we try to understand/predict (the \emph{dependent variable (DV)}), \(\beta_0\) is the \emph{intercept} (a constant value that represents the expected mean value of \(y_t\) when \(x_t = 0\)), the coefficient \(\beta_1\) is the \emph{slope}, representing the average change in \(y\) at one unit increase in \(x\) (the \emph{independent variable (IV) or explanatory variable}), and \(\epsilon_t\) is the time series of residuals (the error term).

A multiple regression, with more than one explanatory variable, can be written as follows:

\[
y_t = \beta_0 + \beta_1x_{1,t} + \beta_2x_{2,t} + ... + \beta_kx_{k,t} + \epsilon_t
\]

\hypertarget{static-and-dynamic-models}{%
\subsection{Static and Dynamic Models}\label{static-and-dynamic-models}}

From a time series analysis perspective, a general distinction can be made between ``static'' and ``dynamic'' regression models:

\begin{itemize}
\tightlist
\item
  A \textbf{static regression model} includes just contemporary relations between the explanatory variables (independent variables) and the response (dependent variable). This model could be appropriate when the expected value of the response changes \emph{immediately} when the value of the explanatory variable changes. Considering a model with \(k\) independent variables \{\(x_1\), \(x_2\), \ldots, \(x_k\)\}, a static (multiple) regression model, has the form just seen above:
\end{itemize}

\[
y_t = \beta_0 + \beta_1x_{1,t} + \beta_2x_{2,t} + ... + \beta_kx_{k,t} + \epsilon_t
\]

Each \(\beta\) coefficient models the \emph{instant change} in the conditional expected value of the response variable \(y_t\) as the value of \(x_{k,t}\) changes by one unit, keeping constant all the other predictors (i.e.: the other \(x_{k,t}\)):

\begin{itemize}
\tightlist
\item
  A \textbf{dynamic regression model} includes relations between \emph{both the current and the lagged (past) values of the explanatory (independent) variables}, that is, the expected value of the response variable may change \emph{after} a change in the values of the explanatory variables.
\end{itemize}

\[
\begin{aligned} 
y_t = \beta_0  & + \beta_{10}x_{1,t} + \beta_{11}x_{1,t-1} + ... + \beta_{1m}x_{1,t-m} \\
& + \beta_{20}x_{2,t} + \beta_{21}x_{2,t-1} + ... + \beta_{2m}x_{2,t-m} \\
& + \dots \\
& + \beta_{k0}x_{k,t} + \beta_{k1}x_{k,t-1} + ... + \beta_{km}x_{2,t-m} \\
& + \epsilon_t \\
\end{aligned} 
\]

Despite the differences between these two analytic perspectives, the term \emph{dynamic regression} is also used, in the literature, in a more general way to refer to regression models with autocorrelated errors (also when they are used to analyze only contemporary relations between variables).

\hypertarget{regression-models}{%
\subsection{Regression models}\label{regression-models}}

Except for the possible use of lagged regressors, which are typical of time series, the above described statistical models are standard regression models, commonly used with cross-sectional data.

Standard linear regression models can sometimes work well enough with time series data, \textbf{if specific conditions are met}. Besides standard assumptions of linear regression\footnote{1) Linearity: The relationship between X and Y must be linear; 2) Independence of errors: There is not a relationship between the residuals and the Y variable; 3) Normality of errors: The residuals must be approximately normally distributed; 4) Equal variances: The variance of the residuals is the same for all values of X}, a careful analysis should be done in order to ascertain that \textbf{residuals are not autocorrelated}, since this can cause problems in the estimated model.

In this chapter we'll see how to deal with autocorrelated residuals. However, even before that, it is important that the series are \textbf{stationary}, in order to avoid possible \emph{spurious correlations}.

\hypertarget{stationarity}{%
\subsubsection{Stationarity}\label{stationarity}}

We already discussed stationarity in the previous chapters. Here we can observe that time series can be nonstationary due to different reasons, thus different strategies can be employed to \emph{stationarize} the data.

For instance, a nonstationary series can be a series with \textbf{unequal variance} over time. A common way to try to fix the problem is by applying a log-transformation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(xts)}

\NormalTok{elections_news <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/elections-stories-over-time-20210111144254.csv"}\NormalTok{)}
\NormalTok{elections_news}\OperatorTok{$}\NormalTok{date <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(elections_news}\OperatorTok{$}\NormalTok{date)}

\NormalTok{elections_news <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(elections_news}\OperatorTok{$}\NormalTok{count, }\DataTypeTok{order.by =}\NormalTok{ elections_news}\OperatorTok{$}\NormalTok{date)}
\NormalTok{elections_news_log <-}\StringTok{ }\KeywordTok{log}\NormalTok{(elections_news}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{elections_news_xts <-}\StringTok{ }\KeywordTok{merge.xts}\NormalTok{(elections_news, elections_news_log)}

\KeywordTok{plot.xts}\NormalTok{(elections_news_xts, }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{),}
         \DataTypeTok{multi.panel =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{yaxis.same =} \OtherTok{FALSE}\NormalTok{,}
         \DataTypeTok{main =} \StringTok{"Original vs Log-transformed series"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-162-1.pdf}

Another reason for nonstationarity is the periodic variation due to \textbf{seasonality} (regular fluctuations in a time series that follow a specific time pattern, e.g.: social media activity during week-ends, Christmas effect in consumption, etc.).

To remove the seasonal pattern, you might want to use a \emph{seasonally-adjusted} time series. Otherwise, you could create a dummy variable for the seasonal period (that is, a variable that follows the seasonal pattern in the data in order to account, in the model, for these fluctuations).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the ts dataset AirPassenger}
\KeywordTok{data}\NormalTok{(}\StringTok{"AirPassengers"}\NormalTok{)}

\CommentTok{# remove seasonality from a multiplicative model}
\NormalTok{AirPassengers_decomposed <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(AirPassengers, }\DataTypeTok{type=}\StringTok{"multiplicative"}\NormalTok{)}
\NormalTok{AirPassengers_seasonal_component <-}\StringTok{ }\NormalTok{AirPassengers_decomposed}\OperatorTok{$}\NormalTok{seasonal}
\NormalTok{AirPassengers_seasonally_adjusted <-}\StringTok{ }\NormalTok{AirPassengers}\OperatorTok{/}\NormalTok{AirPassengers_seasonal_component}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot.ts}\NormalTok{(AirPassengers, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Original series"}\NormalTok{)}
\KeywordTok{plot.ts}\NormalTok{(AirPassengers_seasonally_adjusted, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,  }
        \DataTypeTok{main =} \StringTok{"Seasonally-adjusted series"}\NormalTok{, }
        \DataTypeTok{ylab =} \StringTok{"Seasonally-adjusted values"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-163-1.pdf}

An important reason for nonstationarity is also the presence of a trend in the data. There are \textbf{stochastic trends} and \textbf{deterministic trends}. Deterministic trends are a fixed function of time, while stochastic trends change in an unpredictable way.

Series with a deterministic trend are also called \emph{trend stationary} because they can be stationary around a deterministic trend, and it could be possible to achieve stationarity by removing the time trend. In trend stationary processes, the shocks to the process are transitory and the process is \emph{mean reverting}.

Processes with a \emph{stochastic trend} are also called \emph{difference stationary} because they can become stationary through \emph{differencing}. In series with stochastic trends we could see that shocks have permanent effects.

When dealing with \emph{deterministic trend}, we might want to work with detrended series.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove the trend from a multiplicative model}
\NormalTok{AirPassengers_decomposed <-}\StringTok{ }\KeywordTok{decompose}\NormalTok{(AirPassengers, }\DataTypeTok{type=}\StringTok{"multiplicative"}\NormalTok{)}
\NormalTok{AirPassengers_trend_component <-}\StringTok{ }\NormalTok{AirPassengers_decomposed}\OperatorTok{$}\NormalTok{trend}
\NormalTok{AirPassengers_detrended <-}\StringTok{ }\NormalTok{AirPassengers}\OperatorTok{/}\NormalTok{AirPassengers_trend_component}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot.ts}\NormalTok{(AirPassengers, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Original series"}\NormalTok{)}
\KeywordTok{plot.ts}\NormalTok{(AirPassengers_detrended, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,  }
        \DataTypeTok{main =} \StringTok{"Detrended series"}\NormalTok{, }
        \DataTypeTok{ylab =} \StringTok{"Detrended values"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-164-1.pdf}

Otherwise, in regression analysis, it is more common to add a dummy variable consisting of a value that increases with time, to account for a linear deterministic time trend. This time-count variable will remove the deterministic trend from the dependent variable, allowing the other predictors to explain the remaining variance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a simulate series}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1312}\NormalTok{)}
\NormalTok{toy_data <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)))}

\CommentTok{# add a deterministic trend to the series}
\NormalTok{toy_data_trend <-}\StringTok{ }\NormalTok{toy_data }\OperatorTok{+}\StringTok{ }\FloatTok{0.2}\OperatorTok{*}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(toy_data)}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{plot.ts}\NormalTok{(toy_data, }\DataTypeTok{main =} \StringTok{"Original series"}\NormalTok{)}
\KeywordTok{plot.ts}\NormalTok{(toy_data_trend, }\DataTypeTok{main =} \StringTok{"Series with Trend"}\NormalTok{)}

\NormalTok{dummy_trend <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(toy_data_trend)}
\NormalTok{lm_toydata <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(toy_data_trend }\OperatorTok{~}\StringTok{ }\NormalTok{dummy_trend)}
\KeywordTok{plot.ts}\NormalTok{(lm_toydata}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main =} \StringTok{"Residuals (detrended)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-165-1.pdf}

When we have a series with a stochastic trend, we can achieve stationarity through differencing.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{111}\NormalTok{)}
\NormalTok{Random_Walk <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)))}

\NormalTok{Random_Walk_diff <-}\StringTok{ }\KeywordTok{diff}\NormalTok{(Random_Walk)}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot.ts}\NormalTok{(Random_Walk,}
        \DataTypeTok{main =} \StringTok{"Random Walk"}\NormalTok{, }
        \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}

\KeywordTok{plot.ts}\NormalTok{(Random_Walk_diff, }
        \DataTypeTok{main =} \StringTok{"Differenced Random Walk"}\NormalTok{, }
        \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-166-1.pdf}

\hypertarget{tests-for-stochastic-and-deterministic-trend}{%
\paragraph{Tests for Stochastic and Deterministic Trend}\label{tests-for-stochastic-and-deterministic-trend}}

The correct detrending method depends on the type of trend. First differencing is appropriate for intergrated \emph{I(1)} time series and time-trend regression is appropriate for trend stationary \emph{I(0)} time series.

In case of deterministic trend, differencing is the incorrect solution, while detrending the series in function of time (regressing the series on a variable such as time and saving the residuals) is the correct solution. Differencing when none is required (\emph{over-differencing}) may induce dynamics into the series that are not part of the data-generating process (for instance, it could create a first-order moving average process).

Specific statistical tests have been developed to distinguish between the two types of trends. In particular, \emph{unit root tests} and \emph{stationary test} can be used to determine if trending data should be first differenced or regressed on deterministic functions of time to render the data stationary.

Considering a simple model like the following, where \(Td\) is a deterministic linear trend and \(z_t\) is an autoregressive process of order 1 \emph{AR(1)}. The difference between a process with stochastic and deterministic trend can be traced back to the parameter \(|\phi|\): When \(|\phi| = 1\), then \(z_t\) is a \emph{stochastic trend} and \(y_t\) is an integrated process \emph{I(1)} with \emph{drift} (the so-called ``drift'' refers to the presence of a constant term, in this case \(\kappa\)). When \(\phi < 1\), the process is not integrated (\emph{I(0)}) and \(y_t\) exhibits a \emph{deterministic trend}\footnote{Reference of this part is Zivot E., Wang J. (2003), Unit Root Tests, in \emph{Modeling Financial Time Series with S-Plus}. Springer, New York}:

\[
\begin{aligned} 
& y_t = Td_t + z_t \\
& Td_t = \kappa + \delta_t \\
& z_t = \phi z_{t-1} + \epsilon_t, \ \epsilon_t \sim N(0, \sigma^2)
\end{aligned} 
\]
Let's simulate and visualize the above equation (\(y_t = \kappa + \delta_t + \phi z_{t-1} + \epsilon_t\)):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{t <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{500} 
\NormalTok{kappa <-}\StringTok{ }\DecValTok{5} \CommentTok{# costant term (or "drift")}
\NormalTok{delta <-}\StringTok{ }\FloatTok{0.1} 
\NormalTok{epsilon <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(n)\{ }\CommentTok{# function for the error term}
    \KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =} \FloatTok{0.8}\NormalTok{)}
\NormalTok{    \} }

\NormalTok{y_I1 <-}\StringTok{ }\NormalTok{kappa }\OperatorTok{+}\StringTok{ }\NormalTok{(delta }\OperatorTok{*}\StringTok{ }\NormalTok{t) }\OperatorTok{+}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n=}\DecValTok{499}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)), }
                                        \DataTypeTok{rand.gen =}\NormalTok{ epsilon)}
\NormalTok{y_I0 <-}\StringTok{ }\NormalTok{kappa }\OperatorTok{+}\StringTok{ }\NormalTok{(delta }\OperatorTok{*}\StringTok{ }\NormalTok{t) }\OperatorTok{+}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n=}\DecValTok{500}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.8}\NormalTok{),}
                                        \DataTypeTok{rand.gen =}\NormalTok{ epsilon)}

\NormalTok{ts_y <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(y_I1, y_I0)}

\KeywordTok{plot.ts}\NormalTok{(ts_y, }\DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{, }
        \DataTypeTok{lty=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
        \DataTypeTok{main =} \StringTok{"Stochastic w/ drift (red) Deterministic Trend (blue)"}\NormalTok{,}
        \DataTypeTok{ylab=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-167-1.pdf}

\textbf{Unit root tests} are aimed at testing the null hypothesis that \(|\phi| = 1\) (\emph{difference stationary}), against the alternative hypothesis that \(|\phi| < 1\) (\emph{trend stationary}).

\textbf{Stationarity tests} take the null hypothesis that \(y_t\) is trend stationary, and are based on testing for a moving average element in \(\Delta z_t\) (\(\Delta\) represents the operation of differencing).

\[
\begin{aligned} 
& Original \\
& y_t = Td_t + z_t  \\
& \ Td_t = \kappa + \delta_t \\
& z_t = \phi z_{t-1} + \epsilon_t, \ \epsilon_t \sim N(0, \sigma^2) 
\end{aligned} 
\]
\$\$
\textbackslash begin\{aligned\}
\& First ~difference \textbackslash{}
\& \Delta y\_t = \Delta Td\_t + \Delta z\_t \textbackslash{}
\& \Delta Td\_t = \Delta \kappa + \Delta \delta\emph{t = \delta \textbackslash{}
\& \Delta z\_t = \phi \Delta z}\{t-1\} + \Delta \epsilon\emph{t = \phi \Delta z}\{t-1\} + \epsilon\emph{t - \epsilon}\{t-1\}

\textbackslash end\{aligned\}
\$\$

\(\Delta z_t\) can be also written as:

\[
\Delta \epsilon_t = \phi \Delta z_{t-1} + \epsilon_t + \theta \epsilon_{t-1}
\]

with \(\theta = -1\). That is, when the series is trend stationary, taking the first difference results in overdifferencing and in the creation of a moving average (MA) term \(\theta \epsilon_{t-1}\). The creation of a moving average element, which is missing in the original series, is also why differencing a trend-stationary process is problematic.

\hypertarget{kpss-test}{%
\subparagraph{KPSS Test}\label{kpss-test}}

A test to verify if the series is \emph{trend stationary} is the \textbf{Kwiatkowski-Phillips-Schmidt-Shin (KPSS)} test. It is one of the most commonly used stationarity test, and is implemented in the library \emph{tseries} (function \emph{kpss.test}). KPSS test the \emph{null hypothesis} that the series is \emph{trend stationary}.

In this case, the \emph{p-value} of the test is higher than 0.05, so the test cannot reject the null hypothesis of trend stationarity. That is to say, there are some evidence of trend-stationary process.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("tseries") # install the library if not yet installed}
\KeywordTok{library}\NormalTok{(tseries)}
\KeywordTok{kpss.test}\NormalTok{(y_I0, }\DataTypeTok{null =} \StringTok{"Trend"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  KPSS Test for Trend Stationarity
## 
## data:  y_I0
## KPSS Trend = 0.10404, Truncation lag parameter = 5, p-value =
## 0.1
\end{verbatim}

By changing the null from ``Trend'' to ``Level'', the KPSS test can also test the \emph{null hypothesis} of \textbf{level stationarity}. A level stationary time series is a \emph{time series with a non-zero but constant mean}, that is to say, without trend.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kpss.test}\NormalTok{(y_I0, }\DataTypeTok{null =} \StringTok{"Level"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  KPSS Test for Level Stationarity
## 
## data:  y_I0
## KPSS Level = 8.4092, Truncation lag parameter = 5, p-value =
## 0.01
\end{verbatim}

In this case, the KPSS test for level stationarity reject the null hypothesis, that is to say, the process seems not to be level stationary. Considered together, the KPSS tests suggest that the series has a deterministic trend.

If we use the KPSS test to test if the \emph{stochastic trend} series we created above is trend or level stationary, the test \emph{rejects} the null hypothesis (i.e.: reject the hypothesis of both a trend and level stationary process).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kpss.test}\NormalTok{(y_I1, }\DataTypeTok{null =} \StringTok{"Trend"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  KPSS Test for Trend Stationarity
## 
## data:  y_I1
## KPSS Trend = 1.5, Truncation lag parameter = 5, p-value = 0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kpss.test}\NormalTok{(y_I1, }\DataTypeTok{null =} \StringTok{"Level"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  KPSS Test for Level Stationarity
## 
## data:  y_I1
## KPSS Level = 7.7377, Truncation lag parameter = 5, p-value =
## 0.01
\end{verbatim}

When a series has a stochastic trend, we can achieve stationarity through differencing. Indeed, the KPSS test does not reject the null hypothesis of level stationarity when applied to the the stochastic-trend series, once differenced.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kpss.test}\NormalTok{(}\KeywordTok{diff}\NormalTok{(y_I1), }\DataTypeTok{null =} \StringTok{"Level"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  KPSS Test for Level Stationarity
## 
## data:  diff(y_I1)
## KPSS Level = 0.18432, Truncation lag parameter = 5, p-value =
## 0.1
\end{verbatim}

In the above cases the KPSS results are correct, since we have simulated and tested a time series with a deterministic and stochastic trend. However, these kind of tests can also be wrong. For instance, it is possible they reject the null hypothesis when it is actually true (\href{https://en.wikipedia.org/wiki/Type_I_and_type_II_errors}{``Type I error''}). For this reason, it can be useful to use more than one test. For instance, the KPSS can be used along with the Augmented Dickey-Fuller Test (ADF), a popular \emph{unit root test}.

\hypertarget{augmented-dickey-fuller-adf-test}{%
\subparagraph{Augmented Dickey-Fuller (ADF) Test}\label{augmented-dickey-fuller-adf-test}}

The Augmented Dickey-Fuller Test (ADF) is a popular \emph{unit root test}. An R implementation of the test can be found in the library \emph{tseries} (function \emph{adf.test}). The null hypothesis is that the series has a unit root, and the alternative hypothesis is that the series is stationary or trend stationary.

If we use the ADF test on the integrated series (which has a unit root), the test fails to reject the null hypothesis of unit root, which is correct.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{adf.test}\NormalTok{(y_I1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Augmented Dickey-Fuller Test
## 
## data:  y_I1
## Dickey-Fuller = -1.7632, Lag order = 7, p-value = 0.6785
## alternative hypothesis: stationary
\end{verbatim}

If we use the ADF test on the trend-stationary series (without unit root), the test reject the null hypothesis of unit root, which is correct.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{adf.test}\NormalTok{(y_I0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Augmented Dickey-Fuller Test
## 
## data:  y_I0
## Dickey-Fuller = -5.8397, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary
\end{verbatim}

If we use the ADF test on the integrated series, after having transformed through differencing, the test reject the null hypothesis of unit root, which is correct.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{adf.test}\NormalTok{(}\KeywordTok{diff}\NormalTok{(y_I1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Augmented Dickey-Fuller Test
## 
## data:  diff(y_I1)
## Dickey-Fuller = -7.8913, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary
\end{verbatim}

\hypertarget{phillips-perron-test}{%
\subparagraph{Phillips-Perron Test}\label{phillips-perron-test}}

Another \emph{unit root test} is the \textbf{Phillips-Perron} test. It differs from the ADF test in some aspects (how it deals with serial correlation and heteroskedasticity in the errors). Also this test is implemented in the library \emph{tseries} (funtion \emph{pp.test}). Results of the test are similar to those of the ADF test:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# series with deterministic trend}
\KeywordTok{pp.test}\NormalTok{(y_I0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Phillips-Perron Unit Root Test
## 
## data:  y_I0
## Dickey-Fuller Z(alpha) = -144.04, Truncation lag parameter =
## 5, p-value = 0.01
## alternative hypothesis: stationary
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# series with unit roots}
\KeywordTok{pp.test}\NormalTok{(y_I1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Phillips-Perron Unit Root Test
## 
## data:  y_I1
## Dickey-Fuller Z(alpha) = -5.4817, Truncation lag parameter =
## 5, p-value = 0.8039
## alternative hypothesis: stationary
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# series with unit roots, differenced}
\KeywordTok{pp.test}\NormalTok{(}\KeywordTok{diff}\NormalTok{(y_I1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Phillips-Perron Unit Root Test
## 
## data:  diff(y_I1)
## Dickey-Fuller Z(alpha) = -489.82, Truncation lag parameter =
## 5, p-value = 0.01
## alternative hypothesis: stationary
\end{verbatim}

In case of uncertainty, more than one test can be used.

\hypertarget{non-autocorrelated-residuals}{%
\subsubsection{Non-autocorrelated residuals}\label{non-autocorrelated-residuals}}

We try to fit a linear regression model. First, we create two series \(x\) and \(y\), with \(x\) correlated with \(y\) at lags \(x_{t-3}\) and \(x_{t-4}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulated data of x series correlated to y at lag 3 and 4}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{999}\NormalTok{)}
\NormalTok{x_series <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{200}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{1}\NormalTok{))}
\NormalTok{z <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x_series, }\DecValTok{-3}\NormalTok{), stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x_series, }\DecValTok{-4}\NormalTok{)) }
\NormalTok{y_series <-}\StringTok{ }\DecValTok{15} \OperatorTok{+}\StringTok{ }\FloatTok{0.8}\OperatorTok{*}\NormalTok{z[,}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\FloatTok{1.5}\OperatorTok{*}\NormalTok{z[,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{196}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}

\NormalTok{xy_series <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(y_series, z)}
\end{Highlighting}
\end{Shaded}

The \emph{real} model (in this case we know it because we created it through the above simulation), is as follows:

\[
y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1)
\]
\#\#\#\# lm

To fit a linear regression, we can use the function \textbf{lm} (the standard funtion to perform linear regression analysis in base R, no additional packages are necessary).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(xy_series[,}\DecValTok{1}\NormalTok{] }\OperatorTok{~}\StringTok{ }\NormalTok{xy_series[,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{xy_series[,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

The function \textbf{summary} prints the summary of the model, which includes the estimates (the ``coefficients'' of the variables), the standard errors, the statistical significance of the variables, and other information.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(lm1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = xy_series[, 1] ~ xy_series[, 2] + xy_series[, 3])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.57051 -0.73392  0.06238  0.74187  2.14794 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    14.96869    0.07238  206.80   <2e-16 ***
## xy_series[, 2]  0.85549    0.07680   11.14   <2e-16 ***
## xy_series[, 3]  1.42126    0.07678   18.51   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.002 on 196 degrees of freedom
## Multiple R-squared:  0.873,  Adjusted R-squared:  0.8717 
## F-statistic: 673.5 on 2 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

We said that regression models sometimes work well enough with time series data, if specific conditions are met. Regards the conditions (or \textbf{assumptions}), in particular, the \textbf{residuals} of the models should have zero mean, they shouldn't show any significant autocorrelation, and they should be normally distributed.

To check whether these assumptions are met, we can visualize the \emph{plot of residuals, its ACF/PACF and histogram}, and also test the residuals for possible autocorrelation using a statistical test like the \href{https://en.wikipedia.org/wiki/BreuschGodfrey_test}{Breusch-Godfrey test} (this test is the default in the forecast library when a linear regression object \emph{lm} is tested).

To create the plots we can use the base R functions, or we can use the convenient \emph{checkresiduals} function in the \emph{forecast} package.

In this case everything seems fine.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.package("forecast") # install the package if necessary}
\KeywordTok{library}\NormalTok{(forecast)}
\KeywordTok{checkresiduals}\NormalTok{(lm1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-180-1.pdf}

\begin{verbatim}
## 
##  Breusch-Godfrey test for serial correlation of order up to 10
## 
## data:  Residuals
## LM test = 8.689, df = 10, p-value = 0.5618
\end{verbatim}

If we look at the model summary printed above, we can see that the estimated model is the following (the standard deviation of residuals is \href{https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sigma.html}{misnamed as ``residual standard error'' in the summary of \emph{lm}}):

\[
\begin{equation} 
y_t = 14.96869 + 0.85549x_{t-3} + 1.42126x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1.002^2)
\end{equation} 
\]
The estimated model is also close to the ``true'' model:

\[
y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1)
\]
\#\#\#\# dynml

Instead of \emph{lm}, the package \textbf{dynml} and the function with the same name (\emph{dynml}) can be used to fit a dynamic regression models in R. One of the main advantages of this package is that it allows users to fit time series linear regression models without calculating the lagged values by hand. To add a lagged variable, it can simply be used the \emph{L} (\emph{Lag}) function. The \emph{L} function takes as arguments the name of the variable and the lag length. For instance \emph{L(x, 4)} corresponds to \(x_{t-4}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("dynml") # install the package if necessary}
\KeywordTok{library}\NormalTok{(dynlm)}

\NormalTok{dynlm.fit <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(y_series }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(x_series, }\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(x_series, }\DecValTok{4}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(dynlm.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Time series regression with "ts" data:
## Start = 5, End = 203
## 
## Call:
## dynlm(formula = y_series ~ L(x_series, 3) + L(x_series, 4))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.57051 -0.73392  0.06238  0.74187  2.14794 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    14.96869    0.07238  206.80   <2e-16 ***
## L(x_series, 3)  0.85549    0.07680   11.14   <2e-16 ***
## L(x_series, 4)  1.42126    0.07678   18.51   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.002 on 196 degrees of freedom
## Multiple R-squared:  0.873,  Adjusted R-squared:  0.8717 
## F-statistic: 673.5 on 2 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

The \emph{dynlm} function also permits to include trend (function \emph{trend}) and seasonal (function \emph{season}) components in the model (it is also possible to change the reference value for the seasonal period, see \emph{?dynlm}). Just to make an example of the code to perform a dynamic regression with \emph{dynlm}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\KeywordTok{data}\NormalTok{(}\StringTok{"AirPassengers"}\NormalTok{)}
\NormalTok{ap <-}\StringTok{ }\KeywordTok{log}\NormalTok{(AirPassengers)}
\NormalTok{ap_x <-}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(ap, }\DecValTok{-3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(ap), }\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}

\NormalTok{ap_fm <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ap }\OperatorTok{~}\StringTok{ }\KeywordTok{trend}\NormalTok{(ap) }\OperatorTok{+}\StringTok{ }\KeywordTok{season}\NormalTok{(ap) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(ap_x, }\DecValTok{3}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(ap_fm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Time series regression with "ts" data:
## Start = 1949(7), End = 1960(12)
## 
## Call:
## dynlm(formula = ap ~ trend(ap) + season(ap) + L(ap_x, 3))
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.156306 -0.035098  0.007821  0.041535  0.141713 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    4.116392   0.224848  18.307  < 2e-16 ***
## trend(ap)      0.105758   0.005596  18.899  < 2e-16 ***
## season(ap)Feb -0.026936   0.024867  -1.083 0.280811    
## season(ap)Mar  0.127650   0.026209   4.870 3.32e-06 ***
## season(ap)Apr  0.111029   0.028324   3.920 0.000146 ***
## season(ap)May  0.132132   0.031750   4.162 5.86e-05 ***
## season(ap)Jun  0.248250   0.030001   8.275 1.71e-13 ***
## season(ap)Jul  0.334578   0.027595  12.125  < 2e-16 ***
## season(ap)Aug  0.333578   0.029138  11.448  < 2e-16 ***
## season(ap)Sep  0.172679   0.026344   6.555 1.35e-09 ***
## season(ap)Oct  0.034292   0.026311   1.303 0.194866    
## season(ap)Nov -0.101556   0.027526  -3.689 0.000335 ***
## season(ap)Dec -0.010445   0.024767  -0.422 0.673966    
## L(ap_x, 3)     0.061495   0.022439   2.741 0.007039 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.05831 on 124 degrees of freedom
## Multiple R-squared:  0.9829, Adjusted R-squared:  0.9811 
## F-statistic: 546.9 on 13 and 124 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{regression-with-arma-errors}{%
\subsubsection{Regression with ARMA errors}\label{regression-with-arma-errors}}

While in the previous case a standard linear model works well, it is often the case that \emph{residuals of times series regressions are autocorrelated}, and a linear regression model can be suboptimal or even wrong. For instance, let's create other two time series that are, as the previous ones, cross-correlated at lag 3 and 4, but with a bit more complicated structure.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# another set of simulated data }
\CommentTok{# the x series is correlated at lag 3 and 4}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{999}\NormalTok{)}
\NormalTok{x2_series <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{200}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{1}\NormalTok{))}
\NormalTok{z2 <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(x2_series, stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x2_series, }\DecValTok{-3}\NormalTok{), stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x2_series, }\DecValTok{-4}\NormalTok{)) }
\NormalTok{y2_series <-}\StringTok{ }\DecValTok{15} \OperatorTok{+}\StringTok{ }\FloatTok{0.8}\OperatorTok{*}\NormalTok{z2[,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\FloatTok{1.5}\OperatorTok{*}\NormalTok{z2[,}\DecValTok{3}\NormalTok{] }
\NormalTok{y2_errors <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{196}\NormalTok{, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.6}\NormalTok{, }\DataTypeTok{ma =} \FloatTok{0.6}\NormalTok{), }\DataTypeTok{sd=}\DecValTok{1}\NormalTok{)}
\NormalTok{y2_series <-}\StringTok{ }\NormalTok{y2_series }\OperatorTok{+}\StringTok{ }\NormalTok{y2_errors}

\CommentTok{# check the cross-correlations at lag 3 and 4}
\KeywordTok{library}\NormalTok{(TSA)}
\NormalTok{prew <-}\StringTok{ }\KeywordTok{prewhiten}\NormalTok{(x2_series, y2_series) }
\KeywordTok{prewhiten}\NormalTok{(x2_series, y2_series)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-183-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prew}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $ccf
## 
## Autocorrelations of series 'X', by lag
## 
##    -19    -18    -17    -16    -15    -14    -13    -12    -11    -10 
## -0.021 -0.029  0.012  0.065  0.024  0.008  0.076  0.107  0.001 -0.031 
##     -9     -8     -7     -6     -5     -4     -3     -2     -1      0 
## -0.024 -0.039  0.048  0.070 -0.021  0.620  0.425  0.036 -0.001  0.052 
##      1      2      3      4      5      6      7      8      9     10 
##  0.080  0.068 -0.008 -0.011  0.096  0.124  0.062 -0.022  0.030  0.023 
##     11     12     13     14     15     16     17     18     19 
## -0.035 -0.039 -0.008  0.016 -0.059 -0.149 -0.120 -0.027 -0.020 
## 
## $model
## 
## Call:
## ar.ols(x = x)
## 
## Coefficients:
##       1        2        3        4        5        6        7  
##  0.6362   0.1109   0.0496  -0.1533  -0.0240  -0.0499   0.2041  
##       8        9       10       11       12       13       14  
##  0.0221  -0.0990  -0.0981   0.1703  -0.0558  -0.0812   0.1179  
## 
## Intercept: 0.006091 (0.06307) 
## 
## Order selected 14  sigma^2 estimated as  0.7336
\end{verbatim}

Considering the autocorrelated structure of the series, the true model can be written as follows:

\[
\begin{aligned} 
& y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \eta_t \\
& \eta_t = 0.7\eta_{t-1} + \epsilon_t + 0.6\epsilon_{t-1} \\
& \epsilon \sim N(0, 1)
\end{aligned} 
\]
It is possible to calculate the regression using the \emph{lm} function, calculating the lagged variables by hand, or to use the \emph{dynml} library and function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate the lagged variables by hand and apply the lm function...}
\NormalTok{x2Lagged <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}
    \DataTypeTok{xLag0 =}\NormalTok{ x2_series,}
    \DataTypeTok{xLag3 =}\NormalTok{ stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x2_series,}\OperatorTok{-}\DecValTok{3}\NormalTok{),}
    \DataTypeTok{xLag4 =}\NormalTok{ stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x2_series,}\OperatorTok{-}\DecValTok{4}\NormalTok{))}

\NormalTok{xy2_series <-}\StringTok{ }\KeywordTok{ts.union}\NormalTok{(y2_series, x2Lagged)}

\NormalTok{lm2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(xy2_series[,}\DecValTok{1}\NormalTok{] }\OperatorTok{~}\StringTok{ }\NormalTok{xy2_series[,}\DecValTok{3}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\KeywordTok{summary}\NormalTok{(lm2) }\CommentTok{# AIC: 821.45}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = xy2_series[, 1] ~ xy2_series[, 3:4])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.9200 -1.4508  0.0667  1.5395  5.1083 
## 
## Coefficients:
##                                 Estimate Std. Error t value Pr(>|t|)
## (Intercept)                      14.9005     0.1486 100.273  < 2e-16
## xy2_series[, 3:4]x2Lagged.xLag3   1.0407     0.1595   6.523 6.14e-10
## xy2_series[, 3:4]x2Lagged.xLag4   1.5171     0.1599   9.488  < 2e-16
##                                    
## (Intercept)                     ***
## xy2_series[, 3:4]x2Lagged.xLag3 ***
## xy2_series[, 3:4]x2Lagged.xLag4 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.028 on 189 degrees of freedom
##   (12 observations deleted due to missingness)
## Multiple R-squared:  0.6735, Adjusted R-squared:   0.67 
## F-statistic: 194.9 on 2 and 189 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ... or use the dynml function}
\NormalTok{dynlm.fit2 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(y2_series }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(x2_series, }\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(x2_series, }\DecValTok{4}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(dynlm.fit2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Time series regression with "ts" data:
## Start = 5, End = 196
## 
## Call:
## dynlm(formula = y2_series ~ L(x2_series, 3) + L(x2_series, 4))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.9200 -1.4508  0.0667  1.5395  5.1083 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(>|t|)    
## (Intercept)      14.9005     0.1486 100.273  < 2e-16 ***
## L(x2_series, 3)   1.0407     0.1595   6.523 6.14e-10 ***
## L(x2_series, 4)   1.5171     0.1599   9.488  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.028 on 189 degrees of freedom
## Multiple R-squared:  0.6735, Adjusted R-squared:   0.67 
## F-statistic: 194.9 on 2 and 189 DF,  p-value: < 2.2e-16
\end{verbatim}

The estimated model is the following:

\[
\begin{aligned} 
& y_t = 14.9005 + 1.0407x_{t-3} + 1.5171x_{t-4} + \epsilon_t \\
& \epsilon \sim N(0, 2.028^2)
\end{aligned}
\]
The original series can also be visualized with the fitted values (the values resulting from the model), to visually inspect how well the model represents the original series. The differences between the original and the fitted series are the \emph{residuals}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm2d <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(xy2_series[,}\DecValTok{1}\NormalTok{]), lm2}\OperatorTok{$}\NormalTok{fitted.values)}

\KeywordTok{plot.ts}\NormalTok{(lm2d, }\DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{, }\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"orange"}\NormalTok{,}\StringTok{"blue"}\NormalTok{), }
        \DataTypeTok{lty=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{), }\DataTypeTok{lwd=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
        \DataTypeTok{main =} \StringTok{"'Classic' Linear Model - Original (orange) and Fitted series (blue)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-185-1.pdf}

The diagnostic plots of the residuals show the presence of autocorrelation, and the Breusch-Godfrey test is highly significant (its value is far lower than the critical value \(\alpha = 0.05\))

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{checkresiduals}\NormalTok{(lm2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-186-1.pdf}

\begin{verbatim}
## 
##  Breusch-Godfrey test for serial correlation of order up to 10
## 
## data:  Residuals
## LM test = 149.45, df = 10, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pacf}\NormalTok{(lm2}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-186-2.pdf}

In this case, it's better to take into account the residuals' autocorrelation by using a regression model capable to handle autocorrelated time series structures.

In the previous chapter we said that ARIMA models are a special type of regression model, in which the dependent variable is the time series itself, and the independent variables are all lags of the time series. This model is capable to take into account the \emph{autocorrelated} structure of time series.

ARIMA is a modeling technique that can be applied to a single time series, but it can be extended to include additional, \textbf{exogenous variables}. The ARIMA model including exogenous regressors (i.e.: other time series besides the lagged dependent variable) is like a multiple regression models for time series. In particular, it can be considered a regression model capable to control for autocorrelation in residuals.

It is possible to use more than one option to fit an ARIMA model with external regressors. A convenient option is provided by the function \textbf{auto.arima}, in the package \emph{forecast}. This library has an argument \textbf{xreg} which can be use with \emph{a numerical vector or matrix of external regressors, which must have the same number of rows as y} (see ?auto.arima).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{999}\NormalTok{)}
\NormalTok{arima1 <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(xy2_series[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{xreg =}\NormalTok{ xy2_series[,}\DecValTok{3}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\NormalTok{arima1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: xy2_series[, 1] 
## Regression with ARIMA(1,0,1) errors 
## 
## Coefficients:
##          ar1     ma1  intercept  x2Lagged.xLag3  x2Lagged.xLag4
##       0.6863  0.6491    14.8532          0.9506          1.5732
## s.e.  0.0555  0.0528     0.3607          0.0757          0.0762
## 
## sigma^2 estimated as 0.9482:  log likelihood=-265.76
## AIC=543.52   AICc=543.97   BIC=563.06
\end{verbatim}

The resulting model seems to be more appropriate than the previous one, fitted by using just a ``classic'' linear regression. This is clear also by comparing the two models through the \href{https://en.wikipedia.org/wiki/Akaike_information_criterion}{\textbf{AIC criterion (Akaike information criterion)}}. The AIC value is used to compare the \emph{goodness-of-fit} of different models fitted to the same dataset. The lower the AIC value, the better the fit (see also the next paragraph).

The auto.arima function prints the AIC value by default, while this value is not given with the \emph{lm} function. To get it, we need to use the \textbf{AIC} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{AIC}\NormalTok{(lm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 821.4495
\end{verbatim}

In this case, the ARIMA regression model results a far better model (\emph{AIC=543.52}) compared with the classic linear model (\emph{AIC=821.45}).

\[
\begin{aligned} 
& y_t = 14.8532 + 0.9506x_{t-3} + 1.5732x_{t-4} + \eta_t \\
& \eta_t = 0.6863\eta_{t-1} + \epsilon_t + 0.6491\epsilon_{t-1} \\
& \epsilon \sim N(0, 0.9482)
\end{aligned} 
\]
Diagnostic analysis of the residuals, shows that there is no concerning sign of autocorrelation in the residuals, which looks like white noise. Also the test for autocorrelated errors is not significant (the default test for autocorrelation when testing an ARIMA models with external regressors in the \emph{forecast} package is the \textbf{Ljung-Box test})\footnote{There are many tests for detecting autocorrelation. Besides the already mentioned \emph{Breusch-Godfrey test} and \emph{Ljung-Box test}, other popular tests are the \emph{Durbin Watson test}, and the \emph{Box--Pierce test}. Each test has its own characteristics. For instance, the Durbin-Watson test is a popular way to test for autocorrelation, but it \href{https://www.jstor.org/stable/pdf/1909870.pdf?refreqid=excelsior\%3A9526730d9debe4fa8f1a4d5fa601d523}{shouldn't be used with lagged dependent variables}. In this case it can be used the Breusch-Godfrey test}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{checkresiduals}\NormalTok{(arima1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-189-1.pdf}

\begin{verbatim}
## 
##  Ljung-Box test
## 
## data:  Residuals from Regression with ARIMA(1,0,1) errors
## Q* = 6.3861, df = 5, p-value = 0.2704
## 
## Model df: 5.   Total lags used: 10
\end{verbatim}

Also by visually inspect the original series along with the fitted series (the values resulting from the model), it can be seen that the model is better than the previous one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arima1d <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(xy2_series[,}\DecValTok{1}\NormalTok{]), arima1}\OperatorTok{$}\NormalTok{fitted)}
\KeywordTok{plot.ts}\NormalTok{(arima1d, }\DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{, }\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"orange"}\NormalTok{,}\StringTok{"blue"}\NormalTok{), }
        \DataTypeTok{lty=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{), }\DataTypeTok{lwd=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
        \DataTypeTok{main =} \StringTok{"ARIMA errors model - Original (orange) and Fitted series (blue)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-190-1.pdf}

We can also compare the fitted versus original values by using a scatterplot. A better model produces a thinner diagonal line.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(xy2_series[,}\DecValTok{1}\NormalTok{]), lm2}\OperatorTok{$}\NormalTok{fitted.values, }\DataTypeTok{main =} \StringTok{"LM"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Original"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Fitted"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(xy2_series[,}\DecValTok{1}\NormalTok{]), arima1}\OperatorTok{$}\NormalTok{fitted, }\DataTypeTok{main =} \StringTok{"ARIMA regression model"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Original"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Fitted"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-191-1.pdf}
The \emph{auto.arima} function does not give the statistical significance of the coefficients (the approach adopted by the \emph{forecast} library is different, based on the choice of the best model to do forecasting), but it is possible to get that by using the function \emph{coeftest} in the library \emph{lmtest}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages(lmtest) # installa the package}
\KeywordTok{library}\NormalTok{(lmtest)}
\KeywordTok{coeftest}\NormalTok{(arima1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## z test of coefficients:
## 
##                 Estimate Std. Error z value  Pr(>|z|)    
## ar1             0.686330   0.055525  12.361 < 2.2e-16 ***
## ma1             0.649134   0.052850  12.283 < 2.2e-16 ***
## intercept      14.853152   0.360689  41.180 < 2.2e-16 ***
## x2Lagged.xLag3  0.950588   0.075705  12.557 < 2.2e-16 ***
## x2Lagged.xLag4  1.573242   0.076224  20.640 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\hypertarget{count-regression-models}{%
\subsubsection{Count regression models}\label{count-regression-models}}

The models described above are mostly used with \textbf{continuous} variables (expressed as \emph{numeric} or \emph{double} in the R data format). However, it is often the case that time series are composed of integer values, or \textbf{count data} (expressed as \emph{integer}).

Sometimes, the above mentioned methods work well also with this type of data (for instance, when the counts are large). Other times, time series model developed for count data can be a better choice (for instance, when the series include mostly small integer values).

Two of the most common statistical models to deal with count data are based on the \href{https://en.wikipedia.org/wiki/Poisson_distribution}{\textbf{Poisson}} and the \href{https://en.wikipedia.org/wiki/Negative_binomial_distribution}{\textbf{Negative Binomial}} distributions. These probability distributions are the ones that are usually employed to model count data.

There are a few libraries to fit count time series regression models in R. We take into consideration \textbf{tscount}, and its function \emph{tsglm}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("tscount")}
\KeywordTok{library}\NormalTok{(tscount)}
\end{Highlighting}
\end{Shaded}

We consider, as an example of the \emph{tscount} function to fit count time series regression models, the dataset ``Seatbelts'' (monthly number of killed drivers of light goods vehicles in Great Britain between January 1969 and December 1984), following the \href{https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf}{paper} that describes the \emph{tscount} library.

The authors of the package write in the \href{https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf}{paper (par. 7.2)} describing the library:

\begin{quote}
This time series is part of a dataset which was first considered by Harvey and Durbin (1986) for studying the effect of compulsory wearing of seatbelts introduced on 31 January 1983. The dataset, including additional covariates, is available in R in the object Seatbelts. In their paper Harvey and Durbin (1986) analyze the numbers of casualties for drivers and passengers of cars, which are \textbf{so large} that they can be treated with \textbf{methods for continuous-valued data}. The monthly number of killed drivers of vans analyzed here is \textbf{much smaller} (its \emph{minimum is 2 and its maximum 17}) and \textbf{therefore methods for count data are to be preferred}.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"Seatbelts"}\NormalTok{)}

\NormalTok{timeseries <-}\StringTok{ }\NormalTok{Seatbelts[, }\StringTok{"VanKilled"}\NormalTok{]}

\NormalTok{regressors <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{PetrolPrice =}\NormalTok{ Seatbelts[, }\KeywordTok{c}\NormalTok{(}\StringTok{"PetrolPrice"}\NormalTok{)], }
                    \DataTypeTok{linearTrend =} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{along =}\NormalTok{ timeseries))}

\NormalTok{timeseries_until1981 <-}\StringTok{ }\KeywordTok{window}\NormalTok{(timeseries, }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{1981}\NormalTok{, }\DecValTok{12}\NormalTok{))}
\NormalTok{regressors_until1981 <-}\StringTok{ }\KeywordTok{window}\NormalTok{(regressors, }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{1981}\NormalTok{, }\DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We are going to fit a model aimed at capturing a first order autoregressive \emph{AR(1)} term and a yearly \emph{seasonality} by a 12th order autoregressive term.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot.ts}\NormalTok{(timeseries, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(timeseries, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{acf}\NormalTok{(timeseries, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{pacf}\NormalTok{(timeseries, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-195-1.pdf}
The function \emph{tsglm} allows users to declare the autoregressive and seasonal autoregressive terms in a convenient way (in the following part of the function: \emph{model = list(past\_obs = c(1, 12))}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poisson_fit <-}\StringTok{ }\KeywordTok{tsglm}\NormalTok{(timeseries_until1981,}
                     \DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{past_obs =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{12}\NormalTok{)), }
                     \DataTypeTok{xreg =}\NormalTok{ regressors_until1981,}
                     \DataTypeTok{distr =} \StringTok{"poisson"}\NormalTok{, }\DataTypeTok{link =} \StringTok{"log"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It is possible to check the residuals with the usual plots.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\KeywordTok{plot}\NormalTok{(poisson_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(poisson_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{acf}\NormalTok{(poisson_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{pacf}\NormalTok{(poisson_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-197-1.pdf}
To function \emph{summary} can be used to get the parameter estimates for the model (in this case the function can also emply a parametric bootstrap procedure (\emph{B}) to obtain standard errors and confidence intervals of the regression parameters. The authors use \emph{B=500} in the original paper, since in their experience this value yields stable results. Higher B values can be more precise but require time to be calculated).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(poisson_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## tsglm(ts = timeseries_until1981, model = list(past_obs = c(1, 
##     12)), xreg = regressors_until1981, link = "log", distr = "poisson")
## 
## Coefficients:
##              Estimate  Std.Error  CI(lower)  CI(upper)
## (Intercept)   1.83284   0.367830    1.11191    2.55377
## beta_1        0.08672   0.080913   -0.07187    0.24530
## beta_12       0.15288   0.084479   -0.01269    0.31846
## PetrolPrice   0.83069   2.303555   -3.68420    5.34557
## linearTrend  -0.00255   0.000653   -0.00383   -0.00127
## Standard errors and confidence intervals (level =  95 %) obtained
## by normal approximation.
## 
## Link function: log 
## Distribution family: poisson 
## Number of coefficients: 5 
## Log-likelihood: -396.1765 
## AIC: 802.3529 
## BIC: 817.6022 
## QIC: 802.3529
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# summary(poisson_fit, B=500) # to use the bootstrap procedure}
\end{Highlighting}
\end{Shaded}

The model is as follows:

\[
log(\lambda_t) = 1.83 + 0.09Y_{t-1} + 0.15Y_{t-12} + 0.83X_t - 0.003t
\]

In the above equation notice that, the Poisson regression, models the logarithm of the \emph{Y} values at times \emph{t} (expressed as \(log(\lambda_t)\)).

Another example (using the dataset you can download \href{https://drive.google.com/file/d/1eIOERBLCUCaap3WCoM5QT6I0iTW-Hqwa/view?usp=sharing}{here}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{gtrend_fakenews_qanon <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"data/gtrend_fakenews_qanon.csv"}\NormalTok{,}
                                  \DataTypeTok{col_types =} \KeywordTok{cols}\NormalTok{(}\DataTypeTok{date =} \KeywordTok{col_date}\NormalTok{(}\DataTypeTok{format =} \StringTok{"%Y-%m"}\NormalTok{),}
                                                   \DataTypeTok{fake_news =} \KeywordTok{col_integer}\NormalTok{(), }
                                                   \DataTypeTok{qanon =} \KeywordTok{col_integer}\NormalTok{()))}

\CommentTok{# head(gtrend_fakenews_qanon$date,1) # 2015-01-01}
\CommentTok{# tail(gtrend_fakenews_qanon$date,1) # 2020-12-01}

\NormalTok{fake_news <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(gtrend_fakenews_qanon}\OperatorTok{$}\NormalTok{fake_news, }
                \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{2015}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2020}\NormalTok{,}\DecValTok{12}\NormalTok{), }\DataTypeTok{frequency =} \DecValTok{12}\NormalTok{)}

\NormalTok{qanon <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(gtrend_fakenews_qanon}\OperatorTok{$}\NormalTok{qanon, }
            \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{2015}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2020}\NormalTok{,}\DecValTok{12}\NormalTok{), }\DataTypeTok{frequency =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{layout}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{), }\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{, }\DataTypeTok{byrow=}\NormalTok{T))}
\KeywordTok{plot.ts}\NormalTok{(qanon, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(qanon, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{acf}\NormalTok{(qanon, }\DecValTok{48}\NormalTok{, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{pacf}\NormalTok{(qanon, }\DecValTok{48}\NormalTok{, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\NormalTok{qanon_fake_ccf <-}\StringTok{ }\KeywordTok{prewhiten}\NormalTok{(fake_news, qanon, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-200-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qanon_fake_ccf}\OperatorTok{$}\NormalTok{ccf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Autocorrelations of series 'X', by lag
## 
## -1.2500 -1.1667 -1.0833 -1.0000 -0.9167 -0.8333 -0.7500 -0.6667 
##  -0.127   0.064  -0.013   0.008   0.033   0.022  -0.041  -0.278 
## -0.5833 -0.5000 -0.4167 -0.3333 -0.2500 -0.1667 -0.0833  0.0000 
##   0.120  -0.313   0.417   0.282  -0.061  -0.025  -0.075  -0.012 
##  0.0833  0.1667  0.2500  0.3333  0.4167  0.5000  0.5833  0.6667 
##  -0.180   0.063   0.039  -0.154  -0.070  -0.040   0.029   0.068 
##  0.7500  0.8333  0.9167  1.0000  1.0833  1.1667  1.2500 
##  -0.008  -0.045  -0.221  -0.068  -0.039   0.112   0.177
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{fake_news_lag4 =}\NormalTok{ stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(fake_news, }\DecValTok{-4}\NormalTok{),}
             \DataTypeTok{fake_news_lag5 =}\NormalTok{ stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(fake_news, }\DecValTok{-5}\NormalTok{),}
             \DataTypeTok{fake_news_lag6 =}\NormalTok{ stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(fake_news, }\DecValTok{-6}\NormalTok{))}

\CommentTok{# NA values derives from the application of the "lag" function}
\CommentTok{# and have to be removed, since the regression function cannot}
\CommentTok{# work properly with them}
\NormalTok{reg <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(reg)}

\CommentTok{# start(reg) # 2015, 6}
\CommentTok{# end(reg) # 2021, 4}

\NormalTok{reg <-}\StringTok{  }\KeywordTok{window}\NormalTok{(reg, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{2015}\NormalTok{, }\DecValTok{7}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2020}\NormalTok{, }\DecValTok{12}\NormalTok{))}
\NormalTok{qanon <-}\StringTok{ }\KeywordTok{window}\NormalTok{(qanon, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{2015}\NormalTok{, }\DecValTok{7}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2020}\NormalTok{, }\DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poisson_gtrend_fit <-}\StringTok{ }\KeywordTok{tsglm}\NormalTok{(qanon,}
                            \DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{past_obs =} \DecValTok{1}\NormalTok{), }
                            \DataTypeTok{xreg =}\NormalTok{ reg,}
                            \DataTypeTok{distr =} \StringTok{"poisson"}\NormalTok{, }\DataTypeTok{link =} \StringTok{"log"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{layout}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{), }\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DataTypeTok{byrow=}\NormalTok{T))}

\KeywordTok{plot}\NormalTok{(poisson_gtrend_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(poisson_gtrend_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{acf}\NormalTok{(poisson_gtrend_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}
\KeywordTok{pacf}\NormalTok{(poisson_gtrend_fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.vector}\NormalTok{(poisson_gtrend_fit}\OperatorTok{$}\NormalTok{response), }
     \KeywordTok{as.vector}\NormalTok{(poisson_gtrend_fit}\OperatorTok{$}\NormalTok{fitted.values),}
     \DataTypeTok{xlab=}\StringTok{"response"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"fitted"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-203-1.pdf}
Besides checking the residuals, it is possible to plot the \textbf{PIT histogram}, provided by the function \textbf{pit} in \emph{tscount}:
\textgreater A PIT histogram is a tool for evaluating the statistical consistency between the probabilistic forecast and the observation. The predictive distributions of the observations are compared with the actual observations. If the predictive distribution is ideal the result should be a flat PIT histogram with no bin having an extraordinary high or low level. For more information about PIT histograms see the references listed below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pit}\NormalTok{(poisson_gtrend_fit, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.5}\NormalTok{), }\DataTypeTok{main =} \StringTok{"PIT Poisson"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-204-1.pdf}
In the library are included other diagnostic tools and metrics that can help choosing between poisson and negative binomial models (see the \href{https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf}{paper} for further information).

The function \emph{summary} prints the coefficients of the model and their confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(poisson_gtrend_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## tsglm(ts = qanon, model = list(past_obs = 1), xreg = reg, link = "log", 
##     distr = "poisson")
## 
## Coefficients:
##                 Estimate  Std.Error  CI(lower)  CI(upper)
## (Intercept)      0.58373    0.18036   0.230232     0.9372
## beta_1           0.83746    0.04834   0.742715     0.9322
## fake_news_lag4   0.00892    0.00274   0.003546     0.0143
## fake_news_lag5   0.00638    0.00354  -0.000554     0.0133
## fake_news_lag6  -0.01617    0.00272  -0.021506    -0.0108
## Standard errors and confidence intervals (level =  95 %) obtained
## by normal approximation.
## 
## Link function: log 
## Distribution family: poisson 
## Number of coefficients: 5 
## Log-likelihood: -239.6582 
## AIC: 489.3163 
## BIC: 500.2646 
## QIC: 489.3163
\end{verbatim}

\hypertarget{model-selection-aic-aicc-bic}{%
\subsection{Model Selection (AIC, AICc, BIC)}\label{model-selection-aic-aicc-bic}}

Statistical modeling is, usually, a recursive process that requires to fit several different models and, eventually, to select the most appropriate one. For instance, the Box and Jenkins approach employed to find an appropriate ARIMA model for a time series (see the previous chapter), requires the fitting of multiple models to find the most suitable one based on the data. Similarly, the ``auto.arima'' function in the library \emph{forecast}, that automatizes the search for an appropriate ARIMA model, conducts a search over possible model.

To compare the models and select the most appropriate one, it is necessary to use some criteria. In the example above we have employed the AIC criterion. Other similar criteria are the AICc, and the BIC. They all can be used to find the most appropriate model, by comparing the \emph{goodness-of-fit} of different models fitted to the same dataset. For instance, the documentation of the ``auto.arima'' function says that the function \emph{``returns best ARIMA model according to either AIC, AICc or BIC value''}.

The \textbf{AIC} criterion is the acronym for \href{https://en.wikipedia.org/wiki/Akaike_information_criterion}{\emph{Akaike information criterion)}}. The lower the AIC value, the better the fit (see also the next paragraph).

The \textbf{AICc} criterion, is the same, but with a \emph{correction for small sample size}. When the sample is small it can be used in place of the AIC criterion. As the sample size increases, the AICc converges to the AIC.

The \textbf{BIC} criterion is the \emph{Bayesian Information Criterion (or Schwartz's Bayesian Criterion)} and has a stronger penalty than the AIC for overparametrized models (more complex models, with several predictors).

These criteria can also be used when searching for an appropriate regression model, to compare several different models including different lags of the variables.

When comparing models by using these criteria, it is important that the models are fitted to \textbf{the same dataset}, otherwise the results are not comparable. This is an important aspect to take into account when using lagged predictors. For instance, you may want to try a model including one lagged predictor \(x_{t-1}\) and a model including two lagged predictors \(x_{t-1}\) and \(x_{t-2}\), and to compare them in order to select the best one according to AIC, AICc or the BIC criterion. However, when you add lagged predictor you loose data points.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x_example <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{40}\NormalTok{))}
\NormalTok{y_example <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{40}\NormalTok{))}

\NormalTok{example_data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ y_example,}
                      \DataTypeTok{xLag0 =}\NormalTok{ x_example,}
                      \DataTypeTok{xLag1 =}\NormalTok{ stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x_example, }\DecValTok{-1}\NormalTok{),}
                      \DataTypeTok{xLag2 =}\NormalTok{ stats}\OperatorTok{::}\KeywordTok{lag}\NormalTok{(x_example, }\DecValTok{-2}\NormalTok{))}

\NormalTok{example_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time Series:
## Start = 1 
## End = 42 
## Frequency = 1 
##               y       xLag0       xLag1       xLag2
##  1 -0.485636726 -0.28174016          NA          NA
##  2  0.008498139 -1.31255963 -0.28174016          NA
##  3 -1.282113287  0.79518398 -1.31255963 -0.28174016
##  4 -1.111578841  0.27007049  0.79518398 -1.31255963
##  5  0.300665411 -0.27730642  0.27007049  0.79518398
##  6  0.276478845 -0.56602374 -0.27730642  0.27007049
##  7 -2.050877659 -1.87865826 -0.56602374 -0.27730642
##  8  0.014190211 -1.26679114 -1.87865826 -0.56602374
##  9  0.582266478 -0.96774968 -1.26679114 -1.87865826
## 10 -0.034726387 -1.12100936 -0.96774968 -1.26679114
## 11 -0.116664147  1.32546371 -1.12100936 -0.96774968
## 12 -0.644982088  0.13397739  1.32546371 -1.12100936
## 13  1.744411600  0.93874945  0.13397739  1.32546371
## 14  0.366094474  0.17253810  0.93874945  0.13397739
## 15 -0.066809926  0.95765045  0.17253810  0.93874945
## 16  0.282612466 -1.36268625  0.95765045  0.17253810
## 17  0.567695175  0.06833513 -1.36268625  0.95765045
## 18 -1.279215897  0.10065765  0.06833513 -1.36268625
## 19  0.435368808  0.90134475  0.10065765  0.06833513
## 20 -0.565500982 -2.07435711  0.90134475  0.10065765
## 21 -0.923311356 -1.22856330 -2.07435711  0.90134475
## 22  1.164954003  0.64304432 -1.22856330 -2.07435711
## 23  1.042068682 -0.35976291  0.64304432 -1.22856330
## 24  1.103828363  0.29403559 -0.35976291  0.64304432
## 25 -0.018574963 -1.12526849  0.29403559 -0.35976291
## 26 -1.146957669  0.64226565 -1.12526849  0.29403559
## 27 -1.408179520 -1.10673758  0.64226565 -1.12526849
## 28 -0.282328729 -0.88484039 -1.10673758  0.64226565
## 29 -0.417770016 -1.55409514 -0.88484039 -1.10673758
## 30  0.996663515 -0.12667899 -1.55409514 -0.88484039
## 31 -0.106285779  2.38266416 -0.12667899 -1.55409514
## 32 -0.069402153  0.60127610  2.38266416 -0.12667899
## 33  0.949701101  0.17936127  0.60127610  2.38266416
## 34 -0.416532807  1.08053148  0.17936127  0.60127610
## 35  0.974000406 -0.24681211  1.08053148  0.17936127
## 36  0.062291430 -2.11373699 -0.24681211  1.08053148
## 37  0.538422053 -0.37052747 -2.11373699 -0.24681211
## 38 -2.064823254  0.52286779 -0.37052747 -2.11373699
## 39  0.436431652  0.51780554  0.52286779 -0.37052747
## 40 -0.160226689 -1.40251087  0.51780554  0.52286779
## 41           NA          NA -1.40251087  0.51780554
## 42           NA          NA          NA -1.40251087
\end{verbatim}

Thus, when you fit models with different lags, you have to fit them on the same dataset. In this case, for instance, you have to skip the NA rows, and use just the rows from 3 to 40.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Restrict data so models use same fitting period}
\NormalTok{fit1 <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(example_data[}\DecValTok{3}\OperatorTok{:}\DecValTok{40}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{xreg=}\NormalTok{example_data[}\DecValTok{3}\OperatorTok{:}\DecValTok{40}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\NormalTok{fit2 <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(example_data[}\DecValTok{3}\OperatorTok{:}\DecValTok{40}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{xreg=}\NormalTok{example_data[}\DecValTok{3}\OperatorTok{:}\DecValTok{40}\NormalTok{,}\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{])}
\NormalTok{fit3 <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(example_data[}\DecValTok{3}\OperatorTok{:}\DecValTok{40}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{xreg=}\NormalTok{example_data[}\DecValTok{3}\OperatorTok{:}\DecValTok{40}\NormalTok{,}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

Then you can compare the model, for instance, using the AIC criterion, and choose the model with the smallest value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit1}\OperatorTok{$}\NormalTok{aic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 101.2893
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit2}\OperatorTok{$}\NormalTok{aic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 103.2719
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit3}\OperatorTok{$}\NormalTok{aic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 101.474
\end{verbatim}

Finally, you fit the model using all the available data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit1 <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(example_data[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{xreg=}\NormalTok{example_data[,}\DecValTok{2}\NormalTok{])}
\NormalTok{fit1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: example_data[, 1] 
## Regression with ARIMA(0,0,0) errors 
## 
## Coefficients:
##         xreg
##       0.1388
## s.e.  0.1292
## 
## sigma^2 estimated as 0.7442:  log likelihood=-50.34
## AIC=104.68   AICc=105.01   BIC=108.06
\end{verbatim}

Besides these criteria, there are also other strategies for \href{https://en.wikipedia.org/wiki/Model_selection}{model selection}.

\hypertarget{some-examples-in-the-literature}{%
\subsection{Some examples in the literature}\label{some-examples-in-the-literature}}

There are several examples of the use of time series regression models in the literature in the field of communication science.

For instance, in \href{https://ijoc.org/index.php/ijoc/article/viewFile/14843/3344}{The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates}\footnote{Wozniak, A., Wessler, H., Chan, C. H., \& Lck, J. (2021). The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates. \emph{International Journal of Communication}, 15(27)}, the authors \emph{examined whether the UN climate change conferences are conducive to an emergence of a transnational public sphere by triggering issue convergence and increased transnational interconnectedness across national media debates}. They authors detail the method they follows in this way:

\begin{quote}
{[}\ldots{]} Given the autoregressive nature and other properties of time series, an ordinary least squares regression analysis would violate the normality of error and the independence of observations assumption (Wells et al., 2019). Instead, \textbf{we applied the dynamic regression approach} (Gujarati \& Porter, 2009; Hyndman \& Athanasopoulos, 2018), which assumes that the \textbf{error term follows an autoregressive integrated moving average (ARIMA) model} (\ldots). we found the best ARIMA structure of the error term by using the \emph{auto.arima function from the forecast R package} (Hyndman \& Khandakar, 2008). It searches for an ARIMA structure that can explain the most variance according to the \emph{Akaike information criterion} (Akaike, 1973).
\end{quote}

In this case they use the term ``dynamic regression'' to refer to a time series regression with ARIMA errors, but they did not include lagged values of their variables, thus analyzing contemporary relationships between variables.

The found, for instance, that \emph{events taking place on a supranational level of governance (\ldots) consistently led to spikes in media attention across countries. In contrast, a bottom-up effort such as Fridays for Future showed an inconsistent relationship with media attention across the four countries.}

\includegraphics[width=20.99in]{images/Event-Centered Nature of Global Public Spheres}

In \href{https://ijoc.org/index.php/ijoc/article/viewFile/11666/2819}{Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event}\footnote{Lee, F. L., Liang, H., \& Tang, G. K. (2019). Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event. International Journal of Communication, 13, 20.}, the authors used both standard regression and regression with ARIMA errors to show that \emph{``online incivility --- operationalized as the use of foul language --- grew as volume of political discussions and levels of cyberbalkanization increased. Incivility led to higher levels of opinion polarization.''}. Also in this case the authors analyze a ``static process'', that is, focus on contemporary relationships between variables.

\includegraphics[width=23.74in]{images/Online-Incivility}

In \href{https://journals.sagepub.com/doi/abs/10.1177/1077699013493792}{Beyond cognitions: A longitudinal study of online search salience and media coverage of the president}\footnote{Ragas, M. W., \& Tran, H. (2013). Beyond cognitions: A longitudinal study of online search salience and media coverage of the president. Journalism \& Mass Communication Quarterly, 90(3), 478-499.}, the authors used regression models with ARIMA errors to examine \emph{shifts in newswire coverage and search interest among Internet users in President Obama during the first two years of his administration (2009-2010)}.

\includegraphics[width=15.28in]{images/Beyond-Cognitions}

In this case, the authors analyze relationships between variables taking into account lagged values, thus adopting a ``dynamic process'' perspective. For instance, they write:

\begin{quote}
RQ2 sought to determine the time span of linkages between coverage volume and search volume. (\ldots) \textbf{ARIMA} models were run to gauge the \emph{dynamics} of mutual influence between these two time series. The first model examined the effect of coverage volume on search volume over time (i.e., basic agenda setting) (\ldots) presidential public relations, was included as an additional input series. The first model, with search volume being a single dependent variable, was \textbf{identified} through a \textbf{close examination of autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs)}. This analysis revealed a classic \textbf{autoregressive model for the series (1, 0, 0)}. {[}\ldots{]} According to the results, \emph{shifts in aggregate search volume over this two-year period were significantly} \textbf{predicted by coverage volume over the prior five weeks} (p \textless{} .010)* and by presidential public relations efforts in the preceding two, three (p \textless{} .001), and five weeks (p \textless{} .005). The ARIMA model with two predictors was correctly specified (\textbf{Ljung--Box Q} = 18.132, p = .381) and it explained roughly 35\% of the observed variation in the series.
\end{quote}

In \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4126885/}{AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993--2007}\footnote{Stevens, R., \& Hornik, R. C. (2014). AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993--2007. Journal of health communication, 19(8), 893-906}, the authors \emph{examined the effect of newspaper coverage of HIV/AIDS on HIV testing behavior in a U.S. population.}, using a \emph{lagged regression} to support \emph{causal order claims by ensuring that newspaper coverage precedes the testing behavior with the inclusion of the 1-month lagged newspaper coverage variable in the model}. Counterintuitively, they found that the news media coverage had a negative effect on testing behavior: \emph{For every additional 100 HIV/AIDS risk related newspaper stories published in this group of U.S. newspapers each month, there was a 1.7\% decline in HIV testing levels in the following month}, with a higher negative effects on African Americans.

\includegraphics[width=14.43in]{images/AIDS in Black and White}

\hypertarget{intervention-analysis}{%
\section{Intervention Analysis}\label{intervention-analysis}}

In this chapter we are going to learn about \emph{intervention analysis} (sometimes also called \emph{interrupted time-series analysis}) and to see how to conduct a intervention analysis.

\textbf{Intervention analysis} is typically conducted with the Box \& Jenkins ARIMA framework and traditionally uses a method introduced by \href{https://www.jstor.org/stable/pdf/2285379.pdf}{Box and Tiao (1975)}\footnote{Box, G. E., \& Tiao, G. C. (1975). Intervention analysis with applications to economic and environmental problems. Journal of the American Statistical association, 70(349), 70-79}, who provided a framework for assessing \emph{the effect of an intervention on a time series} under study.

As summarized by Box and Tiao: \emph{Given a known intervention, is there evidence that change in the series of the kind expected actually occurred, and, if so, what can be said of the nature and magnitude of the change?}. In other words \emph{Intervention analysis estimates the effect of an external or exogenous intervention on a time-series}. To conduct such an analysis, it is necessary to know the date of the intervention.

Intervention analysis is a ``quasi-experimental'' design and an interesting approach to test whether \emph{exogenous shocks}, such as, for instance, the introduction of a new policy, \emph{impact on a time series process in a significant way}, that is, \textbf{by changing the mean function or trend} of a time series.

Behind intervention analysis there is the \emph{causal hypothesis} that observations \emph{after} a \textbf{treatment} (the \textbf{``intervention''}) have a different level or slope from those before the intervention/interruption.

Besides \emph{intervention} or \emph{interrupted time-series analysis}, the analysis can be conducted through the \emph{segmented regression} method. However, as in the case of traditional regression models applied to time series data, this approach does not take into account the autocorrelated structure of time series. Other methods include more complex computational approaches.

\hypertarget{types-of-intervention}{%
\subsection{Types of intervention}\label{types-of-intervention}}

There are different types of interventions. For instance, an intervention can have an abrupt impact determining a permanent or temporary change, a sudden and short-lived change due to an event, or a more gradual yet permanent change.

\includegraphics[width=19.28in]{images/intervention}

\hypertarget{intervention-analysis-with-arima}{%
\subsection{Intervention analysis with ARIMA}\label{intervention-analysis-with-arima}}

To exemplify an intervention analysis we are going to reproduce the example in the paper \href{https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01235-8\#Sec13}{Interrupted time series analysis using autoregressive integrated moving average (ARIMA) models: a guide for evaluating large-scale health interventions}.

The data to run the analysis can be downloaded \href{https://static-content.springer.com/esm/art\%3A10.1186\%2Fs12874-021-01235-8/MediaObjects/12874_2021_1235_MOESM1_ESM.csv}{here}.

The example evaulates the impact of a health policy intervention (an Australian health policy intervention that restricted the conditions under which a particular medicine (quetiapine) could be subsidised). The same methodological process can be applied to evaluate any intervention in any context.

The case study is described as follows:

\begin{quote}
(\ldots) due to growing concerns about inappropriate prescribing, after January 1, 2014 new prescriptions for this tablet strength could not include refills. Our primary outcome was the \textbf{number of monthly dispensings} of 25mg quetiapine, of which we had \textbf{48months of observations} (January 2011 to December 2014).
\end{quote}

Thus, data comprises 48 months of observations, and the date of the intervention is January 1, 2014.

There is also seasonality in the process:

\begin{quote}
In Australia, medicine dispensing claims have significant \textbf{yearly seasonality}. Medicines are subsidised for citizens and eligible residents through the Pharmaceutical Benefits Scheme (PBS), with people paying an out-of-pocket co-payment towards the cost of their medicines, while the remainder is subsidised. If a person's (or family's) total out-of-pocket costs reach the ``Safety Net threshold'' for the calendar year, they are eligible for a reduced co-payment for the remainder of that year. Thus, there is an incentive for people reaching their Safety Net to refill their medicines more frequently towards the end of the year. Hence, we see an \emph{increase in prescriptions at the end of the year, followed by a decrease in January}.
\end{quote}

The researchers hypothesize the nature of the intervention as follows (see the picture below):

\begin{quote}
(\ldots) due to the nature of the intervention we postulated there would be an immediate drop in dispensings post-intervention (step change), as well as a change in slope (ramp). Thus, we included variables representing both types of impacts in our model. For both impacts, h=0 and r=0.
\end{quote}

In the sentence above, \emph{h} describes when the effect happens while \emph{r} represents the decay pattern (see the picture below).

\includegraphics[width=31.14in]{images/intervention-functions}

First we load the data, converting it to a time series format, and we visualize the time series along with a vertical lines representing the date of the intervention.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load data}
\NormalTok{quet <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\DataTypeTok{file =} \StringTok{"./data/12874_2021_1235_MOESM1_ESM.csv"}\NormalTok{)}

\CommentTok{# Convert data to time series object}
\NormalTok{quet.ts <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(quet[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{frequency=}\DecValTok{12}\NormalTok{, }\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{2011}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{# Plot data to visualize time series}
\KeywordTok{plot.ts}\NormalTok{(quet.ts, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{40000}\NormalTok{), }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Month"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Dispensings"}\NormalTok{)}
\CommentTok{# Add vertical line indicating date of intervention (January 1, 2014)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\DecValTok{2014}\NormalTok{, }\DataTypeTok{col =} \StringTok{"gray"}\NormalTok{, }\DataTypeTok{lty =} \StringTok{"dashed"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-216-1.pdf}
Next, we have to create the dummy variables representing our intervention.
This can be tricky in R. In this case, the authors convert the time of the \emph{ts} object in a more human-readable format through the \emph{as.yearmon} function (this is a \emph{zoo} function and you can use it by loading the \emph{xts} library).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(xts)}
\CommentTok{# Create variable representing step change and view}
\NormalTok{step <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.yearmon}\NormalTok{(}\KeywordTok{time}\NormalTok{(quet.ts)) }\OperatorTok{>=}\StringTok{ "Jan 2014"}\NormalTok{)}
\NormalTok{step}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## [34] 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1
\end{verbatim}

The above vectors is a dummy variable for the intervention. It has value equal zero before the date of the intervention, and 1 after that.

Next, in this specific case, we also want to create a variable representing a constant increasing change, capturing an increasing effect of the intervention over time.
Also in this case the creation of the variable can be a little tricky. We create two vectors by using the \emph{rep} and the \emph{seq} function, and concatenate them by using the \emph{c} function.

The argument of the \emph{rep} function are two integers \emph{x} and \emph{times} (\emph{rep(x, times)}), and the function creates a vectors that repeat (``rep'') the \emph{x} values the number of times specified by \emph{times}. We have 36 months before the intervention, and we assign them the value zero.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{36}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## [34] 0 0 0
\end{verbatim}

Instead, we use the \emph{seq} function to create a vectors with increasing values. This part of the variable represent a gradual increase after the intervention (we have 12 months of data after the intervention). The function \emph{seq} takes the three arguments \emph{from}, \emph{to}, and \emph{by}: \emph{from} and \emph{to} are the starting and end values of the sequence, \emph{by} is the increment of the sequence. In our case we create a sequence of values that increases from 1 to 12 by 1.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1}\NormalTok{, }\DataTypeTok{to =} \DecValTok{12}\NormalTok{, }\DataTypeTok{by =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12
\end{verbatim}

To create the variable we need, we concatenate both the function with the \emph{c} function, as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create variable representing ramp (change in slope) and view}
\NormalTok{ramp <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{36}\NormalTok{), }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{ramp }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
## [23]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8
## [45]  9 10 11 12
\end{verbatim}

We search for an appropriate ARIMA model for the data by using the \emph{auto.arima} function (\emph{forecast} package). We include the variables we have created as external regressors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(forecast)}

\CommentTok{# Use automated algorithm to identify parameters}
\NormalTok{model1 <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(quet.ts, }\DataTypeTok{xreg =} \KeywordTok{cbind}\NormalTok{(step, ramp), }\DataTypeTok{stepwise=}\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# Check residuals}
\KeywordTok{checkresiduals}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-221-1.pdf}

\begin{verbatim}
## 
##  Ljung-Box test
## 
## data:  Residuals from Regression with ARIMA(2,1,0)(0,1,1)[12] errors
## Q* = 9.5692, df = 5, p-value = 0.0884
## 
## Model df: 5.   Total lags used: 10
\end{verbatim}

The resulting model is an \emph{ARIMA(2,1,0)(0,1,1){[}12{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: quet.ts 
## Regression with ARIMA(2,1,0)(0,1,1)[12] errors 
## 
## Coefficients:
##          ar1      ar2     sma1        step        ramp
##       -0.873  -0.6731  -0.6069  -3284.7792  -1396.6523
## s.e.   0.124   0.1259   0.3872    602.3347    106.6327
## 
## sigma^2 estimated as 648828:  log likelihood=-284.45
## AIC=580.89   AICc=583.89   BIC=590.23
\end{verbatim}

We use the information retrieved from the auto.arima function to fit the same ARIMA model to the data, without including the intervention (the variables we created), using just the data up to the date of the intervention (up to January 2014). To do that, we use the \emph{window} function in order to restrict the set of data we consider, indicating December 2013 as the end of our series.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# To forecast the counterfactual, model data excluding post-intervention time period}
\NormalTok{model2 <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(}\KeywordTok{window}\NormalTok{(quet.ts, }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{, }\DecValTok{12}\NormalTok{)), }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }
                \DataTypeTok{seasonal =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{period =} \DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Next, we forecast the 12 months we didn't include (starting from January 2014 until the end of the period of observation, December 2014) by using the \emph{forecast} function (library \emph{forecast}). The logic behind this operation is to see what would have happened to the series in the absence of the intervention. In other words, we use the prediction as a \emph{couterfactual} in order to describe a possible effect of the intervention on the series, by determining how the observed values diverges from this forecast.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Forecast 12 months post-intervention and convert to time series object}
\NormalTok{fc <-}\StringTok{ }\KeywordTok{forecast}\NormalTok{(model2, }\DataTypeTok{h =} \DecValTok{12}\NormalTok{)}

\CommentTok{# covert the average forecast (fc$mean) in a time series object}
\NormalTok{fc.ts <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(fc}\OperatorTok{$}\NormalTok{mean), }\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{2014}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{frequency =} \DecValTok{12}\NormalTok{)}

\CommentTok{# Combine the observed and the forecast data}
\NormalTok{quet.ts}\FloatTok{.2}\NormalTok{ <-}\StringTok{ }\KeywordTok{ts.union}\NormalTok{(quet.ts, fc.ts)}
\NormalTok{quet.ts}\FloatTok{.2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          quet.ts    fc.ts
## Jan 2011   16831       NA
## Feb 2011   17234       NA
## Mar 2011   20546       NA
## Apr 2011   19226       NA
## May 2011   21136       NA
## Jun 2011   20939       NA
## Jul 2011   21103       NA
## Aug 2011   22897       NA
## Sep 2011   22162       NA
## Oct 2011   22184       NA
## Nov 2011   23108       NA
## Dec 2011   25967       NA
## Jan 2012   20123       NA
## Feb 2012   21715       NA
## Mar 2012   24497       NA
## Apr 2012   21720       NA
## May 2012   25053       NA
## Jun 2012   23915       NA
## Jul 2012   24972       NA
## Aug 2012   26183       NA
## Sep 2012   24163       NA
## Oct 2012   26172       NA
## Nov 2012   26642       NA
## Dec 2012   29086       NA
## Jan 2013   24002       NA
## Feb 2013   24190       NA
## Mar 2013   26052       NA
## Apr 2013   26707       NA
## May 2013   29077       NA
## Jun 2013   26927       NA
## Jul 2013   30300       NA
## Aug 2013   29854       NA
## Sep 2013   28824       NA
## Oct 2013   31519       NA
## Nov 2013   32084       NA
## Dec 2013   33160       NA
## Jan 2014   24827 29127.50
## Feb 2014   23285 29671.28
## Mar 2014   23884 31156.37
## Apr 2014   21921 31339.65
## May 2014   22715 33843.48
## Jun 2014   19919 31809.61
## Jul 2014   20560 34498.50
## Aug 2014   18961 34774.18
## Sep 2014   18780 33302.09
## Oct 2014   17998 35641.85
## Nov 2014   16624 36184.57
## Dec 2014   18450 37792.03
\end{verbatim}

By plotting the data, we can visualize the predicted values in the absence of the intervention (red dashed line) as well as the observed values (blue line). It seems that the health policy considerably impacted the analyzed prescriptions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot}
\KeywordTok{plot.ts}\NormalTok{(quet.ts}\FloatTok{.2}\NormalTok{, }\DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{, }
     \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'blue'}\NormalTok{,}\StringTok{'red'}\NormalTok{), }\DataTypeTok{xlab=}\StringTok{"Month"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Dispensings"}\NormalTok{, }
     \DataTypeTok{lty=}\KeywordTok{c}\NormalTok{(}\StringTok{"solid"}\NormalTok{, }\StringTok{"dashed"}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{40000}\NormalTok{))}

\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\DecValTok{2014}\NormalTok{, }\DataTypeTok{lty=}\StringTok{"dashed"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-225-1.pdf}
Coming back to our initial ARIMA model including the intervention variables, calculating also the confidence intervals and the significance of the coefficients by using the \emph{coeftest} and the \emph{confint} function in the \emph{lmtest} library, we can quantify the impact of the policy.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lmtest)}

\NormalTok{model1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: quet.ts 
## Regression with ARIMA(2,1,0)(0,1,1)[12] errors 
## 
## Coefficients:
##          ar1      ar2     sma1        step        ramp
##       -0.873  -0.6731  -0.6069  -3284.7792  -1396.6523
## s.e.   0.124   0.1259   0.3872    602.3347    106.6327
## 
## sigma^2 estimated as 648828:  log likelihood=-284.45
## AIC=580.89   AICc=583.89   BIC=590.23
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coeftest}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## z test of coefficients:
## 
##         Estimate  Std. Error  z value  Pr(>|z|)    
## ar1     -0.87301     0.12396  -7.0428 1.885e-12 ***
## ar2     -0.67314     0.12587  -5.3480 8.893e-08 ***
## sma1    -0.60694     0.38722  -1.5674     0.117    
## step -3284.77921   602.33466  -5.4534 4.941e-08 ***
## ramp -1396.65226   106.63266 -13.0978 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              2.5 %        97.5 %
## ar1     -1.1159662    -0.6300574
## ar2     -0.9198342    -0.4264429
## sma1    -1.3658734     0.1519965
## step -4465.3334555 -2104.2249712
## ramp -1605.6484348 -1187.6560814
\end{verbatim}

\begin{quote}
The estimated step change was 3285 dispensings (95\% CI 4465 to 2104) while the estimated change in slope was 1397 dispensings per month (95\% CI 1606 to 1188). (The figure, ndr) shows the values predicted by our ARIMA model in absence of the intervention (counterfactual) compared with the observed values. This means that the change in subsidy for 25mg quetiapine in January 2014 was associated with an immediate, sustained decrease of 3285 dispensings, with a further decrease of 1397 dispensings every month. In other words, there were 4682 (3285+1397) fewer dispensings in January 2014 than predicted had the subsidy changes not been implemented. In February 2014, there were 6079 fewer dispensings (3285+2*1397). Importantly, our findings should only be considered valid for the duration of the study period (i.e.~until December 2014).
\end{quote}

\hypertarget{causalimpact}{%
\subsection{CausalImpact}\label{causalimpact}}

While ARIMA modeling is the classic choice for intervention models, more complex computational apporaches have been developed. We can consider the Bayesian approach implemented in the package \emph{CausalImpact} developed at \emph{Google} to estimate causal impacts in a quasi-experimental framework (you can find here a \href{https://www.youtube.com/watch?v=GTgZfCltMm8}{video presentation})\footnote{CausalImpact 1.2.1, Brodersen et al., Annals of Applied Statistics (2015). \url{http://google.github.io/CausalImpact/}}.

\includegraphics[width=20.49in]{images/causal-impact}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Install and load the package}
\CommentTok{# install.packages("CausalImpact")}
\KeywordTok{library}\NormalTok{(CausalImpact)}
\end{Highlighting}
\end{Shaded}

We use the simulated dataset used in the Google tutorial on the package, creating two time series \(y\) and \(x\) of length 100, simulating an abrupt intervention at time 71 determining a permanent increment of 10 points in the \(y\) series.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{x1 <-}\StringTok{ }\DecValTok{100} \OperatorTok{+}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{ar =} \FloatTok{0.999}\NormalTok{), }\DataTypeTok{n =} \DecValTok{100}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\FloatTok{1.2} \OperatorTok{*}\StringTok{ }\NormalTok{x1 }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{y[}\DecValTok{71}\OperatorTok{:}\DecValTok{100}\NormalTok{] <-}\StringTok{ }\NormalTok{y[}\DecValTok{71}\OperatorTok{:}\DecValTok{100}\NormalTok{] }\OperatorTok{+}\StringTok{ }\DecValTok{10}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{ts.intersect}\NormalTok{(y, x1)}
\end{Highlighting}
\end{Shaded}

Then, it is necessary to specify the pre-intervention and post-intervention period. In the pre-intervention period no impact is expected.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre.period <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{70}\NormalTok{)}
\NormalTok{post.period <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{71}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The function \emph{CausalImpact} uses the values of the original time series \(y\) in the pre-intervention period, and the predictors correlated to the \(y\) (in this case \(x\)), to forecast the values that \(y\) would have had without the intervention (\emph{counterfactual}).

To accurately forecast the \(y\) values, which is necessary to obtain valid results from the analysis, it is necessary to have a proper model of the \(y\) series (based on the series itself and its predictors). Then, the differences in the expected (forecasted) \(y\) values without intervention, and the actual \(y\) values following the intervention, are compared in order to estimate the impact of the intervention.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{impact <-}\StringTok{ }\KeywordTok{CausalImpact}\NormalTok{(dat, pre.period, post.period)}
\end{Highlighting}
\end{Shaded}

By using the function \emph{plot} on the resulting model, three plots are visualized:

\begin{quote}
The first panel shows the data and a counterfactual prediction for the post-treatment period. The second panel shows the difference between observed data and counterfactual predictions. This is the pointwise causal effect, as estimated by the model. The third panel adds up the pointwise contributions from the second panel, resulting in a plot of the cumulative effect of the intervention. (\ldots) the above inferences depend critically on the assumption that the covariates were not themselves affected by the intervention. The model also assumes that the relationship between covariates and treated time series, as established during the pre-period, remains stable throughout the post-period.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(impact)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-232-1.pdf}

Besides plotting the results, it is possible to create a summary of the model, and by adding the argument ``report'' inside the function \emph{summary}, a convenient explanations of the results is printed.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(impact)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Posterior inference {CausalImpact}
## 
##                          Average        Cumulative  
## Actual                   117            3511        
## Prediction (s.d.)        107 (0.37)     3196 (10.96)
## 95% CI                   [106, 107]     [3175, 3217]
##                                                     
## Absolute effect (s.d.)   11 (0.37)      315 (10.96) 
## 95% CI                   [9.8, 11]      [294.0, 336]
##                                                     
## Relative effect (s.d.)   9.9% (0.34%)   9.9% (0.34%)
## 95% CI                   [9.2%, 11%]    [9.2%, 11%] 
## 
## Posterior tail-area probability p:   0.00101
## Posterior prob. of a causal effect:  99.8993%
## 
## For more details, type: summary(impact, "report")
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(impact, }\StringTok{"report"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis report {CausalImpact}
## 
## 
## During the post-intervention period, the response variable had an average value of approx. 117.05. By contrast, in the absence of an intervention, we would have expected an average response of 106.54. The 95% interval of this counterfactual prediction is [105.83, 107.25]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 10.51 with a 95% interval of [9.80, 11.21]. For a discussion of the significance of this effect, see below.
## 
## Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 3.51K. By contrast, had the intervention not taken place, we would have expected a sum of 3.20K. The 95% interval of this prediction is [3.18K, 3.22K].
## 
## The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +10%. The 95% interval of this percentage is [+9%, +11%].
## 
## This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (10.51) to the original goal of the underlying intervention.
## 
## The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant.
\end{verbatim}

The authors of the package underline the importance of the statistical assumptions to get valid results, and about possible strategies to ascertain that the assumptions are met, they write the following advice:

\begin{quote}
Here are a few ways of getting started. First of all, it is critical to reason why the covariates that are included in the model (this was x1 in the example) were not themselves affected by the intervention. Sometimes it helps to plot all covariates and do a visual sanity check. Next, it is a good idea to examine how well the outcome data y can be predicted before the beginning of the intervention. This can be done by running CausalImpact() on an imaginary intervention. Then check how well the model predicted the data following this imaginary intervention. We would expect not to find a significant effect, i.e., counterfactual estimates and actual data should agree reasonably closely. Finally, when presenting or writing up results, be sure to list the above assumptions explicitly, including the priors in model.args, and discuss them with your audience.
\end{quote}

We can now try to use the library with the data from the previous example (the library requires data in the same format, thus some pre-processing is needed).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quet}\OperatorTok{$}\NormalTok{month <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(quet}\OperatorTok{$}\NormalTok{month, }\DataTypeTok{format=}\StringTok{"%d-%b-%y"}\NormalTok{)}
\NormalTok{quet_xts <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(quet}\OperatorTok{$}\NormalTok{dispensings, }\DataTypeTok{order.by =}\NormalTok{ quet}\OperatorTok{$}\NormalTok{month)}
\NormalTok{pre.period <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2011-01-01"}\NormalTok{), }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2013-12-01"}\NormalTok{))}
\NormalTok{post.period <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2014-01-01"}\NormalTok{), }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2014-12-01"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We fit an automated model (without specifying a particular form for the intervention).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{impact2 <-}\StringTok{ }\KeywordTok{CausalImpact}\NormalTok{(quet_xts, pre.period, post.period)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(impact2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-237-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(impact)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Posterior inference {CausalImpact}
## 
##                          Average        Cumulative  
## Actual                   117            3511        
## Prediction (s.d.)        107 (0.37)     3196 (10.96)
## 95% CI                   [106, 107]     [3175, 3217]
##                                                     
## Absolute effect (s.d.)   11 (0.37)      315 (10.96) 
## 95% CI                   [9.8, 11]      [294.0, 336]
##                                                     
## Relative effect (s.d.)   9.9% (0.34%)   9.9% (0.34%)
## 95% CI                   [9.2%, 11%]    [9.2%, 11%] 
## 
## Posterior tail-area probability p:   0.00101
## Posterior prob. of a causal effect:  99.8993%
## 
## For more details, type: summary(impact, "report")
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(impact, }\StringTok{"report"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis report {CausalImpact}
## 
## 
## During the post-intervention period, the response variable had an average value of approx. 117.05. By contrast, in the absence of an intervention, we would have expected an average response of 106.54. The 95% interval of this counterfactual prediction is [105.83, 107.25]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 10.51 with a 95% interval of [9.80, 11.21]. For a discussion of the significance of this effect, see below.
## 
## Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 3.51K. By contrast, had the intervention not taken place, we would have expected a sum of 3.20K. The 95% interval of this prediction is [3.18K, 3.22K].
## 
## The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +10%. The 95% interval of this percentage is [+9%, +11%].
## 
## This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (10.51) to the original goal of the underlying intervention.
## 
## The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant.
\end{verbatim}

\hypertarget{exercise-2}{%
\subsubsection{Exercise}\label{exercise-2}}

Let's try to analyze a real-world case study in the field of online communication.

We hypothesize that the uncertainty and anxiety caused by the pandemic has increased people's need of information. Social media are an important information source today, and the pages of the Health Ministries can be considered trustable information sources. At the same time, Facebook has favored the circulation of posts from verified health and government organization. It is therefore possible that the pages of the Ministries of Health have increased their follower base during the pandemic.

Thus, let's try to assess the impact of the pandemic on the growth in ``page likes'' of some Facebook pages of Health Ministry. We'll use the \emph{CausalImpact} library. It is the easiest solution, since we just need to indicate the intervention period, but it does not (necessarily) require other variables.

Download here the dataset of the \href{https://drive.google.com/file/d/1t4ncdNlanQwZwBvWx9AQe6R2dp-7Faew/view?usp=sharing}{Facebook page of the Italian Ministry of Health} and save it in your \emph{data} folder.

Some background information: On 9 March 2020, the government of Italy under Prime Minister Giuseppe Conte imposed a national lockdown. The World Health Organization (WHO) on March 11 declared COVID-19 a pandemic. For the sake of simplicity, we can use the declaration of WHO as the date of our ``intervention'' (more fine-grained analysis are possible as well).

Upload the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MinisteroSalute <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/MinisteroSalute.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's use the \emph{xts} format, since in this case it is easier to use (in particular, it's easier to work with the date format).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# dates in format "date"}
\NormalTok{MinisteroSalute}\OperatorTok{$}\NormalTok{beginning_of_interval <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(MinisteroSalute}\OperatorTok{$}\NormalTok{beginning_of_interval)}

\CommentTok{# xts time seires}
\NormalTok{MinisteroSalute <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(MinisteroSalute}\OperatorTok{$}\NormalTok{page_likes, }
                       \DataTypeTok{order.by =}\NormalTok{ MinisteroSalute}\OperatorTok{$}\NormalTok{beginning_of_interval)}
\end{Highlighting}
\end{Shaded}

Let's take a look at the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.xts}\NormalTok{(MinisteroSalute, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}
         \DataTypeTok{main=}\StringTok{"Page Likes - Italian Ministry of Health Facebook Page"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-242-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{start}\NormalTok{(MinisteroSalute)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2018-12-30"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{end}\NormalTok{(MinisteroSalute)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2020-12-31"
\end{verbatim}

The dates of the intervention:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{date_pre <-}\StringTok{  }\KeywordTok{c}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2018-12-30"}\NormalTok{), }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2020-03-10"}\NormalTok{))}
\NormalTok{date_post <-}\StringTok{  }\KeywordTok{c}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2020-03-11"}\NormalTok{), }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2020-12-31"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Fit the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{impact2 <-}\StringTok{ }\KeywordTok{CausalImpact}\NormalTok{(MinisteroSalute, }
                       \DataTypeTok{pre.period =}\NormalTok{  date_pre, }
                       \DataTypeTok{post.period =}\NormalTok{ date_post)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(impact2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdownproj_files/figure-latex/unnamed-chunk-246-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(impact2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Posterior inference {CausalImpact}
## 
##                          Average              Cumulative        
## Actual                   5.5e+05              1.6e+08           
## Prediction (s.d.)        1.8e+05 (13743)      5.2e+07 (4068069) 
## 95% CI                   [1.5e+05, 2e+05]     [4.5e+07, 6e+07]  
##                                                                 
## Absolute effect (s.d.)   3.8e+05 (13743)      1.1e+08 (4068069) 
## 95% CI                   [3.5e+05, 4.0e+05]   [1.0e+08, 1.2e+08]
##                                                                 
## Relative effect (s.d.)   214% (7.8%)          214% (7.8%)       
## 95% CI                   [199%, 228%]         [199%, 228%]      
## 
## Posterior tail-area probability p:   0.00107
## Posterior prob. of a causal effect:  99.89293%
## 
## For more details, type: summary(impact, "report")
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(impact2, }\StringTok{"report"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis report {CausalImpact}
## 
## 
## During the post-intervention period, the response variable had an average value of approx. 554.03K. By contrast, in the absence of an intervention, we would have expected an average response of 176.52K. The 95% interval of this counterfactual prediction is [151.27K, 202.82K]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 377.51K with a 95% interval of [351.21K, 402.75K]. For a discussion of the significance of this effect, see below.
## 
## Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 163.99M. By contrast, had the intervention not taken place, we would have expected a sum of 52.25M. The 95% interval of this prediction is [44.78M, 60.03M].
## 
## The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +214%. The 95% interval of this percentage is [+199%, +228%].
## 
## This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (377.51K) to the original goal of the underlying intervention.
## 
## The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant.
\end{verbatim}

We can also repeat the same analysis with the data from the Facebook page of the Austrian Ministry of Health (``Bundesministerium fr Soziales, Gesundheit, Pflege und Konsumentenschutz - Eingang Sozialministerium''): \href{https://drive.google.com/file/d/1ogZxpKd6jLyhFRn8I1Z-Kt5gQQX7h5cH/view?usp=sharing}{Download here the data}, save it in your \emph{data} folder, and perform the intervention analysis.

The chart below is from Markus Pollak, Nikolaus Kowarz und Julia Partheymller (2020): \href{https://viecer.univie.ac.at/en/projects-and-cooperations/austrian-corona-panel-project/corona-blog/corona-blog-beitraege/blog51/}{Chronology of the Corona Crisis in Austria - Part 1: Background, the way to the lockdown, the acute phase and economic consequences}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{include_graphics}\NormalTok{(}\StringTok{"images/covid-austria.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=5.93in]{images/covid-austria}

\hypertarget{var-introduction}{%
\section{VAR (Introduction)}\label{var-introduction}}

\emph{VAR} is an acronym that stands for \textbf{Vector Autoregressive Model}. It is a common method for the analysis of multivariate time series.

It can be conceived as a way to model a \textbf{system} of time series. In a VAR model, there is no rigid distinction between independent and dependent variables, but each variable is both dependent and independent. Besides these \textbf{endogenous} variables that dynamically interact, a VAR model can include \textbf{exogenous} variables. Exogenous variables can have an impact on the endogenous variables, while the opposite is not true.

To make a simple example, the time series of fans sold by month may be influenced by the quantity of fans produced and distributed by the industry, and by the monthly temperature. While the purchase and production of fans can interact (endogenous), the weather is not impacted by these processes, and just impact them as an external force.

To make another example (from the paper \href{http://www.italiansociologicalreview.com/ojs/index.php?journal=ISR\&page=article\&op=view\&path\%5B\%5D=448\&path\%5B\%5D=346}{The Impact of the Politicization of Health on Online Misinformation and Quality Information on Vaccines}), the political debate on a certain topic - for instance the political debate that led to the promulgation of the law on mandatory vaccinations in Italy - can be considered an exogenous variable that impacts both the news coverage of the topic and the spread of problematic information on Twitter. While news media coverage and Twitter discussions can be considered part of the same communication system and dependent on each other (news media could set the discussion agenda on Twitter, but also social media can stimulate news media coverage) it can be assumed that the political debate that led to the promulgation of the law on mandatory vaccinations was independent from the Twitter discussions on the topic.

Stationary tests are usually applied to ascertain that variables are not integrated (the ``I'' in the ARIMA model). If this is the case, variables are differenced before starting the VAR analysis. Other preliminary analysis (for instance testing for \emph{cointegration}) and pre-processing activity can be performed before the analysis. Next, the number of lags to be used has to be selected. This number can be automatically identify through lag-length selection criteria methods. After having fitted the model, residuals are analyzed. Results from a VAR model are usually complicated. To interpret them there are specific statistical methods, such as \emph{Granger causality test} and \emph{Impulse Response Function (IRF)}

\href{https://en.wikipedia.org/wiki/Granger_causality}{\emph{Granger causality test}} is a test developed by the nobel prize winner Clive Granger.

\begin{quote}
The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969. Ordinarily, regressions reflect ``mere'' correlations, but Clive Granger argued that causality (\ldots) could be tested for by measuring the ability to predict the future values of a time series using prior values of another time series. Since the question of ``true causality'' is deeply philosophical, and because of the post hoc ergo propter hoc fallacy of assuming that one thing preceding another can be used as a proof of causation, (\ldots) the Granger test finds only ``predictive causality''
\end{quote}

A variable \(X_t\) is said to Granger cause another variable \(Y_t\) if \(Y_t\) can be better predicted from the past of \(X_t\) and \(Y_t\) together than from the past of \(Y_t\) alone.

\emph{Impulse Response Function (IRF)} traces the dynamic impact to a system of a ``shock'' or change to an input, and asnwer the question of the reaction of a time series to a shock from another series in the system.

An R package to perform VAR modeling is \href{https://cran.r-project.org/web/packages/vars/vars.pdf}{\textbf{vars}}.

Practical applications of VAR modeling, including Granger causality and Impulse Response Function analysis, can be found, for instance, in the paper \href{https://academic.oup.com/joc/article-abstract/71/2/305/6104044}{Assembling the Networks and Audiences of Disinformation: How Successful Russian IRA Twitter Accounts Built Their Followings,2015--2017} or
\href{https://www-tandfonline-com.uaccess.univie.ac.at/doi/full/10.1080/10584609.2019.1661889}{Coordinating a Multi-Platform Disinformation Campaign: Internet Research Agency Activity on Three U.S. Social Media Platforms, 2015 to 2017} \href{https://github.com/jlukito/timeseries-bootcamp/blob/master/3_multivariate/varmodeling.md}{at this link}.

\hypertarget{readings-and-bibliographical-references}{%
\section{Readings and Bibliographical References}\label{readings-and-bibliographical-references}}

\emph{This page will be updated as the course goes on}

Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, A., \& Yang, J. (2019). \href{https://ijoc.org/index.php/ijoc/article/view/10635}{The Temporal Turn in Communication Research: Time Series Analyses Using Computational Approaches}. International Journal of Communication (19328036), 13.

Brodersen KH, Gallusser F, Koehler J, Remy N, Scott SL. \href{http://research.google.com/pubs/pub41854.html}{Inferring causal impact using Bayesian structural time-series models}. Annals of Applied Statistics, 2015, Vol. 9, No.~1, 247-274.

Gaubatz, K. T. (2014). \href{https://methods.sagepub.com/Book/a-survivors-guide-to-r}{A Survivor's Guide to R: An Introduction for the Uninitiated and the Unnerved}. SAGE Publications.

Liboschik, T., Fokianos, K., \& Fried, R. (2017). \href{https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf}{tscount: An R package for analysis of count time series following generalized linear models}. Journal of Statistical Software, 82(1), 1-51.

Schaffer, A. L., Dobbins, T. A., \& Pearson, S. A. (2021). I\href{https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01235-8}{nterrupted time series analysis using autoregressive integrated moving average (ARIMA) models: a guide for evaluating large-scale health interventions}. BMC medical research methodology, 21(1), 1-12.

Zivot E., Wang J. (2003) \href{https://faculty.washington.edu/ezivot/econ584/notes/unitroot.pdf}{Unit Root Tests}. In: Modeling Financial Time Series with S-Plus. Springer, New York, NY. \url{https://doi.org/10.1007/978-0-387-21763-5_4}

\hypertarget{useful-resources}{%
\subsection{Useful resources}\label{useful-resources}}

\href{https://stats.stackexchange.com}{Cross Validated} for statistics-related questions
\href{https://stackoverflow.com/questions}{Stackoverflow} for coding-related questions

\end{document}
