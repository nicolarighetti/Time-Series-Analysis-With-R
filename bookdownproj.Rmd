--- 
title: "Time Series Analysis With R"
author: "Nicola Righetti"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Time Series Analysis With R

*This book will be updated as the course goes on.*

## Objectives

This course is a practical introduction to time series analysis with R. 

It will introduce students to:

*   The specificity of time series data;
*   The free statistical software R to conduct time series analysis;
*   Some of the main univariate and multivariate techniques to analyze time series data.

At the end of the course, the students are expected to know the specificity of time series data and to be able to use R to perform simple time series analysis by applying the techniques described during the course.

## Lectures

Structure of the course:

*   **Theoretical concepts**: this part of the course will introduce students to the main theoretical concepts of time series analysis;
*   **R Tutorial**: this part of the course consists in a hands-on tutorial on the R functions necessary to perform time series analysis. Every part of a time series analysis project will be taken into account, including data wrangling, visual representation, and statistical analysis;
*   **Individual/Group work**: this part of the course consists in individual and group work based on the application of the theoretical and practical knowledge described in the previous part of the course

## Home Work

Theoretical concepts can be studied, but you have to practice in order to learn R and the data analysis tecniques.

## Assessment 

*   In-class participation (20%)
*   Final quiz on the main concepts (30%)
*   A final data analysis project where participants will apply the knowledge and techniques learned during the course (50%).

## Syllabus and readings

I created this open book for the *Advanced Data Analysis* students of the Master in Communication Science of the University of Vienna. The book includes both theoretical concepts and an R tutorial with the necessary code to perform all the operations we are going to learn. 

The book also includes hyperlinks to additional free resources and readings. 
The mandatory readings will be listed in the *Readings* section of the book.

A new part of the book will be uploaded online every weeks, following the program of the lessons. 

The link to this book is the following: [Time-Series-Analysis-With-R](https://nicolarighetti.github.io/Time-Series-Analysis-With-R/).

## Further information and support
For any information, communication, or request for clarification, reach out to me at [my University of Vienna address](https://ufind.univie.ac.at/de/person.html?id=113451).


<!--chapter:end:index.Rmd-->

# Getting started with R

## RStudio Interface and Data

### Download and Install RStudio 

This course is based on the statistical software R. R is easier to use in the development environment RStudio (it works on both Windows, Apple, and other OS).

It is possible to [download a free version of RStudio Desktop](https://rstudio.com/products/rstudio/download/) from the official websites.

You might also use a free *online* version of RStudio by registering to the [RStudio Cloud free plan](https://rstudio.cloud). However, the free plan gives you just 15 hours per months. Our lessons take 4.5 hours per month, and since you also need to practice, the best choice is to install RStudio and R on your computer. 

Now we are going to see how to get started with RStudio Desktop.

First,  [download and install a free version of RStudio Desktop](https://rstudio.com/products/rstudio/download/) and open the software.

### Create a RStudio Project and Import data

When starting a data analysis project with RStudio, we create a new dedicated environment where we will keep all the scripts (files containing the code to perform the analysis), data sets, and outputs of the analysis (such as plots and tables). This dedicated work-space is simply called a *project*. 

To create a new project with RStudio, follows these steps:

*   click on *File* (on the top left); 
*   then, click on *New Project*;
*   select *New Directory*, and *New Project*;
*   choose a folder for the project, and give a name to your project. You can use the name *Time-Series-Analysis-With-R*;

In this way, it will be created a new folder for the project, in the main folder specified in the previous step. In this folder, you will find a file **.Rproj**, the name of which is the name you assigned to your project. To work on this project, you just need to open the *.Rproj* file.

```{r echo=FALSE}
knitr::include_graphics("images/r-studio-project-creation.png")
```

### Create a Script 

Once the project has been created, we can open a new **script** and save it.

A script is a file containing code. We can create a first script named *basic-r-syntax*, where you will test the basic code we are going to see. The script will be saved with extension *.r*. 

You can open, change, and save the file every time you work on it.
To save your code is important, otherwise you would have to write the same code every time you work on the project!

```{r echo=FALSE}
knitr::include_graphics("images/create-script.png")
```

![Create and save a script](videos/create-script.mov){width="80%"}

![Update a script and run code](videos/update-script.mov){width="80%"}

### The RStudio User Interface

The interface of RStudio is organized in four main quadrants:

*   The top-left quadrant is the editor. Here you can create or open a script and compose the R commands. 
* The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. The bottom-right quadrant is a window for graphics output, but it also has tabs to manage your file directories, R packages, and the R Help facility.
*   On the bottom left is the R Console window, where the code gets executed and the output is produced. You can run the commands, sending the code from the editor to the console, by highlighting it and hitting the *Run* button, or the Ctrl-Enter key combination. It is also possible to type and run commands directly into the console window (in this case, nothing will be saved).
*   The top-right quadrant shows the R workspace, which holds the data and other objects you have created in the current R session. There is the *file* tab, where you can navigate files and folders and find, for instance, the data sets you want to upload.
*   The bottom-right quadrant is a window for graphics output. Here you can visualize your plots. There is also a tab for the R packages, and the R Help facility.

```{r echo=FALSE}
knitr::include_graphics("images/r-environment.png")
```

### Load and Save Data

To load data into R you can click on the **file** window in the top-right quadrant, navigate your files/folders, and once you have found your data set file, you can just click it and follow the semi-automated import procedure.

```{r echo=FALSE}
knitr::include_graphics("images/load-data.png")
```

```{r echo=FALSE}
knitr::include_graphics("images/import-data-1.png")
```

![Import Data](videos/import-data.mov){width="80%"}

Otherwise, you can upload a data set by using a function. For instance, to import a *csv* file, one of the most common format for data sets, it can be employed the function **read.csv**. The main argument of this function is the *path* of the file you want to upload. 
To specify the file path, consider that you are working within a specific environment, that is, your *working directory* is the folder of the project (you can double check the working directory you are working in, by running the command **getwd()**). Thus, to indicate the path of the data set you want to upload, you can write a dot followed by a slash **./**, followed by the path of the data set *inside the working directory*. For instance, in the case below, the data set is saved in a folder named *data* inside the working directory. The name of the data set is *tweets_vienna* and its extension is *.csv*. Therefore, the code to upload the file is as follows:

```{r}
fake_news <- read.csv("./data/fake-news-stories-over-time-20210111144200.csv")
```

To save data there are a few options. Generally, if you want to save a data set, you can opt for the *.csv* or the *.rds* format. The *.rds* format is only readable by R, while the *.csv* is a "universal" format (you can read it with Excel, for instance).

To save a file as *.csv* it can be used the function **write.csv**. The main arguments of this function are the name of the object that has to be saved, the path to the folder where the object will be saved, and the name we want to assign to the file.

```{r}
write.csv(fake_news, file = "./data/fake_news.csv")
```

To save *.rds* file the procedure is similar, but the **saveRDS** function has to be employed. Instead, to read an *rds* file, the appropriate function is **readRDS**.

```{r}
saveRDS(fake_news, file = "./data/fake_news.rds")

fake_news <- readRDS("./data/fake_news.rds")   # read a .rds file
```

In the code above you can notice an hash mark sign followed by some text. It is a **comment**. Comments are textual content used to describe the code in order to make it easier to understand and reuse it. Comments are written after the **hash mark sign (#)**, because the text written after the hash mark sign is ignored by R: you can read the comments, but R does not consider them as code. 

### Create new Folders

It is a good practice to create, in the main folder of the project, sub-folders dedicated to different type of files used in the project, such as a folder "data" for the data sets. 

To create a new folder you can go to the *Files* windows in the RStudio interface, click **New Folder**, and give it a name.

```{r echo=FALSE}
knitr::include_graphics("images/new-folder.png")
```


## Basic R  

### Objects 

An *object* is an R entity composed of a name and a value.

The **arrow (<-)** sign is used to *create objects* and *assign a value* to an object (or to change or "update" its previous value).

Example: create an object with name "object_consisting_of_a_number" and value equal 2:

```{r}

object_consisting_of_a_number <- 2
```

Enter the name of the object in the console and run the command: the value assigned to the object will be printed.

```{r}
object_consisting_of_a_number
```

The object is equal to its value. Therefore, for instance, an object with a numerical value can be used to perform arithmetical operations.

```{r}
object_consisting_of_a_number * 10
```

The value of an object can be transformed:

```{r}
object_consisting_of_a_number <- object_consisting_of_a_number * 10

object_consisting_of_a_number
```

An object can also represent a **function**. 

Example: create an object for the *sum* (addition) function:

```{r}
function_sum <- function(x, y){
  result <- x + y
  return(result)
}
```

The function can now be applied to two numerical values: 

```{r}
function_sum(5, 2)
```

Actually, we don't need this function, since mathematical functions are already implemented in R.

```{r}
sum(5, 2)
```

```{r}
5 + 7
```

```{r}
2 * 3
```

```{r}
3^2
```

```{r}
sqrt(9)
```

The value of an object can be a number, a function, but also a **vector**.
Vectors are sequences of values.

```{r}
vector_of_numbers <- c(1,2,3,4,5,6,7,8,9,10) 
```

```{r}
vector_of_numbers
```

A vector of numbers can be the argument of mathematical operations.

```{r}
vector_of_numbers * 2
```

```{r}
vector_of_numbers + 3
```

Other R objects are *matrix*, *list*, and *data.frame.* 

A **matrix** is a table composed of rows and columns containing only numerical values.

```{r}
a_matrix <- matrix(data = 1:50, nrow = 10, ncol = 5)

a_matrix
```

A **list** is just a list of other objects.
For instance, this list includes a numerical value, a vectors of numbers, and a matrix.

```{r}
a_list <- list(object_consisting_of_a_number, vector_of_numbers, a_matrix)

a_list
```

A **data.frame** is like a matrix that can contain *numbers* but also other types of data, such as *characters* (a textual type of data), or *factors* (unordered categorical variables, such as gender, or ordered categories, such as low, medium, high).

Data sets are usually stored in data.frame. For instance, if you import a *csv* or an *Excel* file in R, the corresponding R object is a *data.frame*.

```{r}
# this is an object (vector) consisting of a series of numerical values
numerical_vector <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)
numerical_vector
```

```{r}
# this is another object (vector) consisting of a series of categorical values
categorical_vector <- c("Monday", "Tuesday", "Monday", "Tuesday", "Monday", "Wednesday","Thursday", "Wednesday", "Thursday", "Saturday", "Sunday", "Friday", "Saturday", "Sunday")
categorical_vector
```

```{r}
# this is an object consisting of a data.frame, created combining vectors through the function "data.frame"
a_dataframe <- data.frame("first_variable" = numerical_vector,
                          "second_variable" = categorical_vector)
a_dataframe
```

To access a specific column of a data.frame, you can use the name of the data.frame, the dollar symbol **$**, and the name of the column.

```{r}
a_dataframe$first_variable
```

```{r}
a_dataframe$second_variable
```

It is possible to add columns to a data.frame by writing:

*   the name of the data.frame
*   the dollar sign
*   a name for the new column
*   the *arrow sign <-*
*   a vector of values to be stored in the new column (it has to have length equal to the other vectors composing the data.frame)

```{r}
a_dataframe$a_new_variable <- c(12, 261, 45, 29, 54, 234, 45, 42, 6, 267, 87, 3, 12, 9)
```

```{r}
a_dataframe
```

It is possible to visualize the first few rows of a data.frame by using the function *head*.

```{r}
head(a_dataframe)
```

```{r echo=FALSE}
knitr::include_graphics("images/data-frame-example.png")
```

![Exercise: visualize the first rows of a data.frame and access its columns](videos/data-frame-exercise-1.mov){width="80%"}

### Functions

A **function** is a coded operation that applies to an object (e.g.: a number, a textual feature etc.) to transform it based on specific rules. A function has a name (the name of the function) and some *arguments*. Among the arguments of a function there is always an *object* or a value, for instance a numerical value, which is the content the function is applied to, and other possible arguments (either mandatory or optional).

Functions are operations applied to objects that give a certain output. E.g.: the arithmetical operation "addition" is a function that applies to two or more numbers to give, as its output, their sum. The *arguments* of the "sum" function are the numbers that are added together.

The name of the function is written out of *parentheses*, and the arguments of the function inside the parentheses:

```{r}
sum(5, 3)
```

Arguments of functions can be numbers but also textual features. For instance, the function *paste* creates a string composed of the strings that it takes as arguments.

```{r}
paste("the", "cat", "is", "at", "home")
```

In R you can sometimes find a "nested" syntax, which can be confusing. The best practice is to keep things as simple as possible.

```{r}
# this comment, written after the hash mark, describe what is going on here: two "paste" function nested together have been used (improperly! because they make the code more complicated than necessary) to show how functions can be nested together. It would have been better to use the "paste" function just one time!
paste(paste("the", "cat", "is", "at", "home"), "and", "sleeps", "on", "the", "sofa")
```

To sum up, functions manipulate and transform objects. Data wrangling, data visualization, as well as data analysis, are performed through functions.

### Data Types

Variables can have different R *formats*, such as:

*   **double**: numbers that include decimals (0.1, 5.676, 121.67). This format is appropriate for *continuous variables*;
*   **integer**: such as 1, 2, 3, 10, 400. It is a format suitable to *count data*;
*   **factors**: for categorical variables. Factors can be ordered (e.g.: level of agreement: "high", "medium", "low"), or not (e.g.: hair colors "blond", "dark brown", "brown");
*   **characters**: textual labels;
*   **logicals**: the format of logical values (i.e.: TRUE and FALSE)
*   **dates**: used to represent days;
*   **POSIX**: a class of R format to represent dates and times.

```{r fig.cap="R data formats. Tables from Gaubatz, K. T. (2014). [A Survivor's Guide to R: An Introduction for the Uninitiated and the Unnerved](https://us.sagepub.com/en-us/nam/a-survivors-guide-to-r/book242607). SAGE Publications.", echo=FALSE}
knitr::include_graphics("images/r-data-format.png")
```

It is better to specify the appropriate type of data when importing a data set. In the example below, the data format are specified by using the import process of RStudio. 

Notice that the data of type "date" requires users to specify the additional information regarding the format of the dates. Indeed, dates can be written in many different ways, and to read dates in R it is necessary to specify the structure of the date. In the example, dates are in the format Year-Month-Day, which is represented in R as "%Y-%m-%d" (further details will be provided in another section of the book).

![Import data and specify data types](videos/import-data-types.mov){width="80%"}

### Excercise

*   Upload the data set "election news small", using the appropriate data format; 
*   Open the script "basic-r-script" and perform the following operations:
    + Check the first few rows of the data set;
    + Access the single columns;
    + Save the data frame with the name "election_news_small_test" in the folder "data" by using the function "write.csv" (to review the procedure go to the section "Load and Save Data" on this book);
    + Comment the code (the comments have to be written after the hash sign *#*);
    + Save the script.

# Basic Data Wrangling with Tidyverse

[Data wrangling](https://en.wikipedia.org/wiki/Data_wrangling) *is the process of transforming and mapping data from one "raw" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.*

[Another definition](https://link.springer.com/referenceworkentry/10.1007%2F978-3-319-63962-8_9-1) is as follows: *Data wrangling is the process of profiling and transforming datasets to ensure they are actionable for a set of analysis tasks. One central goal is to make data usable: to put data in a form that can be parsed and manipulated by analysis tools. Another goal is to ensure that data is responsive to the intended analyses: that the data contain the necessary information, at an acceptable level of description and correctness, to support successful modeling and decision-making.*

How to "manipulate" data sets in R: 

*   use basic R functions; 
*   employ specific libraries such as **tidyverse**. Tidyverse is an R library composed of functions that allow users to perform basic and advanced data science operations.  [https://www.tidyverse.org](https://www.tidyverse.org). 

In R, a **library (or "package")** is a coherent collection of functions, usually created for specific purposes.

To work with the *tidyverse* library, it is necessary to install it first, by using the following command: *install.packages("tidyverse")*.

After having installed tidyverse (or any other library), it is necessary to load it, so as we can work with its functions in the current R session:

```{r warning=FALSE}
# to load a library used the command library(NAME-OF-THE-LIBRARY)
library(tidyverse)
```

Besides using the function *install.packages(NAME-OF-THE-LIBRARY)* by using a line of code, it is also possible to use the RStudio interface.

![Install and Load a Library](videos/install-load-library.mov){width="80%"}

## The Pipe Operator %>%

Tidyverse has a peculiar syntax that makes use of the so-called [**pipe**](https://style.tidyverse.org/pipes.html) operator **%>%**, like in the following example:

```{r paged.print=TRUE}
a_dataframe %>%
  group_by(second_variable) %>%
  summarize(mean = mean(a_new_variable))
```

To manipulate data sets we can rely on the functions included in [**dplyr**](https://dplyr.tidyverse.org): *a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges*, such as *mutate*, *rename*, *summarize*.


```{r "Load an example data set", paged.print=TRUE}
library(readr)
tweets <- read_csv("data/tweets_covid_small.csv", 
    col_types = cols(created_at = col_datetime(format = "%Y-%m-%d %H:%M:%S"), 
        retweet_count = col_integer()))
```

```{r}
head(tweets)
```
![Import data and specify data types (dates and times)](videos/upload-covid-tweets-small.mov){width="80%"}

## Mutate 

The function **mutate** adds new variables to a data.frame or overwrites existing variables.

```{r paged.print=TRUE}
tweets <- tweets %>%
  mutate(log_retweet_count = log(retweet_count))
  
head(tweets)
```

## Rename

Rename is a function to change the name of columns (sometimes it could be useful).

```{r paged.print=TRUE}
tweets <- tweets %>%
  # rename (new_name = old_name)
  rename(device = source)

head(tweets)
```

The previous two steps can be performed at the same time, by concatenating the operations through the *pipe %>%* operator.

```{r paged.print=TRUE}
# load again the data set
library(readr)
tweets <- read_csv("data/tweets_covid_small.csv", 
    col_types = cols(created_at = col_datetime(format = "%Y-%m-%d %H:%M:%S"), 
        retweet_count = col_integer()))

tweets <- tweets %>%
  mutate(log_retweet_count = log(retweet_count+1)) %>%
  rename(device = source)

head(tweets)
```

To check the data format of the variables stored in the data.frame, it can be used the command *str()*:

```{r paged.print=TRUE}
str(tweets)
```

Sometimes variables are stored in the data.frame in the wrong format (see the paragraph "data type"), so we want to convert them into a new format. For this purpose we can use, again, the function **mutate**, along with other functions such *as.integer*, *as.numeric*, *as.character*, *as.factors*, or *as.logical*, *as.Date*, or *as.POSIXct()* based on the desired data format (it is possible and advisable to upload the data by paying attention to the type of data. If you upload the data in the right format, you can skip this step).

```{r paged.print=TRUE}
tweets %>%
  mutate(device = as.character(device)) %>%
  head()

```

## Summarize and group_by

To aggregate data and calculate synthetic values (for instance, the average number of tweets *by day*), it can be used the function *group_by* (to aggregate data, for instance by day), and *summarize*, to calculate the summary values.

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
tweets_summary <- tweets %>%
  group_by(screen_name) %>%
  summarize(average_retweets = mean(retweet_count))

head(tweets_summary)
```

It is also possible to create more than one summary variables at once.

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
tweets_summary <- tweets %>%
  group_by(screen_name) %>%
  summarize(average_retweets = mean(retweet_count),
            average_log_retweets = mean(log_retweet_count))

head(tweets_summary)
```

### Count occurrences

A useful operation to perform when summarizing data, is to count the occurrences of a certain variable. For instance, to count the number of tweets sent by each user, it can be used the function **n()** inside the *summarize* function.

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
tweets_summary <- tweets %>%
  group_by(screen_name) %>%
  summarize(average_retweets = mean(retweet_count),
            average_log_retweets = mean(log_retweet_count),
            number_of_tweets = n())

head(tweets_summary)
```

## Arrange

To explore a data set it can be useful to sort the data (e.g.: from the lowest to the highest value of a variable). With tidyverse, we can order a data.frame by using the function *arrange*.

To sort the data from the highest to the lowest value (descending order) the *minus* sign (or the "desc" function) has to be added.

```{r paged.print=TRUE}
tweets_summary  %>%
  arrange(-number_of_tweets) %>%
  head()
```
```{r paged.print=TRUE}
tweets_summary  %>%
  arrange(desc(average_retweets)) %>%
  head()
```

Without the minus sign (or the "desc" command), data are sorted from the lowest to the highest value. 

```{r paged.print=TRUE}
tweets_summary  %>%
  arrange(number_of_tweets) %>%
  head()
```

## Filter

The function *filter* keeps only the cases (the "rows") we want to focus on. The arguments of this function are the conditions that have to be fulfilled to filter the data: a) the name of the column that we want to filter, b) the values to be kept.

```{r paged.print=TRUE}
tweets %>%
  filter(retweet_count >= 500) %>%
  arrange(-retweet_count)
```

In the examples below, notice the use of a double equal sign *==*, and also of the quotation marks to indicate the modalities of a categorical variable.

```{r paged.print=TRUE}
tweets %>%
  filter(retweet_count == 1988) 
```

```{r paged.print=TRUE}
tweets %>%
  filter(device == "Twitter for Android")
```

It is also possible to use several conditions at the same time.

```{r paged.print=TRUE}
tweets %>%
  filter(device == "Twitter for Android",
         retweet_count > 200) %>%
  arrange(-retweet_count)
```

## Select

**Select** is used to keep just some of the columns of the original data.frame. For instance, we can apply the function in order to keep just the column "device" and "retweet_count".

```{r paged.print=TRUE}
tweets %>%
  select(device, retweet_count) %>%
  head()
```

## Exercise

Here are some exercises to consolidate the fundamental R skills learned during these first lessons:

*   Download the csv file *tweets_vienna_small.csv* in [this folder](https://drive.google.com/file/d/1RKPFOLsIAPLuACcrcB0iiq289CvDDgiT/view?usp=sharing) and put it into the project folder "data"
*   Upload the data set in R, setting the appropriate formats for the variables
*   Create a new script named "data-manipulation" with  your surname and name as follows: *"YOUR SURNAME-YOUR NAME-data-manipulation"*, and save it
*   In the script, write the code to perform the following operations:

    + load the library "Tidyverse"
    + show the first rows of the data frame by using the function *head*
    + create a new data frame "tweets_vienna_small_updated" by updating the dataframe "tweets_vienna_small" by using the function *mutate* to create a new column "log_friends_count" whose values are the log of the values in the column "friends_count" (you don't need to add 1 to the values in the column "friends_count")
    + save the updated dataframe "tweets_vienna_small_updated", by using the following code to save a csv file (please change *YOUR SURNAME-YOUR NAME* with your actual surname and name): *write.csv(tweets_vienna_small_updated, file = "./data/YOUR SURNAME-YOUR NAME-tweets_vienna_small_updated.csv", row.names=F)* (we add row.names=F to avoid saving the number that indexes each row)
    
    + create a new data frame named "summary_tweets_vienna_small" aggregating the data by "screen_name" (using the function *group_by*) and then summarizing the data (by using the function *summarize*) as follows:
    
      * in a column named "average_favorite_count", calculate the average of "favorite_count" by "screen_name" (that is, by user)
      * in a column named "average_retweet_count", calculate the average of "retweet_count" (by user, obviously, since the data are already aggregated by user' name)
      * in a column named "number_of_tweets", calculate the number of tweets published by each users (by using the function *n()*)
      * save the "summary_tweets_vienna_small" in the data folder of the project, in csv format, and with the name *"YOUR SURNAME-YOUR NAME-summary_tweets_vienna_small.csv"* (remember to specify *row.names=F* and  to change *YOUR SURNAME-YOUR NAME* with your actual surname and name)
      
      * create a new data frame object called "summary_tweets_vienna_small_filtered", where you will save the data.frame summary_tweets_vienna_small, after having filtered the rows with average_retweet_count higher than 10 (by using the function *filter*), and after having selected the column "screen_name" and "average_retweet_count" (so, you should end up with a data frame with just two columns, "screen_name" and "average_retweet_count", and the rows with "average_retweet_count" higher than 10)
      * save the data frame "summary_tweets_vienna_small_filtered" with the name *"YOUR SURNAME-YOUR NAME-summary_tweets_vienna_small_filtered.csv"* in the folder data (remember to specify *row.names=F* and  to change *YOUR SURNAME-YOUR NAME* with your actual surname and name).
      
Save the script "YOUR SURNAME-YOUR NAME-data-manipulation" with all the code you have used to perform these analysis. Write a comment in the script (using the hash mark *#*) if you are not able to do something. 

Upload the script "YOUR SURNAME-YOUR NAME-data-manipulation.r" and the files "YOUR SURNAME-YOUR NAME-tweets_vienna_small_updated.csv", "YOUR SURNAME-YOUR NAME-summary_tweets_vienna_small.csv", and the file "YOUR SURNAME-YOUR NAME-summary_tweets_vienna_small_filtered.csv" on Moodle, in the folder "HomeWork-1". *The deadline is Sunday 11 April*.
      


<!--chapter:end:02-Getting-Started-with-R.Rmd-->

# Basic Concepts

## Time Series

A **time series** is a serially sequenced set of values representing a variable value at different points in time (VanLear, ["Time Series Analysis"](https://us.sagepub.com/en-us/nam/the-sage-encyclopedia-of-communication-research-methods/book244974)). It consists in measures collected through time, at **regular time intervals**, about an unit of observation, resulting in a set of ordered values. This *regularity* is the **frequency** of time series (which can be, for instance,  hourly, weekly, monthly, quarterly, yearly etc.).

Time series data are different from *cross-sectional data*, which are set of data observed on a sample of units taken at a *given point in time*, or where the *time dimension is not relevant* and can be ignored. Cross-sectional data are a **snapshot** of a population of interest at one particular point in time, while time series show the **dynamical** evolution of a variable over time. *Panel data* combine cross-sectional and time series data by observing the same units over time. 

**Time** is a fundamental variable in time series. It is often not relevant in other types of statistical analyses. Also from a **sociological perspective** (and psychological as well), we can see that **past events influence future** behaviors. Oftentimes, we can make reasonable **prediction** about future social behaviors just by observing past behaviors. Actually, social reproduction of behaviors over time and predictability of future social behaviors based on past experience and shared knowledge are  essential to social order, and thus, a fundamental dimension of human society.

From a **statistical perspective**, the impact of time resulting from **repeated measurements** over time on a single subject or unit, introduce a **dependency among data points** which *prevents the use of some of the most common statistical techniques*. In cross-sectional data, observations are assumed to be independent: values observed on one unit has no influence on values observed on other units. Time series observations have a different nature: a time series is not a collection of independent observations, or observations taken on independent units, but a collection of successive observations on the same unit. Observations are not taken across units at the same time (or without regards to time), but across time on the same unit. 

When dealing with time series data, time is an important factor to be taken into account. It introduces a new dimension to the data. For instance, we can calculate how a variable increases or decreases over time, if it peaks at a given moment in time, or at regular intervals. We consider not just if, and how much, a variable is correlated with another variable, but if there is a correlation over time among them, if the peaks in one variable precedes the peaks in the other one, or how much time it requires for a variable to have an impact on another one, and how much this impact changes over time.
 
Importantly, when dealing with time series data, we have to to acknowledge that sampling adjacent points in time introduces a **correlation in the data**. This **serial dependency** creates *correlated errors* which violates the assumptions of many traditional statistical analyses and can *bias* the estimation of error for confidence intervals or significance tests. This characteristic of time series data, in general, *precludes the use of common statistical approaches* such as linear regression and correlation analysis, *which assume the observations to be independent*.

The application of "standard" statistical techniques to time series data might lead to foolish, and totally unreliable results. For instance, the statisticians [George Udny Yule](https://en.wikipedia.org/wiki/Udny_Yule) wrote: 

>«It is fairly familiar knowledge that we sometimes obtain between quantities varying with the time (time-variables) quite high correlations to which we cannot attach any physical significance whatever, although under the ordinary test the correlation would be held to be certainly "significant." (…) the occurrence of such "nonsense-correlations" makes one mistrust the serious arguments that are sometimes put forward on the basis of correlations between time-series. […] When the successive x's and y's in a sample no longer form a random series, but a series in which successive terms are closely related to one another, the usual conceptions (of correlation, ed.) to which we are accustomed fail totally and entirely to apply» (Yule, G.U. (1926). [Why do we sometimes get nonsense-correlations between Time-Series? A study in sampling and the nature of time-series](https://www.jstor.org/stable/pdf/2341482.pdf). *Journal of the royal statistical society*, 89(1), 1-63.) 

A funny website reporting spurious time series correlation is [tylervigen.com](https://www.tylervigen.com/spurious-correlations). 

```{r echo=FALSE}
knitr::include_graphics("images/chart.png")
knitr::include_graphics("images/chart-2.png")
```

Despite it can be funny to see these improbable correlations, we have to keep in mind that adopting the right approach to analyze data is a serious issue when doing research. In a [paper on the American Journal of Political Science](https://gking.harvard.edu/files/gking/files/epr.pdf) we can read, for instance: 

>The results of the analysis below strongly suggest that the way event counts have been analyzed in hundreds of important political science studies have produced statistically and substantively unreliable results. Misspecification, inefficiency, bias, inconsistency, insufficiency, and other problems result from the unknowing application of two common methods that are without theoretical justification or empirical utility in this type of data. 

Due to the peculiarity of time series data, **time series analysis** has been developed as a specific statistical methodology appropriate for the analysis of **time-dependent data**. Time series analysis aims at providing an understanding of the *underlying processes and patterns of change over time* of a unit of observation and the relations between variables observed over time, handling the time structure of the data in a proper way.


## Time Series Analysis

Time series analysis is an approach employed in many disciplines. Almost every field of study has data characterized by a time development, and every phenomenon with a temporal dimension can be conceived as a time series, and can be analyzed through time series analysis methods. Time series analysis are an important part of data analysis in disciplines such as economics, to analyze, for instance, *inflation trends*, marketing to analyze the *number of clients* of a store or number of accesses to an e-commerce website, in demography to study the *growth of national population* overtime or *trends in population ageing*, in engineering to analyze *radio frequencies*, in neurology to analyze *brain waves* detected through electroencephalograms. Political science can be interested in studying patterns in *alternation of political parties in government*, and digital communication can be interested in using time series analysis to study series of tweets using an hashtag, the *news media coverage* on a certain topic, or the trends in *users searches on search engines*, such as those provided by Google Trends. 

About the use of time series analysis in communication science, it can be observed that:

>"Many of the major theories and models in our field contain time as a central player: the two-step flow, cultivation, spiral-of-silence, agenda-setting, framing, and communication mediation models, to name a few (Nabi & Oliver, 2009). Each articulates a set of processes that play out in time: Messages work their way through media systems and networks, citizens perceive the world around them and decide to communicate, or not, and they make choices about participation, presumably as a product of a process that includes communication exposure. Indeed, the words that animate our field—effect, flow, influence, dynamic, cycle—reveal our understanding of communication as a process, and processes have temporal dimensions (Box-Steffensmeier, Freeman, Hitt, & Pevehouse, 2014). The perspective of time series analysis can help expand our notions of time’s role in these dynamics. We see several ways in which we can become more attentive to time in our field". [Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, A., & Yang, J. (2019). The Temporal Turn in Communication Research: Time Series Analyses Using Computational Approaches. International Journal of Communication (19328036), 13](https://ijoc.org/index.php/ijoc/article/view/10635).

>"One of the most common applications of time series analyses in mass communication is in agenda-setting research. The approach is to correlate the national news coverage on a topic over time with public opinion or public policy on that topic, often to estimate lagged effects or the decay of effects over time. Likewise, both trends and cycles of television programming, viewing, and advertising, have been explored through time series analyses. In the interpersonal literature, the most popular and one of the most important applications of time series analysis has been the investigation of mutual adaptation in the form of patterns of reciprocity or compensation between conversational partners over the course of an interaction." (C. Arthur VanLear, "Time Series Analysis", in Allen, M. (Ed.). (2017). [The SAGE encyclopedia of communication research methods](https://us.sagepub.com/en-us/nam/the-sage-encyclopedia-of-communication-research-methods/book244974). Sage Publications).

In general, we can distinguish at least the following objectives of a time series analysis study:

*   **DESCRIPTION**: Description of a process characterized by an intrinsic temporal dimension. Simple examples of related questions are: is there an upward trend? Is there a peak at a certain point in time? Is there a regular pattern recurring every year, in a particular moment in time? Descriptive questions like these can be answered via descriptive time series analysis.
*   **EVALUATION**: Evaluation of the impact of a certain event, occurring in a particular point in time, on a process. For instance: did a change in social media moderation policy, such as those that led to ban accounts linked to conspiracy theories, impacted on the quantity of fake news shared online by users? Specific time series techniques can be used to perform this kind of analysis.
*   **EXPLANATION**: Explanation of a phenomenon characterized by a time series structure on the basis of related variables. For instance: does the quantity of news shared on Facebook help explaining the polarization of the debate online? Does the volume of news media articles on a topic help explaining the growth of the debate online on the same topic? Inferential statistical techniques, such as regression models developed for time series, are used to answer questions like these.
*   **FORECASTING**: Prediction of the future values of a process. For instance: can we expect that news media coverage on a certain topic keep growing in the near future? This is the subject of time series forecasting.

We can also distinguish between **univariate** and **multivariate** time series analysis. Time series analysis can be used to explain the temporal dependencies **within** and **between** processes. By temporal dependency within a social process, we mean that the current value of a variable is, in part, a function of previous values of that same variable. To analyze univariate structure of time series, univariate techniques are used. Temporal dependency between social processes, conversely, indicates that the current value of a variable is in part a function of the previous values of other variables. Multivariate time series analysis are used to explain the relations between time series. 

## Stochastic and Deterministic Processes

A general distinction can be made between time series, based on their deterministic or non-deterministic nature.

A **deterministic time series** is one which can be explicitly expressed by an analytic expression. It has no random or probabilistic parts. It is always possible to exactly predict its future behavior, and state how it behaved in the past. Deterministic processes are pretty rare when dealing with individual and social behaviors! Predicting future behaviors of a crowd, of a person, of a social group, can be reasonably possible, sometimes, based on past behaviors and other contextual information, since human behavior is partly *influenced* by the past. However, it is not totally *determined* by the past. There is always a certain degree of uncertainty in the prediction; human behaviors are, generally speaking, not fully predictable. 

Social and individual behaviors, therefore, are non-deterministic. A **non-deterministic time series** cannot be fully described by an analytic expression. It has some **random**, or probabilistic component, that prevents its behavior from being explicitly described. It could be possible to say, in probabilistic terms, what its future behavior might be. However, there is always a *residual*, unpredictable, component. A time series may be considered non-deterministic also because all the information necessary to describe it explicitly is not available, although it might be in principle, or because the nature of the generating process, or part of it, is inherently random. We can say that the time series analyzed in social science have always, at least, a stochastic component that makes them not totally deterministic. 

Since non-deterministic time series have a random component, they follow probabilistic rather than deterministic laws. Random data are not defined by explicit mathematical relations, but rather in statistical terms, that is, by probability distributions and parameters such as mean and variance. Non-deterministic time series can be analyzed by assuming that they are manifestations of probabilistic or *stochastic processes*.

<!--chapter:end:03-Definitions.Rmd-->

# Time Series Objects

## Time Series Objects

Every *object* we manipulate in R is characterized by a specific structure. Objects' structures vary depending on the type of object: a *list*, a *matrix*, or a *data.frame*, are different objects with different structures. Every structure has its own manipulation methods. For instance, it can be accessed and analyzed by using different functions and strings of code. 

In R there are many different types of object. To get an overview you can refer to this handbook, to the [R manual](https://cran.r-project.org/doc/manuals/r-release/R-intro.html), or the chapter 5 of [this free online book](https://rc2e.com/datastructures). In this course we are going to learn more about the data structures we have to deal with when conducting time series analysis in R, that is, the structure of time series objects and data sets. 

Time series data sets in R can be represented by different objects. Specific *libraries* (coherent collections of functions) can give different structures to time series data sets.

There are many R libraries for handling and working with time series objects. Some of them are more general and other are useful to perform very specific analysis. [On this page](https://cran.r-project.org/web/views/TimeSeries.html) you can find a comprehensive list of the R libraries for time series analysis.

For now, we just need to know that different libraries can create time series objects with different structures which can be manipulated through different functions. This means that **not all the objects can be analyzed with all the functions**, as well that there is **not always compatibility** between R libraries. 
Many functions have been developed with reference to specific libraries and objects, or require a particular object structure. As a consequence, creating a time series object with a certain structure or a certain library can imply that we can use some functions and perform only a certain type of analysis. In other words, specific type of objects could introduce specific **constrains to data analysis** (and visualization), so it could be wise to plan in advance the necessary analyses, so as to select the necessary libraries and data structure.

We now introduce three types of objects that are commonly used to store and analyze time series data:

*   The **data.frame** (in base R)
*   The **ts** object (in base R)
*   The **xts** object (created through the library *xts*).

We analyze the structures of these objects and their strengths and limitations. In the next chapter, we'll also learn the methods available to visualize them.

### Time Series as Data Frames

Data frames (*data.frame*) are the most common data set structure in R. A data.frame is simply a table *cases by variables* (each row is a case and each column is a variable). 

To see an example of data.frame containing time series data, we can upload a data set containing the number of news articles mentioning the keyword "elections" published by USA news media. I retrieved this data set from [MediaCloud](https://mediacloud.org), a free and open source platform for studying media ecosystem that tracks millions of stories published online. You can download the data set [at this link](https://drive.google.com/file/d/1vzpdPFb_ihBlqqbHxJS7mEvzQ3tNhmEm/view?usp=sharing).

We can upload the *.csv* file by using the function **read.csv**. The main argument of the read.csv function is the path of the file. 

```{r}
elections_news <- read.csv("./data/elections-stories-over-time-20210111144254.csv")
```

By using the function **class** we can see that this is a data.frame. 

```{r}
class(elections_news)
```

We can check the first few rows of the data.frame by using the function **head**, which shows the first few rows of the data set, so as to get an idea of the structure of this simple data.frame. 

```{r}
head(elections_news)
```

This data.frame contains time series data: the first column contains dates, and the other columns contain the values of the observations. We can also see that the data.frame seems to contain daily data, where each row corresponds to a specific day. The data frame also includes, in the column "count", the number of news articles mentioning the keyword "elections", in the column "total_count", the total number of news articles on *all the topics*, and in the column "ratio" the proportion of news articles mentioning the keyword (count/total_count).

The function *head* (and *tail*) can be impractical with data.frame including a lot of columns, so it could be better to use the function **str** to check the structure of the data.frame.

```{r}
str(elections_news)
```

As you can see in the output of the function *str*, the format of the column date is *Factor*. The format is, in this case, automatically attributed by R, but (as we have already said) it can be specified before importing the data.

*Factor* is an appropriate format for categorical variables, but R includes a specific format for dates and times. In this case we have just a date, so we can convert it to a variable of type *date*. We can change the format of the variable by using the function **as.Date**.

```{r}
elections_news$date <- as.Date(elections_news$date)
```

```{r}
str(elections_news)
```

We can also perform the same operation with tidyverse, by using the function **mutate**.

```{r}
library(tidyverse)

elections_news <- elections_news %>%
  mutate(date = as.Date(date))
```

A data.frame is the common format for data sets, including time series data sets. We can do many things with data stored in this format, such as creating plots and performing various types of analysis. However, to handle time series in R there are more specific data formats.

### Time Series as TS objects

The basic object created to handle time series in R is the object of class *ts*. The name stands for "Time Series". 

An example of ts object is already present in R under the name of "AirPassengers", a time series data set in ts format. We can load this data set with the function **data**.

```{r}
data(AirPassengers)
```

By applying the function *class* we can see that this is an object of class ts.

```{r}
class(AirPassengers)
```

AirPassengers is a small data set so we can print all the data set to see its structure, which is an example of the standard structure of a ts object. 

```{r}
AirPassengers
```

By calling the *str* function we get synthetic information on the object.

```{r}
str(AirPassengers)
```

The AirPassengers data set is a univariate time series representing monthly totals of international airline passengers from 1949 to 1960. As every time series, it has a **start date** and an **end date**. It also has a **frequency**, which is the frequency at which the observations were taken. 

All this characteristics differentiate a ts object from a data.frame. The structure of a data.frame lacks the start and the end date, and the frequency value.

The functions **start**, **end**, and **frequency**, can be applied to a *ts* object to check their values. We started by saying that some functions work with some objects but not with other types of objects. This is an example. These functions, indeed, work with *ts* objects just because they are part of their structures, and are arguments usually specified when this kind of object is created. They do not work if applied to a data.frame object, since a data.frame structure does not include the start and end date, nor the frequency of observations. It can be seen that the ts structure is much more specific for time series data. 

```{r}
start(AirPassengers)[1]
end(AirPassengers)[1]
frequency(AirPassengers)[1]
```

Importantly, the frequency of a time series is assumed to be **regular** over time. This applies to time series in general, and not just to ts objects. In this case, the time series starts on January 1949 and ends on December 1969, and has *monthly* frequency. Monthly frequency is indicated in *ts* as "12", meaning 12 months. Indeed, the reference unit of a ts object is a year. So, quarterly data, for instance, have frequency equal to 4.

To create a ts object is necessary to follow specific steps and use specific functions. To exemplify the process of creation of a ts object we take the example of the data contained in the AirPassengers data set, and store them in a data.frame (you don't need to learn how to do that, just copy and paste the code). A data.frame is the data set format you will probably start with, so it can be useful to see how to create a ts object starting from a data.frame.

```{r}
date <- seq.Date(from = as.Date("1949-01-01"), 
                 to = as.Date("1960-12-01"), by="month")
passengers <- as.vector(AirPassengers)

data_frame_format <- data.frame("Date" = date, 
                                "Passengers" = passengers)
```

To create a "ts" time series object starting from a data.frame, we need:

*   To specify which column contains the observations. In this case, the column name is "Passengers". 
*   We then need to specify the start and end date, which in this case are in the format year/month, but can be just years in case of yearly data. The ts format for the start/end date is the following: *c=(YEAR, MONTH)*. The *c* represents the *concatenate* function, and it concatenates the year and the month in a single vector. 
*   Finally, we indicate the frequency of the time series observations. The frequency is specified based on the time period of a year, so in this case we have a frequency equal to 12, because we have monthly observation, meaning that we have 12 observation per year.

```{r}
ts_format <- ts(data = data_frame_format$Passengers, 
                start=c(1949, 01), 
                end=c(1960, 12), 
                frequency = 12)
```

```{r}
ts_format
```

```{r}
class(ts_format)
```

It's important to notice that yearly, quarterly, and monthly data work fine with the *ts* structure, but *more fine grained data create complications and are not totally suitable for a ts structure*. 

This is due to the fact that time series objects require the frequency of observations to be regular and in *ts* the observations have to be regular with reference to a year. Unfortunately, a time series that spans over many years cannot be composed of a constant number of days, since the number of days will be sometimes 365 and other time 366, in case of leap years. This is a limitation of *ts* objects. However, when dealing with monthly data or data with frequency lower than one month (such as quarterly data), *ts* works great.

### Time Series as XTS/ZOO objects

Time series can be stored in object of class xts/zoo. This class of objects is created with the library *xts*, which is related and an extension of the package *zoo* (another package to deal with time series data). As other libraries, it requires to be installed and loaded.

```{r message=FALSE, warning=FALSE}
# install.packages("xts")
library(xts)
```

The *xts* object is more flexible that the *ts* one. 

We can create an *xts* time series by starting from the data.frame we have just created. Similarly to what required by *ts*, we need to specify: 

*   the column of the data.frame (or the vector) containing the data;
*   the column of the data.frame (or the vector) containing the dates/times (which has to be in a date/time format);
*   the frequency of observations.

We can use the data.frame already created with the AirPassengers data to create a new *xts* object.

```{r}
xts_format <- xts(x = data_frame_format$Passengers, 
                  order.by = data_frame_format$Date, 
                  frequency = 12)
```

```{r}
class(xts_format)
```

Also the structure of this object, like the *ts* one, includes the range of dates of the time series, with it starting and ending date.

```{r}
str(xts_format)
```

```{r}
head(xts_format)
```

## Exercise

*   Download the data set [Austrian_Local_Media](https://drive.google.com/file/d/1RKPFOLsIAPLuACcrcB0iiq289CvDDgiT/view?usp=sharing), a data set including  metrics from Facebook pages of Austrian Local Media (monthly observations from January 2015 to December 2020 on quantity of posts and interactions)
*   Set the "beginning_of_interval" to the appropriate "Date" format
*   Create a ts object, called "ts_object_1", using as data values the post_count values
*   Create a xts object, called "xts_object_1", using as data values the post_count values
*   Use the Tidyverse function "mutate" to create a new column with the number of interaction by post (total_interactions/post_count) and call it  "interaction_ratio"
*   Create a ts object, called "ts_object_2", using as data values the interaction_ratio values
*   Create a xts object, called "xts_object_2", using as data values the interaction_ratio values
*   Try to plot these objects, by using the command *plot.ts(ts_object_1)*, *plot.ts(ts_object_2)*, *plot.xts(xts_object_1)*, *plot.xts(xts_object_2)*



<!--chapter:end:04-Time-Series-Objects.Rmd-->

# Plot Time Series

## Plot Time Series Objects

In this lecture we are going to learn how to plot time series data. 

We will take into account three main functions: **ggplot** from the tidyverse library, **plot.ts** from base R, and **plot.xts** from the xts library. Ggplot is probably the most versatile function from the perspective of the graphical results that can be obtained, but also the more complex, while for ordinary visualization, plot.ts is probably the easiest tool.

Plotting time series is an important part of the analysis because it permits to visualize and **explore the data**, both from a **univariate** perspective (focusing on the characteristics of a single time series) and from a **multivariate** perspective (focusing on the characteristics of many time series, and on the relations between them). To visualize and explore the relations between time series, we'll learn to plot a single time series as well as many different time series at once.

## plot.ts

You can visualize a time series by using the function plot.ts() applied to a time series data in a ts format.

An example of ts time series is provided by the AirPassengers dataset, already included in R (you can load the data by running *data("AirPassengers")*).

```{r echo=TRUE}
data("AirPassengers")
plot.ts(AirPassengers)
```

You can add many details to your plot, such as a title, a label for the y axis and for the x axis, change the colors of the plot (use *colors()* to see the list of standard colors in R), and the size of the line. 

```{r echo=TRUE}
plot.ts(AirPassengers, 
     main = "PASSENGERS",
     xlab = "1949-1960 (monthly data)",
     ylab = "Passengers (1000's)",
     col = "violetred3", 
     lwd=5)
```

You can use plot.ts to plot two (or more) time series together, a useful operation to take a look at their relations. The different time series must have the same structure (the same starting date, the same ending date, and the same frequency), and should be stored in the same ts object. 

To merge in one "ts" object two or more time series already in the ts format, you can employ the function **ts.union()**. You can plot both the time series in the same plot, or create two different plots, by using the option **"plot.type"** and specifying **single** or **multiple**.

With **lwd** you control the line width (or the line size):

*   Line types can either be specified as an integer (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) or as one of the character strings "blank", "solid", "dashed", "dotted", "dotdash", "longdash", or "twodash", where "blank" uses ‘invisible lines’ (i.e., does not draw them).

With **lty** you control the line type:

*   The line width, a positive number, defaulting to 1. The interpretation is device-specific, and some devices do not implement line widths less than one.


```{r}
# create a "toy" time series with the same lenght of the AirPassenger one
AirPassengers_2 <- AirPassengers + 100
AirPassengers_3 <- AirPassengers + 300

AirPassengers_multi <- ts.union(AirPassengers, AirPassengers_2, AirPassengers_3)

plot.ts(AirPassengers_multi, 
        main = "Three time series",
        xlab = "TIME", ylab = "VALUES",
        col = c("blue", "red", "black"), 
        lwd=c(1, 1, 1), lty=c(1, 2, 3),
        plot.type = "single")

```

```{r}
plot.ts(AirPassengers_multi, 
        main = "Three time series",
        xlab = "TIME", ylab = "VALUES",
        col = "blue", 
        lwd=4,
        plot.type = "multiple")
```

With the parameter **nc** you can control the number of columns used to display the data.

```{r}
plot.ts(AirPassengers_multi, 
        main = "Three time series",
        xlab = "TIME", ylab = "VALUES",
        col = "orange",
        lwd=4,
        plot.type = "multiple",
        nc=3)
```

To explore long time series it can be useful to focus on a limited time window. To do that, you can subset the data by using the function **window**. It is a function which extracts the subset of data observed between the specified start and end time. You can use the function *window* in the plot.ts function (alternatively, you can create a new object by applying the function *window* first, and then plot the new object).

```{r}
plot.ts(window(AirPassengers, start=c(1950, 01), end=c(1954, 12)), 
     main = "PASSENGERS (1950-1955)",
     xlab = "1950-1955 (monthly data)",
     ylab = "Passengers (1000's)",
     col = "violetred3", 
     lwd=5)
```

In the *window* function, you can also specify a **frequency**, and the series is then re-sampled at the new frequency. For instance, by re-sampling at quarterly frequency, the function keeps the observations made on January, April, July, and October, and by re-sampling at a six-month frequency, it keeps the observations made on January and July.

```{r}
plot.ts(window(AirPassengers, start=c(1950, 01), end=c(1954,12), frequency = 4), 
        main = "PASSENGERS (1950-1955) - QUARTERLY DATA",
        xlab = "1950-1955 (Quarterly data)",
        ylab = "Passengers (1000's)",
        col = "violetred3", 
        lwd=5)
```

You can use the *window* function also with more than one time series.

```{r}
plot.ts(window(AirPassengers_multi, start=c(1950, 01), end=c(1954,12), frequency = 4), 
        main = "PASSENGERS (1950-1955) - QUARTERLY DATA",
        xlab = "1950-1955 (Quarterly data)",
        ylab = "Passengers (1000's)",
        col = "violetred3", 
        lwd=5)
```

To learn something more about the graphical options of plot.ts, you can open and read the help page by using **?plot.ts**. The question mark followed by the name of a function opens the help page of that function.

## plot.xts

To plot a *xts* object we can similarly use the **plot.xts** function.

You can create a xts object with the *xts* function (see the previous chapter), but if you already have a *ts* object, you can also convert it to a *xts* object by using the function **as.xts**

```{r}

AirPassengers_xts <- as.xts(AirPassengers)

plot.xts(AirPassengers_xts,
         main = "PASSENGERS",
         ylab = "Passengers (1000's)", 
         col = "steelblue2", 
         lwd=5)
```

By using **multi.panel=TRUE**, or **multi.panel=FALSE** you can plot all the time series in the same panel or using different panels.

```{r}
AirPassengers_multi_xts <- as.xts(AirPassengers_multi)

plot.xts(AirPassengers_multi_xts,
         main = "PASSENGERS",
         ylab = "Passengers (1000's)", 
         lwd=5, lty=1,
         col = c("blue", "orange", "black"),
         multi.panel = T)
```

```{r}
plot.xts(AirPassengers_multi_xts,
         main = "PASSENGERS",
         ylab = "Passengers (1000's)", 
         lwd=5, lty=1,
         col = c("blue", "orange", "black"),
         multi.panel = F)
```

To **subset** the data, in order to visualize and focus on just one part of the series, instead of the function *window*, you have to write the dates into *squared brackets* as in the examples below.

```{r}
plot.xts(AirPassengers_multi_xts["1950-01/1954-12"], 
         main = "PASSENGERS",
         ylab = "Passengers (1000's)", 
         lwd=5, lty=1,
         col = c("blue", "orange", "black"),
         multi.panel = F)
```


```{r}
plot.xts(AirPassengers_xts["1950-01/1956-06"], 
         main = "PASSENGERS",
         ylab = "Passengers (1000's)", 
         lwd=5, lty=1,
         col = c("blue", "orange", "black"),
         multi.panel = F)
```

You can also change the frequency of the observations by using specific functions in the *xts* library.

By using the function **periodicity** you can find the frequency of the time series.

```{r}
periodicity(AirPassengers_xts)
```

With the function **to.period** you can re-sample the data to "seconds", "minutes", "hours", "days", "weeks", "months", "quarters", and "years". You can only re-sample the data from a higher to a lower frequency, but not from a lower to a higher one. For instance, if you have monthly data, you can aggregate the data in quarterly or yearly data, but you cannot create a weekly or hourly time series. 

The result of the *to.period* function will contain the open (first) and close (last) value for the given period, as well as the maximum and minimum over the new period, reflected in the new high and low, respectively.


```{r}
to.period(AirPassengers_xts, period="years")
```

You can plot all the values, or select a value by using the *square brackets* with a comma, followed by the number of the column you want to plot (see the table above, with 4 columns). This notation is a way to access the columns (and the rows) of a data.frame or matrix. You write the name of the data.frame or the matrix, and the squared brackets indicate the index of the rows, in the first position before the comma, and the index of the columns, in the second position, after the comma. So, for instance, to access the value in the second column and the second row of the data.set "data", you can write data[2,2], and to access the values in the third column and first row, you can write data[1,3]. If you leave a blank space in the column or row space, you get all the values in that column or rows. Therefore, by writing data[,2], or data[,3], you get all the values in the column 2 and 3, respectively. 

```{r}
plot.xts(to.period(AirPassengers_xts, period="years")[,2],
         main = "PASSENGERS",
         ylab = "Passengers (1000's)", 
         lwd=5, lty=1,
         col = c("blue", "orange", "black"),
         multi.panel = F)
```

You can also **re-sample and calculate the average** (or another statistics) for the new period. For instance, in the example we re-sample the data by year and calculate the average. It is also possible to calculate other statistics such as, for instance, the median, just by writing *"median"* instead of "mean".

```{r}
index_years <- endpoints(AirPassengers_xts, on = "year")
AirPassengers_xts_year_avg <- period.apply(AirPassengers_xts, INDEX=index_years, FUN=mean)

plot.xts(AirPassengers_xts_year_avg,
         main = "PASSENGERS (Year Average)",
         ylab = "Passengers (1000's)", 
         lwd=5, lty=1,
         col = "blue",
         multi.panel = F)

```
You can find additional details on *xts* and *plot.xts* by reading the help functions. [At this link](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/xts_Cheat_Sheet_R.pdf) you can find a synthetic presentation of the functions of the xts library.

## ggplot 

```{r include=FALSE}
library(tidyverse)
```

Ggplot2 is the tidyverse library for data visualization. We can use it to create time series plots and many other types of plot. 

```{r include=FALSE}
elections_news <- read_csv("data/elections-stories-over-time-20210111144254.csv")
elections_news$date <- as.Date(elections_news$date)
```

We upload a data set, first, and we set the appropriate time format for the date.

We place our data set ("elections_news") inside the ggplot function. Notice that the ggplot syntax is similar to the tidyverse one, but uses the plus sign instead of the pipe one (%>%).

To create a line plot with ggplot it is necessary to use the **geom_line** function. This function requires two parameters: the data for the x-axis and the data for the y-axis. These parameters have to be written inside the **aes** function. You can also specify the colors and the size of the line. By using additional function, after the plus sign, you can also set the labels for the x- and y-axes, and the title, the subtitle, and the caption of the plot. You can also change the overall aspect of the plot by using one of the **themes** included in the library.

```{r}
ggplot(elections_news) +
  geom_line(aes(x = date, y = ratio), color = "snow4", size = 0.5) +
  ylab("News Articles") +
  xlab("Date") +
  labs(title = "Time Series of News Articles on Elections",
       subtitle = "Data from MediaCloud",
       caption = "Data Analysis II") +
  theme_classic()
```

You can also plot more than one series. For instance, you can create two plots, and then use the function **grid.arrange**, from the library **gridExtra** to combine the plots together.

```{r}
# install.packages("gridExtra")
library(gridExtra)

p1 <- elections_news %>%
  ggplot() +
  geom_line(aes(x = date, y = ratio), col = "black", size = 0.5) +
  ylab("News Articles (ratio)") +
  xlab("Date") +
  ggtitle("MediaCloud Data on Elections (Daily)") 

p2 <- elections_news %>%
  ggplot() +
  geom_line(aes(x = date, y = count), col="red", size=0.5) +
  ylab("News Articles (count)") +
  xlab("Date") +
  ggtitle("MediaCloud Data on Elections (Daily)") 

grid.arrange(p1,p2)

```

You can also plot two or more than two series in the same plot.

```{r}

elections_news <- elections_news %>%
  mutate(time_series_data_2 = count*2,
         time_series_data_3 = count*4)

ggplot(elections_news) +
  geom_line(aes(x = date, y = count), col = "black", size = 0.5) +
  geom_line(aes(x = date, y = time_series_data_2), col = "blue", size = 0.5) +
  geom_line(aes(x = date, y = time_series_data_3), col="red", size=0.5) +
  ylab("") +
  xlab("Date") +
  ggtitle("MediaCloud Data on Elections (Daily)") 


```

To focus on a shorter time window, we can use the **dplyr** function **filter**. Besides filtering the data, we add a function "scale_x_datetime", which control the labels on the x-axis, specifying we want to use monthly labels.

```{r}
elections_news %>%
  filter(date >= "2016-01-01" & date < "2017-01-01") %>%
  ggplot() +
  geom_line(aes(x = date, y = count), col = "black", size = 0.5) +
  scale_x_date(breaks="month", date_labels ="%Y-%m") +
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  ylab("News Articles (Ratio)") +
  xlab("Day") +
  ggtitle("MediaCloud Data on Elections (Monthly) - 2016") 

```

You can also use ggplot to annotate the date. To create annotations in ggplot you can use the function "annotate", to label the data point, and "geom_segment", to trace lines for connecting data points to labels.

You can learn more about annotation in ggplot here: https://ggplot2-book.org/annotations.html 
And about lines here: https://ggplot2.tidyverse.org/reference/geom_segment.html 

```{r}

ggplot(elections_news) +
  geom_line(aes(x = date, y = count), col = "grey50", size = 0.25) +
  ylim(c(0, 15000)) +
  # 1 EVENT
  annotate("label", x = as.Date("2018-11-01"), y = 14500, 
           label = "Midterm Elections\nNovember 2018", color = "white", fill="orange", fontface="bold", size=3) +
  # add a line. You can also use an arrow by adding in geom_segment: 
  # arrow = line(length = unit(0.2, "cm"), ends = "last") 
  geom_segment(aes(x = as.Date("2018-11-01"), xend = as.Date("2018-11-01"), y = 0, yend = 14500), 
               color = "orange", size = 0.2, linetype = 1) +
  # 2 EVENT
  annotate("label", x = as.Date("2019-05-01"), y = 12000, 
           label = "Pennsylvania Elections\nMay 2019", color = "white", fill="orange", fontface="bold", size=3) +
  geom_segment(aes(x = as.Date("2019-05-01"), xend = as.Date("2019-05-01"), y = 0, yend = 12000), 
               color = "orange", size = 0.2, linetype = 1) +
  # 3 EVENT
  annotate("label", x = as.Date("2016-11-01"), y = 12000, 
           label = "Presidential Elections\nNovember 2016", color = "white", fill="orange", fontface="bold", size=3) +
  geom_segment(aes(x = as.Date("2016-11-01"), xend = as.Date("2016-11-01"), y = 0, yend = 12000), 
               color = "orange", size = 0.2, linetype = 1) +
  # 4 EVENT
  annotate("label", x = as.Date("2020-11-01"), y = 12000, 
           label = "Presidential\nElections\nNovember\n2020", color = "white", fill="orange", fontface="bold", size=3) +
  geom_segment(aes(x = as.Date("2020-11-01"), xend = as.Date("2020-11-01"), y = 0, yend = 12000), 
               color = "orange", size = 0.2, linetype = 1) +
  ylab("News Articles") +
  xlab("Date") +
  labs(title = "MediaCloud Data on Elections",
       subtitle = "Peaks annotated with relevant political events",
       caption = "Advanced Data Analysis
                  University of Vienna") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5),
        plot.caption = element_text(face = "italic")) +
  theme_gray()

```



<!--chapter:end:05-Plot-Time-Series.Rmd-->

# Structural Decomposition

## Components of a time series

A time series can be considered composed of 4 main parts: **trend**, **cycle**, **seasonality**, and the **irregular** or remainder/residual part.

```{r  echo=FALSE}
knitr::include_graphics("images/Structure.png")
```


### Trend and Cycle

The **Trend** component is the longest-term behavior of a time series. The simplest model for a trend is a linear increase or decrease, but the trend has not to be linear. In the AirPassengers time series there is a clear upward, linear trend.

```{r}
library(xts)
data("AirPassengers")
AirPassengers_xts <- as.xts(AirPassengers)
plot.xts(AirPassengers_xts)
```  

### Stochastic and Deterministic Trend

There is a distinction between **deterministic** and **stochastic** trends.

A **deterministic trend** is a *fixed function of time*. If a series has a deterministic trend, the increase (or decrease) in the value of the series is a function of time. For instance, it may appear to grow or decline steadily over time. A deterministic trend can be linear, as well as non linear. Deterministic trends have plausible explanations (for example, a deterministic increasing trend in the data may be related to an increasing population). A series with deterministic trend is also called *trend stationary*.

A **stochastic trend** wanders up and down or shows change of direction at unpredictable times. Time series with a stochastic trend are also said to be *difference stationary*. An example of stochastic trend is provided by the so-called *random walk* process.

**Random Walk** is a particular time series process in which the current values are combinations of the previous ones ($x_t = x_{t-1} + w_t$, where $x_{t-1}$ is the value immediately before $x$, and $w_t$ is a random component). The resulting time series is characterized by a discernible pattern over time which is not exactly predictable (*stochastic trend*). Starting from the same initial point, the same process can generate different time series.

```{r}
set.seed(111)
Random_Walk <- arima.sim(n = 500, model = list(order = c(0,1,0)))
plot.ts(Random_Walk, ylab = expression(italic(x)[italic(t)]), main = "Random Walk")

set.seed(555)
Random_Walk <- arima.sim(n = 500, model = list(order = c(0,1,0)))
plot.ts(Random_Walk, ylab = expression(italic(x)[italic(t)]), main = "Random Walk")
```
  
A paper on [The effectiveness of social distancing in containing Covid-19](https://www.tandfonline.com/doi/full/10.1080/00036846.2020.1789061) shows an example of stochastic trend and complex, deterministic nonlinear trends represented by polynomials.

```{r echo=FALSE, fig.cap="Figure 5 shows the actual number of Covid-19 cases recorded in the UK up to 17 June 2020. The stochastic trend estimated earlier is superimposed on the actual observations and so are two deterministic nonlinear trends represented by polynomials of degrees 5 and 6. We can see that the stochastic trend captures the slow growth at the beginning of the sample period whereas the two deterministic trends do not. The stochastic trend is better also at capturing the sharp increase represented by observation number 72.. (original caption)"}
knitr::include_graphics("images/covid-trends.jpeg")
```  

The trend component of the series is often considered along with the **cyclic** one (*trend-cycle*). The **cyclical** component is represented by fluctuations (rises and falls) not occurring at a fixed frequency. The cycle component is therefore different from the seasonal variation (see below) in that it does not follow a fixed calendar frequency. 

### Seasonality

The **Seasonal** component is a repeated pattern occurring at a fixed time period such as the time of the year or the day of the week (the frequency of seasonality, which is always a fixed and known frequency). There is a clear seasonal variation in the AirPassenger time series: bookings were highest during the summer months of June, July, and August and lowest during the autumn/winter months.

```{r}
plot.xts(AirPassengers_xts["1954-01/1955-12"])
```  

It is possible to plot the distributions of data by months by using the function *boxplot* and *cycle*, to visualize the increasing number of passengers during the summer months. In this case, *cycle* is used to refer to the positions of each observation in the (yearly, in this case) cycle of observations (every year is considered to be a cycle of 12 observations).

```{r}
boxplot(AirPassengers ~ cycle(AirPassengers))
```

The library *forecast*, an R package that provides methods and tools for displaying and analysing time series forecasts, includes a function to create a "polar" seasonal plot.

```{r}
# install.packages("forecast")
library(forecast)
ggseasonplot(AirPassengers, polar=TRUE)
```

An example of weekly seasonality can be found in the COVID-19 statistics. 

```{r echo=FALSE, fig.cap="Covid statistics (Google)"}
knitr::include_graphics("images/covid-italy.png")
```  

Cyclic and seasonal variations can look similar. Both cyclic and seasonal variations have 'peak-and-trough' patterns. The main difference is that in seasonal patterns the period between successive peaks (or troughs) is constant, while in cyclical patterns the distance between successive peaks is irregular.

### Residuals

The **irregular** or remainder/residual component is the random-like part of the series. 

In general, when we fit mathematical models to time series data, the *residual* error series represents the discrepancies between the fitted values, calculated from the model, and the data. A good model encapsulates most of the deterministic features of the time series, and the residual error series should therefore appear to be a realization of independent random variables from some probability distribution.

The analysis of residuals is thus important to judge the fit of a model. In this case, its residual error series appears to be a realization of *independent random variables*. Often the random variable is conceived as a Gaussian random variable. We'll return to these topics in the last part of the chapter.

```{r echo=TRUE}
AirPassengers_Random <- decompose(AirPassengers, type="multiplicative")$random
```

```{r}
par(mfrow = c(2,1))
plot(AirPassengers_Random, xlab="", ylab="")
hist(na.omit(AirPassengers_Random), main = "", xlab="", ylab="")
```


## Structural decomposition

Along with the analysis of the peaks (see previous chapter), analyzing a time series based on these structural parts can be an important exploratory step. It helps understanding the likely causes of the series features and formulate an appropriate time series model. For instance, in the case of the AirPassengers series, we could hypothesize that the *increasing trend* is due to the rising prosperity in the aftermath of the Second World War, greater availability of aircraft, cheaper flights due to competition between airlines, and an increasing population. The *seasonal* variation, instead, seems to coincide with vacation periods.

**Decomposition methods** try to identify and separate the above mentioned parts of a time series. Usually they consider together the trend and cycle (*trend-cycle*) - the longer-term changes in the series - and the *seasonal* factors - periodic fluctuations of constant length happening at a specific calendar frequency).

There are two main ways through which these elements can be combined together: in the **additive** and the **multiplicative** form:

*   The **additive model** ($x_{t} = m_{t} + s_{t} + z_{t}$, where $x_{t}$ is the observed time series, $m_{t}$ is the trend-cycle component, $s_{t}$ is the seasonal component and $z_{t}$ is the residual) is useful when the seasonal variation is relatively constant over time
*   The **multiplicative model** ($x_{t} = m_{t} * s_{t} * z_{t}$) is useful when the the seasonal effects tends to increase as the trend increases.

There are different methods to decompose a time series. Here we consider the function **decompose**. This function is defined as *Classical Seasonal Decomposition by Moving Averages*. The function *decompose* uses a **moving average (MA)** approach to filter the data. Moving average is a classical approach to extract the trend from a time series by averaging out the seasonal effects. 

### Moving Average 

Moving average is a process that replaces each value $x_{t}$ with an average of its current value $x_{t}$ and its immediate neighbors in the past and future. For instance, it is possible to calculate a simple moving average by using the closest neighbors of a point, as follows: $x_{t} = \frac{1}{3} (x_{t-1} + x_{t} + x_{t+1})$. This is called *Centered Moving Average*.

The number of neighbors in the past and future is determined by the analyst and is also called *width of the window*. The time window for the moving average is chosen by considering the frequency of the data and their seasonal effects. For instance, monthly data, which are supposed to show monthly seasonality (for instance, in the AirPassengers data there are more passengers during the summer months), can be averaged by using a period of 12 months (six months before and after each point. Since we have an even number of months, some other calculation are necessary. For instance, the moving average value for July, is calculated by averaging the average of January up to December, and the average of February up to January. R functions do this for you).

The centered moving average is an example of a **smoothing** procedure that is applied retrospectively to a time series with the objective of identifying an underlying signal or trend. Smoothing procedures usually use points before and after the time at which the smoothed estimate is to be calculated. A consequence is that the smoothed series will have *some points missing at the beginning and the end* unless the smoothing algorithm is adapted for the end points. In the case of monthly data, for instance, the moving average filter determines the lost of the first and last six months of data.

Smoothing procedures like moving average, allows the main underlying trend to emerge by filtering out seasonality and noise, so they are used to get an idea of the long-term underlying process of a time series.

```{r}
elections_news <- read_csv("data/elections-stories-over-time-20210111144254.csv", 
                           col_types = cols(date = col_date(format = "%Y-%m-%d")))

en <- as.xts(x = elections_news$ratio, order.by = elections_news$date)

en2 <- rollmean(en, k = 2)
en4 <- rollmean(en, k = 4)
en8 <- rollmean(en, k = 8)
en16 <- rollmean(en, k = 16)
en32 <- rollmean(en, k = 32)

enALL <- merge.xts(en, en2, en4, en8, en16, en32)

# notice the NA elements increasing as the width of the moving average increase
head(enALL, 10) 
```

```{r}
plot.xts(enALL["2015-01-01/2016-01-01"], multi.panel = T)
```

### Decompose

To apply the function *decompose*, we need a **ts** object.

Considering the AirPassengers time series, since the seasonal effect tends to increase as the trend increases, we can use a multiplicative model.

```{r}
AirPassengers_dec <- decompose(AirPassengers, type="multiplicative")
plot(AirPassengers_dec)
```

As an example of *additive model* we can use data from the "Seatbels" data set. 

```{r}
data("Seatbelts")
seatbelts <- Seatbelts[,5]
plot.ts(seatbelts)
```

```{r}
seatbelts_dec <- decompose(seatbelts, type="additive")
plot(seatbelts_dec)
```

The **residual** part of the model should be (approximately) **random**, which indicates that the model explained (most of) the significant patterns in the data (the *"signal"*), leaving out the *"noise"*. 

```{r}
par(mfrow = c(1,2))
plot.ts(seatbelts_dec$random, main="Residuals", ylab="")
hist(seatbelts_dec$random, breaks = 25, freq = F, main = "Histogram")
lines(density(seatbelts_dec$random, na.rm = T), col="red")
```

We can re-create the original time series starting from its elements (we don't actually need to do that, it is just for illustrative purposes). 

```{r}
par(mfrow=c(2,1))
plot(AirPassengers_dec$trend * AirPassengers_dec$seasonal * AirPassengers_dec$random,  
     xlim=c(1950, 1960), ylim=c(0,600), main = "'Re-composed' series", ylab="")

plot(AirPassengers, xlim=c(1950, 1960),ylim=c(0,600),  main = "Original series", ylab="")
```

### Compare Additive and Multiplicative Models

Sometimes it could be hard to choose between additive or multiplicative models. In general, when seasonality or the variation around the trend-cycle component change proportionally to the level of the series (the trend, or the average), the multiplicative model works better. However, it might be difficult to assess the variability of the series from a time series plot.

Exploratory methods, such as representing the variability in the data through [box plots](https://en.wikipedia.org/wiki/Box_plot) can help. The following "custom" function (which I called  *ts.year.boxplot*) takes as argument a time series *ts*, and shows the [spread](https://en.wikipedia.org/wiki/Statistical_dispersion) of the data by year.

```{r}
# Run the code to "create" the function 
ts.year.boxplot <- function(ts) {
  
  ts %>%
    fortify() %>%
    mutate(x = substring(x, 1, 4)) %>%
    mutate(x = as.Date(x, "%Y")) %>%
    mutate(year = format(as.Date(cut(x, breaks ="year")), "%Y")) %>%
    ggplot() + 
    geom_boxplot(aes(x=year, y=y, group=year)) +
    ylab("") +
    theme_linedraw()
}
```

A multiplicative time series like the *AirPassengers* shows a clear increasing spread as the level of the series goes up:

```{r}
# Apply the function
ts.year.boxplot(AirPassengers)
```

The box plots of an additive time series looks more regular. There is no evident systematic  

```{r}
ts.year.boxplot(seatbelts)
```

Below you can find another function (I called it *compare.decomposition.methods*) that compares the additive and multiplicative decomposition models applied to the same *ts* series. It creates plots of residuals (the time series plot, histogram, acf and pacf plots) and (roughly) measures the total residuals and residuals' autocorrelation (lower values are better). It also create a plot showing the different fit of the adittive and multiplicative model to the data, and includes the boxplot introduced above. We can try looking at these plots and the total autocorrelation measure to get further hints into the most appropriate method.

```{r}
compare.decomposition.methods <- function(ts){

    boxplot_ts <- ts %>%
    fortify() %>%
    mutate(x = substring(x, 1, 4)) %>%
    mutate(x = as.Date(x, "%Y")) %>%
    mutate(year = format(as.Date(cut(x, breaks ="year")), "%Y")) %>%
    ggplot() + 
    geom_boxplot(aes(x=year, y=y, group=year)) +
    ylab("") +
    theme_linedraw()
  
  # decompose the series with both the methods
  xad <- decompose(ts, type = "additive")
  ymu <- decompose(ts, type = "multiplicative")
  
  # plots
  print(boxplot_ts)
  
  par(mfrow=c(1,1))
  plot(ts, main="ADDITIVE (BLUE) - MULTIPLICATIVE (RED)")
  lines(xad$seasonal+xad$trend, col="blue", lty=2)
  lines(ymu$seasonal*ymu$trend, col="red", lty=2)
  
  par(mfrow=c(1,2))
  
  am <- as.vector(xad$seasonal)+as.vector(xad$trend)
  am <- ts(am, start = c(start(ts)[1], start(ts)[2]), 
         end = c(end(ts)[1], end(ts)[2]),
         frequency = frequency(ts))
  am <- ts.union(am, ts)
  am <- am[complete.cases(am),]
  
  mm <- as.vector(ymu$seasonal)*as.vector(ymu$trend)
  mm <- ts(mm, start = c(start(ts)[1], start(ts)[2]), 
         end = c(end(ts)[1], end(ts)[2]),
         frequency = frequency(ts))
  mm <- ts.union(mm, ts)
  mm <- mm[complete.cases(mm),]
  
  plot(as.vector(am[,1]), as.vector(am[,2]), xlab="ADDITIVE")
  plot(as.vector(mm[,1]), as.vector(mm[,2]), xlab="MULTIPLICATIVE")

  par(mfrow=c(2,4))
  plot(xad$random, main="", ylab="ADDITIVE")
  hist(xad$random, main="", ylab="")
  acf(na.omit(xad$random), main="", ylab="")
  pacf(na.omit(xad$random), main="", ylab="")

  plot(ymu$random, main="", ylab="MULTIPLICATIVE")
  hist(ymu$random, main="", ylab="")
  acf(na.omit(ymu$random), main="", ylab="")
  pacf(na.omit(ymu$random), main="", ylab="")


  # Sum of squares of residual auto-correlation (acf)
  cat(paste(
    "###################################\nTOTAL AUTOCORRELATION (ABSOLUTE VALUES)\n###################################",
            "\nADITTIVE MODEL = ", 
    round(sum(abs(acf(na.omit(xad$random), plot = F)$acf)),2),
    "\nMULTIPLICATIVE MODEL = ", 
    round(abs(sum(acf(na.omit(ymu$random), plot = F)$acf)),2),
        "\n\n###################################\nSUM OF RESIDUALS (ABSOLUTE VALUES)\n###################################",
    "\nADITTIVE MODEL = ", 
    round(sum(abs(scale(xad$random)), na.rm=T),2),
    "\nMULTIPLICATIVE = ", 
    round(sum(abs(scale(ymu$random)), na.rm=T),2)
    ))
}
```

In this case, the multiplicative model looks better.

```{r message=FALSE, warning=FALSE}
compare.decomposition.methods(AirPassengers)
```

## Adjust time series 

Sometimes the analyst is not interested in the trend or in the seasonal variation in the data, and might want to remove them, in order to let other underlying process to emerge more clearly. Other times, some components of time series can be misleading, leading to inflated or spurious correlations, and can be preferable to remove them before proceding with the analysis.

### Seasonal adjusted data

It is common to find seasonally adjusted data, that is time series from which the seasonal component has been removed. This happen quite often in economics, for instance (but in other disciplines as well), where certain growing trends can be considered trivial, and explained based on solid theory. Other parts of the series are instead considered more important, and removing the seasonal component allow them to emerge more clearly, as summarized by [Granger, C. W. (1978), *Seasonality: causation, interpretation, and implications*. In Seasonal analysis of economic time series](https://www.nber.org/system/files/chapters/c4321/c4321.pdf):

>Presumably, the seasonal is treated in this fashion, because it is economically uninportant, being dull, superficially easily explained, and easy to forecast but, at the same time, being statistically important in that it is a major contributor to the total variance of many series. The presence of the seasonal could be said to obscure movements in other components of greater economic significance. (...) It can be certainly be stated that, when considering the level of an economic variable, the low frequency components (the trend-cycle, ed.) are usually both statistically and economically important. (...) Because of their dual importance, it is desirable to view this component as clearly as possible and, thus, the interference from the season should be removed. (...) the preference for seasonally adjusted data is so that they can more clearly see the position of local trends or the place on the business cycle. It is certainly true that for any series containing a strong season, it is very difficult to observe these local trends without seasonal adjustment.

Moreover, seasonality can lead to spurious correlations:

>(...) if the relationship between a pair of economic variables is to be analyzed, it is obviously possible to obtain a spurious relationship if the two series contain important seasonals. By adjusting series, one possible source of spurious relationship is removed. 

However, adjust for seasonality a series is application specific, and sometimes this part of the series can be of interest:

>Firms having seasonal fluctuations in demand for their products, for example, may need to make decisions based largely on the seasonal component (...) and a local government may try to partially control seasonal fluctuations in unemployment. (...) Only by having both the the adjusted and the unadjusted data available can these potential users gain the maximum benefit from all of the effort that goes into collecting the information.

There are many different methods to adjust data for seasonality. A simple approach is based on the results of the decomposition process, and consists in substracting (in the case of an addittive decomposition model) the seasonal component from the original series, or dividing the original series by the seasonal component (in the case of a multiplicative model).

```{r}
data("Seatbelts")
seatbelts <- Seatbelts[,5]
seatbelts_dec <- decompose(seatbelts, type="additive")

seatbelts_deseason <- seatbelts - seatbelts_dec$seasonal

seat <- ts.intersect(seatbelts, seatbelts_deseason)

plot.ts(seat, 
        plot.type = "single",
        col = c("red", "blue"),
        main = "Original (red) and Seasonally Adjusted Series (blue)")
        
```

Adjust for seasonal variations makes it possible to observe potentially noteworthy fluctuations. In the case of the AirPassengers data, for instance, the seasonally adjusted plot shows more clearly an anomaly in the year 1960 that was not noticeable in the raw data.

```{r}
AirPassengers_decompose <- decompose(AirPassengers, type="multiplicative")
AirPassengers_seasonal <- AirPassengers_decompose$seasonal
AirPassengers_deseasonal <- (AirPassengers/AirPassengers_seasonal)

AirPass <- ts.intersect(AirPassengers, AirPassengers_deseasonal)

plot.ts(AirPass, 
        plot.type = "single",
        col = c("red", "blue"),
        lty = c(3,1),
        main = "Original (red) and Seasonally Adjusted Series (blue)")
```

Below you can find another example with data from social media (you can download them here [art1](https://drive.google.com/file/d/16NyBB1YOICSQ8lmZpcq62ahkJff9Tx6E/view?usp=sharing), and [art2](https://drive.google.com/file/d/1UWodHEAAphThP-jkALA5u1qEbTBPqHiE/view?usp=sharing)), consisting in posts published by pages of news media.

```{r message=FALSE, warning=FALSE}
art1 <- read_csv("data/art1.csv")
art2 <- read_csv("data/art2.csv")

art1_summary <- art1 %>%
  mutate(post_created_date = as.Date(post_created_date)) %>%
  complete(post_created_date = seq.Date(min(post_created_date), 
           max(post_created_date), by = "day")) %>%
  group_by(post_created_date) %>%
  summarize(posts = n())

art2_summary <- art2 %>%
  mutate(post_created_date = as.Date(post_created_date)) %>%
  complete(post_created_date = seq.Date(min(post_created_date), 
           max(post_created_date), by = "day")) %>%
  group_by(post_created_date) %>%
  summarize(posts = n())

art1_ts <- ts(data = art1_summary$posts, frequency = 7)
art2_ts <- ts(data = art2_summary$posts, frequency = 7)

art <- ts.intersect(art1_ts, art2_ts)

art1_dec <- decompose(art1_ts)
art2_dec <- decompose(art2_ts)

art1_seas <- art1_dec$seasonal
art2_seas <- art2_dec$seasonal

art1_deseas <- art1_ts - art1_seas
art2_deseas <- art2_ts - art2_seas

art_des <- ts.intersect(art1_deseas, art2_deseas)

art_all <- ts.intersect(art, art_des)

plot.ts(art_all, 
        plot.type = "single",
        col = c("red", "blue", "red", "blue"),
        lty = c(3,3,1,1),
        main = "Original (red) and Seasonally Adjusted Series (blue)")

```

By calculating a simple correlation between the original series and the de-seasonalized series, it can be observed that the correlation coefficients changes.

```{r message=FALSE, warning=FALSE}
cor(art)[1,2]
cor(art_des)[1,2]
```

### Detrended series

As the series can be asjusted for seasonality, they can be also adjusted for trend based on the same reasons. A similar process can also be used to remove the trend from the data, in particular in the case of a *deterministic* trend. 

In the case of a *stochastic* trend, instead, the usual practice is to detrend the data through **differencing**. Differecing means taking the first difference of consecutive points in time $x_t$ - $x_{t-1}$. In this way, the resulting series represents the *relative change from one point in time to another*.

```{r}
Random_Walk <- arima.sim(n = 500, model = list(order = c(0,1,0)))

Random_Walk_diff <- diff(Random_Walk)

plot.ts(Random_Walk,
        main = "Random Walk", 
        col = "blue", ylab="")

plot.ts(Random_Walk_diff, 
        main = "Differenced Random Walk", 
        col = "blue", ylab="")
```

Detrending a time series can be important before applying some statistical techniques, for instance before calculating the correlation between two time series. Time series with a trend component can reveal spurious correlations, since correlations may exist just because two variables are trending up or down at the same time. By detrending the time series, it can be more appropriately measured if the change in one time series over time is related to the change in another time series. 

Detrending time series is also used when researchers consider irrelevant the trend. This is the case when the trend is considered an obvious characteristic of the process. For instance, economists can take for granted that there is an increasing trend in GDP due to inflation, and thus they may want to "clean" the data to eliminate this trivial trend. They are more interested in deviations from the growth, than in the growth that they consider a "normal" characteristic of the process.

There are also statistical tests to ascertain the presence of a trend.

A *monotonic* trend can be detected with the **Mann–Kendall trend test**. The null hypothesis is that the data come from a population with independent realizations and are identically distributed. For the two sided test, the alternative hypothesis is that the data follow a monotonic trend (read the help: *?mk.test*). The function to calculate this test is included in the package "trend".

```{r}
# install.packages("trend")
library(trend)
mk.test(AirPassengers, alternative = "greater")
```

### Other Decomposition Methods in R

There are many different methods to decompose (and adjust) time series. Besides the classic *decompose* function, the following can be mentioned:

[**STL**](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/stl.html) (in the base-R *stats* library): Decompose a time series into seasonal, trend and irregular components using loess

[**X11**], a method for decomposing quarterly and monthly data developed by the US Census Bureau and Statistics Canada, and the [**SEATS**] methods, implemented in the [*seasonal*](http://www.seasonal.website/seasonal.html) package.


## White Noise and Stationarity

We said that the **residual** part of the model should be (approximately) **random**, which indicates that the model explained most of the significant patterns in the data (the *"signal"*), leaving out the *"noise"*. 

The standard model of independent random variation in time series analysis is known as **white noise** (a term coined in an article published in Nature in 1922, where it was used to refer to series that contained all frequencies in equal proportions, analogous to white light). The charts below show how a white noise process looks like. 

```{r echo=FALSE}
White_Noise <- arima.sim(n = 500, model = list(order = c(0,0,0)))
plot.ts(White_Noise, ylab = expression(italic(x)[italic(t)]), main = "White Noise")

hist(White_Noise, main = "", freq = F)
lines(density(White_Noise), col="red")

```

When we introduced the concept of deterministic and stochastic trend, we said that the series showing the first type of trend are also called **trend stationary**, and the series showing the second type of trend are also called **difference stationary**. Both these names refer to the concept of **stationarity**. 

A process is *stationary* if it is homogeneous, that is, if it has no distinguished points in times or, in other words, its statistical qualities are the same for any point in time. There are more or less stringent definition of stationarity (*strict and weak stationarity*), and the most used for practical purposes is the so-called **weak-stationarity**. In this sense, a time series is said to be stationary if there is:

*    **no trend** (no systematic change in mean, that is, time invariant mean), and no seasonality (no periodic variations);
*   **no change in variance** over time (time invariant variance);
*   **no auto-correlation** (we’ll return to this topic in the next chapters)

*White noise is an example of stationary time series*. As you can see in the chart above, white noise time series is pretty regular, the mean is always the same (0) and there are no changes in variance over time: the plot looks much the same at any point in time!

Also through differencing, the series can achive a stationary form. This is the case, in particular, of the series with a stochastic trend, that are also called *difference-stationary*, exactly because through differencing they become stationary. 

```{r}
plot.ts(Random_Walk_diff, 
        main = "Differenced Random Walk", 
        col = "blue", ylab="")
```

The process through which stationarity is reached is also called *Pre-Whitening*, and it can be used as a pre-processing phase before conducting correlation and regression analysis:

>Once the form(s) of serial dependency that best account for the series are identified, they are removed from the series. This is called *prewhitening* and is used to produce a series that is a “white noise” process (i.e., a process that is free of serial dependency with each value statistically independent of other values in the series). Once pre-whitening is accomplished the values of that series can be correlated with, used to predict, or predicted from, the values in other contemporaneous time series (usually also pre-whitened) representing other variables of interest. By removing serial dependency, the pre-whitening process  makes these analyses free of correlated errors. It also removes the possibility that a common temporal trend or pattern is a confounding explanation for the observed association between the two variable series (VanLear, ["Time Series Analysis"](https://us.sagepub.com/en-us/nam/the-sage-encyclopedia-of-communication-research-methods/book244974))

The logic behind the process and the importance of white noise is also well explained  in these sentences:

>This "residual" part of the data, indeed, can be used as a dependent variable, giving the analyst confidence that any time series properties in the data will not account for any observed correlation between the covariates and the dependent variable. In the univariate context, the white noise process is important because it is what we would like to “recover” from our data – after stripping away the ways in which a univariate
series can essentially explain itself. By removing the time series properties of our data, leaving only white noise, we have a series that can then be explained by other sources of variation. Another way to conceptualize white noise is as the exogenous portion of the data-generating process. Each of our data series is a function of those forces that cause the series to rise or fall (the independent variables we normally include to test our hypotheses) and of time series properties that lead those forces to
be more or less "sticky". After we filter away those time series properties, we are left with the forces driving the data higher or lower. We often refer to these forces as **shocks** – and these shocks then reverberate in our data, sometimes for a short spell or a long spell or even infinitely. The goal of time series analysis is to separately model the time series properties (the reverberations) so the shocks (i.e., the white noise) can be captured. (Box-Steffensmeier, J. M., Freeman, J. R., Hitt, M. P., & Pevehouse, J. C. (2014). Time series analysis for the social sciences. Cambridge University Press.)


<!--chapter:end:06-Structural-Decomposition.Rmd-->

# Correlations and ARIMA

## Auto-Correlation (ACF and PACF)

In the previous chapter we said that a time series is said to be stationary if there is:

*    **no trend** (no systematic change in mean, that is, time invariant mean), and no seasonality (no periodic variations);
*   **no change in variance** over time (time invariant variance);
*   **no auto-correlation** (we’ll return to this topic in the next chapters)

Auto-correlation or serial correlation is an important characteristic of time series data and can be defined as the *correlation of a variable with itself at different time points*.

Autocorrelation has many consequences. It prevents us to use traditional statistical methods such as linear regression, which assume that the observations are independent from each other. In presence of autocorrelation, the estimated standard errors of the parameter estimates will tend to be less than their true value. This will lead to erroneously high statistical significance being attributed to statistical tests (the *p* values will be smaller than they should be).

In this section we introduce an important tool for the diagnosis of the properties of a time series, including autocorrelation: the **correlogram**. The accurate study of correlogram is a common step in many time series analysis procedures.

### Correlogram: ACF and PACF

The correlogram is a chart that presents one of two statistics: 

*   **the autocorrelation function (ACF)**.The ACF statistic measures the correlation between $x_t$ and $x_{t+k}$ where *k* is the number of lead periods into the future. It measures the correlation between any two points based on a given interval. It is not strictly equivalent to the Pearson product moment correlation. In R, ACF is calculated and visualized with the function "acf";
*   **the partial autocorrelation function (PACF)**. The PACF(k) is a measure of correlation between times series observations that are k units apart, after the correlation at intermediate lags has been controlled for or "partialed" out. In other words, the PACF measures the correlation between $x_t$ and $x_{t+k}$ after it has stripped out the effect of the intermediate *x*’s. In R, the PACF is calculated and visualized with the function "pacf". It is useful to detect correlations that are not evident in ACF. 

Let's consider, as an example, the correlogram of a random walk process. We know that this is a particular time series process in which the current values are combinations of the previous ones ($x_t = x_{t-1} + w_t$, where $x_{t-1}$ is the value immediately before x, and $w_t$ is a random component). The resulting time series is characterized by a discernible pattern over time which is not exactly predictable (*stochastic trend*). The ACF of a random walk time series, indeed, shows a correlation between values in the series: even values not so close are notwithstanding correlated.


```{r}
Random_Walk <- arima.sim(n = 499, model = list(order = c(0,1,0)))

acf(Random_Walk)
```

Instead, the PACF, which removes the correlations between intermediate values, shows a correlation at lag 1, that is, it shows that the overall correlation depends on consequent values. The dotted blue lines signal the boundaries of statistical significance.

```{r}
pacf(Random_Walk)
```

We can clearly visualize the auto-correlation by using a simple scatterplot, by plotting two consecutive lines of points.

```{r}
plot(Random_Walk[1:499], Random_Walk[2:500])
```

The ACF or PACF of a white noise process is very different. We know that white noise is a stationary process, without distinguishable points in time and no correlation between points. Indeed, the ACF of white noise shows no correlation (the only line above statistical significance is at zero, which is nothing to be worried about, since it just means that each point is correlated with itself). 

```{r}
White_Noise <- arima.sim(n = 500, model = list(order = c(0,0,0)))

acf(White_Noise)
```

In the PACF we can see that there is nothing above the dotted line (which means that there is nothing statistically significant).

```{r}
pacf(White_Noise)
```

If we plot two consecutive lists of points by using a scatterplot, we can see there is no serial correlation (no pattern is visible):

```{r}
plot(White_Noise[1:499], White_Noise[2:500])
```

## ARIMA models

The ACF and PACF plots can be used to diagnose the main characteristics of a time series and find a proper statistical model. We talk about univariate models, since they are models to describe a single time series. Univariate time series can be modeled as **Auto Regressive (AR), Integrated (I), and Moving Average (MA) processes**. These models are synthesized using the acronym **ARIMA**. When a seasonal (S) component is also taken into account, we also use the acronym *SARIMA*.

### Auto Regressive (AR) models 

We just said that a time series is often characterized by auto-correlation, so we can clearly deduce that we can model it by using a regression model, that is, by regressing the time series on its past values. In this way we have an **auto-regressive model**: a regression of $x_{t}$ on past terms $x_{t-k}$ from the same series.

In time series analysis, past terms $x_{t-k}$ from a same series are called **lags**. The lagged values of a time series are its delayed values, where the delay can be of an arbitrary amount of time $k$. For instance, considering a simple series of 4 data points distributed from time ${t+0}$ (first data point) to time ${t+3}$ (last data point) ${x_{t+0}, x_{t+1}, x_{t+2}, x_{t+3}}$, the corresponding lagged series, assuming $k=1$, is ${NA, x_{t+0}, x_{t+1}, x_{t+2}}$. Notice that the first data point is missing since there is no data point behind it, and the other data points are shifted one time point ahead. 

An auto-regressive (AR) model can be described as follows (the $\alpha$ are coefficients, $t$ are time points, $w$ is a random component or white noise):

$$
{x_t} = \alpha x_{t-1} + \alpha x_{t-2} + \alpha x_{t-k} + {w_t}
$$

```{r}
AR_1 <- arima.sim(n = 500, list(order = c(1,0,0), ar = 0.90))
plot(AR_1, main = "AR(1)")
```

The ACF of an autoregressive process typically shows a slow and gradual decay in autocorrelation over time.

```{r}
acf(AR_1)
```

The PACF of an autoregressive process shows a peak in correspondence with the order of the model. In the case of an AR(1) the peak is at time 1.

```{r}
pacf(AR_1)
```

In the case of an AR(3) the peak is at time 1, 2, and 3.

```{r}
AR_3 <- arima.sim(n = 500, list(order = c(3,0,0), ar = c(0.3, 0.3, 0.3)))
plot(AR_3, main = "AR(3)")

pacf(AR_3)
```

Now we can see that the **random walk** process we have seen above is a particular case of auto-regressive model. In a  random walk process, each value is the previous one plus a random part:

$$
x_t = x_{t-1} + w_t
$$

Thus each point $x_t$ is correlated with the previous one $x_{t-k}$ where the lag value $k$ is equal to 1 (${k=1}$). Therefore, a random walk process is an auto-regressive model of **order 1**, since just 1 lag is taken into consideration in the auto-regressive model (and with $\alpha = 1$). The order of an auto-regressive model is indicated by parenthesis, e.g.: **AR(1)**.

The AR process can have different characteristics (and different ACF and PACF) based on the parameters.

### Moving Average (MA) models 

We already know Moving Average as a method to smooth time series and detect a trend. When referring to Moving Average as a process (MA), we refer to a process in which the values of the series are a function of a **weighted average of past errors**. In other terms, a moving average (MA) process is a linear combination of the current white noise term and the $q$ most recent past white noise terms:

$$
{x_t} = w_t + \beta w_{t-1} + ... + \beta w_{t-q}
$$

The order of a MA process indicates the lags of white noise taken into account in the model (e.g: *MA(3)*).

```{r}
MA_3 <- arima.sim(n = 500, list(order = c(0,0,3), ma = c(0.3, 0.3, 0.3)))
plot(MA_3, main = "MA(3)")
```
The ACF plot of a MA process shows a more clear cut-off after the term corresponding to the order ot the process. It is different from the ACF of an AR process, which shows a more gradual decay. 

```{r}
acf(MA_3)
```

The PACF of a MA process shows an up-and-down movement and does not shut off, but instead tapers toward 0 in some manner. 

```{r}
pacf(MA_3)
```

### Integrated (I) process

An integrated process is a non-stationary time series process that becomes stationary when transformed by **differencing**. In other words, an integrated process is a difference-stationary process, that is a process with a stochastic trends (see the previous chapter). 

```{r}
I_1 <- arima.sim(n = 500, list(order = c(0,1,0)))
plot(I_1, main = "I(1)")
```
```{r}
plot(diff(I_1), main = "I(1) after 'differencing'")
```

### Seasonal (S) models

We already introduced the seasonal model. For instance, a dataset showing a seasonal component is  AirPassengers. The seasonality appears in the yearly fluctuations in the ACF and in the spikes occurring at 12 months from each other in the PACF.

```{r}
data("AirPassengers")

# this function par(mfrow=c(..., ...))
# is used to combine more than one plot
# in the same frame. The two numerical values
# indicates number of rows and columns
# the frame is made of
par(mfrow=c(1,2))

acf(AirPassengers, lag.max = 48)
pacf(AirPassengers, lag.max = 48)
```

### Fit (S)ARIMA models

The above examples represent simple processes, but real time series are often the result of more complex **mixtures of different types of process**, and therefore it is more complex to identify an appropriate model for the data.

```{r}
set.seed(7623)
arima_112 <- arima.sim(n = 500, list(order = c(1,1,2), ar = 0.8, ma = c(0.7, 0.2)))
plot(arima_112, main = "ARIMA(1,1,2)")
```

A popular methods to find the appropriate model is the [Box-Jenkins method](https://en.wikipedia.org/wiki/Box–Jenkins_method), a recursive process involving the analysis of a time series, the guess of possible (S)ARIMA models, the fit of the hypothesized models, and a meta-analysis to determine the best specification. Once a best-fitting model has been
found, the correlogram of the **residuals** should be verified as **white noise**.

The Box-Jenkins method could be time-consuming and requires some expertise. ACF/PACF can also become difficult to read in case of complex models, and their appropriate interpretation could require a lot of expertise as well. Fortunately, experts have developed **automated methods** that allow us to automatically found and fit an ARIMA model. This is the case of the **auto.arima** function implemented in the **forecast** package (a package for time series analysis and especially for forecasting, developed by [Rob J. Hyndman](https://scholar.google.com/citations?user=vamErfkAAAAJ&hl=en&oi=ao), professor of statistics and time series analysis expert).


```{r}
# Install the package if you haven't installed it yet
# install.packages("forecast")
library(forecast)

arima_fit <- auto.arima(arima_112)

arima_fit
```

As said above, to evaluate the fit of a model we should analyze the **residuals**, and ascertain they behave as white noise. The object resulting from the function *auto.arima* has a slot including the residuals. To ascertain that residuals are white noise we can plot its **ACF and PACF** (no spike should be significant) and also its **histogram**.

```{r}
layout(matrix(c(1,2,3,3), nrow = 2))
acf(arima_fit$residuals)
pacf(arima_fit$residuals)
hist(arima_fit$residuals, main = "Histogram of residuals")
```

The *forecast* package also implements the function **checkresiduals** to create nice and complete plots of residual diagnostics by using a simple function. 

Besides creating the plots, the function calulate the **Ljung-Box test** (default), or the **Breusch-Godfrey test** (if you specify *test="BG"* inside the function):

The *Ljung-Box test* (and also the Breusch–Godfrey test) is a diagnostic tool, applied to the residuals of a time series after fitting an ARIMA model, to test the lack of fit. The test examines the autocorrelations of the residuals. If there are no significant autocorrelations, it can be concluded that the model does not exhibit significant lack of fit. To pass the test, the p-value has to be above the significance level (usually 0.05)

```{r message=FALSE, warning=FALSE}
checkresiduals(arima_fit)
```

#### SARIMA

If we fit a model to the *AirPassengers* dataset, which has a seasonal component, we find a Seasonal Autoregressive Integrated Moving Average model (**SARIMA**). The seasonal component of the AirPassenger dataset is evident in the plot of the series and its ACF and PACF. The *forecast* package has a useful function  **ggtsdisplay** to plot a time series along with its ACF and PACF.

```{r}
ggtsdisplay(AirPassengers)
```

The seasonal part of the ARIMA model consists of terms that are similar to the non-seasonal components of the model, but involves lagged values of the seasonal period.

```{r}
AirPassengers_sarima <- auto.arima(window(AirPassengers))
AirPassengers_sarima
```

#### Forecasting 

Based on the ARIMA models we found, we can also try to **forecast** future values. We can use the function **forecast** of the homonym library. For instance, we could try to forecast the values of the AirPassenger dataset in the next four years.

```{r}
AirPassengers_forecast <- forecast(AirPassengers_sarima, h=48, level = 90)
plot(AirPassengers_forecast, main = "AirPassengers forecast")

AirPassengers_sarima <- auto.arima(AirPassengers)


```


## Cross-correlation

*Cross-correlation *is the correlation between the (lagged) values of a time series and the values of another series. Similarly to ACF and PACF, there is a specific plot that shows the cross-correlation between two time series, and a specific R function: **ccf**.

The cross-correlation can be useful to understand wich lagged values of a *X* series can be used to predict the values of a *Y* series, and thus used, for instance, in a time series regression model.

Unfourtunately, the problem with the cross-correlation function is that, as we said in the preceding sections, with autocorrelated data it is difficult to assess the dependence between two processes, and it is possible to find spurious correlations. 

Thus, it is pertinent to disentangle the linear association between *X* and *Y* from their autocorrelation. A useful device for doing this is **prewhitening**. The prewhitening method works as follows: 

  1) determine an ARIMA *time series model* for the X-variable, and store the residuals from this model;
  2) fit the ARIMA X-model to the Y-variable, and keep the residuals;
  3) examines the CCF between the X and Y model residuals.
  
We can implement this procedure writing all the necessary code, or by using the library *forecast*, or also, alternatively, the library *TSA*.

To make an example, we apply the method to two simulated two series. The Y-variable is created in such a way that it is correlated with the lagged values at time $x_{t-3}$ and $x_{t-4}$. Therefore, we should find a correlation at those lags.

```{r}
x_series <- arima.sim(n = 200, list(order = c(1,1,0), ar = 0.7, sd=1))
z <- ts.intersect(x_series, stats::lag(x_series, -3), stats::lag(x_series, -4)) 
y_series <- 15 + 0.8*z[,2] + 1.5*z[,3] + rnorm(197,0,1)
```

The cross-correlation applied to the original series results in a plot where everything seems to be correlated. The "real" correlation at $x_{t-3}$ and $x_{t-4}$ is not discernible at all.

```{r}
ccf(x_series, y_series, na.action = na.omit)
```

By using the *forecast* library, we can calculate the pre-withened *ccf* as follows:

```{r}
# fit an ARIMA model
x_model <- auto.arima(x_series)
# keep the residuals ("white noise")
x_residuals <- x_model$residuals

# fit the same ARIMA model to the Y-series
# by using the "Arima" function in forecast
y_model <- Arima(y_series, model=x_model)
# keep the residuals
y_filtered <- residuals(y_model)

# apply the ccf to the residuals
ccf(x_residuals, y_filtered)

```

Now, it's clear that the X-variable is correlated with the Y-variable at $x_{t-3}$ and $x_{t-4}$. 

The previous steps show in some detail the steps involved in the pre-whitening strategy, but it is possible to use the original series with the **prewhiten** function of the **TSA** library. Although the function can take, as an argument, a pre-fitted ARIMA model, its greater advantage is that it can take care of all the necessary steps to prewithen the series. In particular, if no model is specified, the library automatically applies a simple AR model. Although this model can be just an approximation of the "true" model (which can be more complex), an approximation can be enough to pre-whiten the series and find a proper cross-correlation (that is, also a simpler and approximate model can do the job).


```{r}
# install.packages("TSA")
library(TSA)
prewhiten(x_series, y_series)
```


## Examples in literature

A few examples to exemplify the use of ARIMA and Cross-Correlation in the scientific literature, with specific reference to communication science.

In [Scheufele, B., Haas, A., & Brosius, H. B. (2011). Mirror or molder? A study of media coverage, stock prices, and trading volumes in Germany. Journal of Communication, 61(1), 48-70](https://academic.oup.com/joc/article/61/1/48/4098436?casa_token=b3Cd_02sN9IAAAAA:MM9rnMjb14X1ZSqxrcswoO3CDEYHry97L9EG9vL4dt5kpkZryx8VSlR8F_wXaOmBBu9VZvqlrLCZ0w), the authors investigate *"the short-term relationship between media coverage, stock prices, and trading volumes of eight listed German companies"*, by using ARIMA and cross-correlation, in particular asking:

>RQ2: Do cross-lagged correlations between media coverage and stock prices or trading volumes differ according to the amount and the valence of coverage?
RQ3: Do cross-lagged correlations between media coverage and stock prices or trading volumes differ according to the type of media (Financial Web sites, daily newspapers, and stock market TV shows) which reports on the company or stock?

To answer these questions, the authors made use of time series analysis. In particular, they:

>estimated cross-lagged correlations between media coverage and stock prices or trading volumes, respectively. Basically, two steps of time-series analysis can be distinguished: (a) In the first step, each media time-series and each time-series of trading volumes was adjusted by ARIMA (Autoregressive Integrated Moving Average) modeling separately. The differences between the original time-series and its ARIMA model are called residuals and were used for analysis. Like with ordinary least squares regression, these residuals should not be auto-correlated. If the residuals are not auto-correlated, time-series analysis speaks of White Noise. This modeling technique called prewhitening was necessary to avoid spurious correlations. (...) (b) In the next step, cross-correlations between each adjusted media time-series and each adjusted stock series were calculated. (...) The coefficient expresses the strength of correlation, whereas the lags offer an insight into dynamics: Correlations at positive (negative) lags indicate that changes in media coverage proceeded (succeeded) shifts in stock prices or trading volumes. 

In [Groshek, J. (2010). A time-series, multinational analysis of democratic forecasts and Internet diffusion. International Journal of Communication, 4, 33](https://ijoc.org/index.php/ijoc/article/viewFile/495/392), the author *examines the democratic effects that the Internet has shown using macro- level, cross-national data in a sequence of time–series statistical tests*:

>this study relies principally on macro-level time–series democracy data from an historical sample that includes 72 countries, reaching back as far as 1946 in some cases, but at least from 1954 to 2003. From this sample, a sequence of ARIMA (autoregressive integrated moving average) time–series regressions were modeled for each country for at least 40 years prior to 1994. These models were then used to generate statistically-forecasted democracy values for each country, in each year from 1994 to 2003. A 95% confidence interval with an upper and lower democracy score was then constructed around each of the forecasted values using dynamic mean squared errors. The actual democracy scores of each country for each year from 1994 to 2003 were then compared to the upper and lower values of the confidence interval.
In the event that the actual democracy level of any country was greater than the upper value of the forecasted democracy score during the time period of 1994 to 2003, Internet diffusion was investigated in case studies as a possible causal mechanism.

In other terms, the author used a forecasting approach to predict the values of the series from 1994 to 2003, in order to find statistically significant differences between the predicted and the actual values. These discrepancies were interpreted as caused by factors that were not present in the past, and possibly by the introduction of the Internet.

The study found that, *based on the results of the 72 countries reported here, the diffusion of the Internet should not be considered a democratic panacea, but rather a component of contemporary democratization processes*






 



<!--chapter:end:07-ARIMA.Rmd-->

# Regression

In this chapter we are going to see how to conduct a regression analysis with time series data.

*Regression analysis* is a used for estimating the relationships between a *dependent variable (DV)* (also called *outcome* or *response*) and one or more *independent variables (IV)* (also called *predictors* or *explanatory variables*).

A standard regression model $Y$ = $\beta$ + $\beta x$ + $\epsilon$ has no time component. Differently, a time series regression model includes a time dimension and can be written, in a simple and general formulation, using just one explanatory variable, as follows:

$$
y_t = \beta_0 + \beta_1x_t + \epsilon_t
$$

In this equation, $y_t$ is the time series we try to understand/predict (the *dependent variable (DV)*), $\beta_0$ is the *intercept* (a constant value that represents the expected mean value of $y_t$ when $x_t = 0$), the coefficient $\beta_1$ is the *slope*, representing the average change in $y$ at one unit increase in $x$ (the *independent variable (IV) or explanatory variable*), and $\epsilon_t$ is the time series of residuals (the error term).

A multiple regression, with more than one explanatory variable, can be written as follows:

$$
y_t = \beta_0 + \beta_1x_{1,t} + \beta_2x_{2,t} + ... + \beta_kx_{k,t} + \epsilon_t
$$

## Static and Dynamic Models

From a time series analysis perspective, a general distinction can be made between "static" and "dynamic" regression models:

*   A **static regression model** includes just contemporary relations between the explanatory variables (independent variables) and the response (dependent variable). This model could be appropriate when the expected value of the response changes *immediately* when the value of the explanatory variable changes. Considering a model with $k$ independent variables {$x_1$, $x_2$, ..., $x_k$}, a static (multiple) regression model, has the form just seen above:

$$
y_t = \beta_0 + \beta_1x_{1,t} + \beta_2x_{2,t} + ... + \beta_kx_{k,t} + \epsilon_t
$$

Each $\beta$ coefficient models the *instant change* in the conditional expected value of the response variable $y_t$ as the value of $x_{k,t}$ changes by one unit, keeping constant all the other predictors (i.e.: the other $x_{k,t}$):

*   A **dynamic regression model** includes relations between *both the current and the lagged (past) values of the explanatory (independent) variables*, that is, the expected value of the response variable may change *after* a change in the values of the explanatory variables. 

$$
\begin{aligned} 
y_t = \beta_0  & + \beta_{10}x_{1,t} + \beta_{11}x_{1,t-1} + ... + \beta_{1m}x_{1,t-m} \\
& + \beta_{20}x_{2,t} + \beta_{21}x_{2,t-1} + ... + \beta_{2m}x_{2,t-m} \\
& + \dots \\
& + \beta_{k0}x_{k,t} + \beta_{k1}x_{k,t-1} + ... + \beta_{km}x_{k,t-m} \\
& + \epsilon_t \\
\end{aligned} 
$$

Despite the differences between these two analytic perspectives, the term *dynamic regression* is also used, in the literature, in a more general way to refer to regression models with autocorrelated errors (also when they are used to analyze only contemporary relations between variables).

## Regression models

Except for the possible use of lagged regressors, which are typical of time series, the above described statistical models are standard regression models, commonly used with cross-sectional data. 

Standard linear regression models can sometimes work well enough with time series data, **if specific conditions are met**. Besides standard assumptions of linear regression^[1) Linearity: The relationship between X and Y must be linear; 2) Independence of errors: There is not a relationship between the residuals and the Y variable; 3) Normality of errors: The residuals must be approximately normally distributed; 4) Equal variances: The variance of the residuals is the same for all values of X], a careful analysis should be done in order to ascertain that **residuals are not autocorrelated**, since this can cause problems in the estimated model. 

In this chapter we'll see how to deal with autocorrelated residuals. However, even before that, it is important that the series are **stationary**, in order to avoid possible *spurious correlations*. 

### Stationarity

We already discussed stationarity in the previous chapters. Here we can observe that time series can be nonstationary due to different reasons, thus different strategies can be employed to *stationarize* the data.

For instance, a nonstationary series can be a series with **unequal variance** over time. A common way to try to fix the problem is by applying a log-transformation.

```{r}
library(xts)

elections_news <- read.csv("data/elections-stories-over-time-20210111144254.csv")
elections_news$date <- as.Date(elections_news$date)

elections_news <- xts(elections_news$count, order.by = elections_news$date)
elections_news_log <- log(elections_news+1)
elections_news_xts <- merge.xts(elections_news, elections_news_log)

plot.xts(elections_news_xts, col = c("blue", "red"),
         multi.panel = TRUE, yaxis.same = FALSE,
         main = "Original vs Log-transformed series")

```

Another reason for nonstationarity is the periodic variation due to **seasonality** (regular fluctuations in a time series that follow a specific time pattern, e.g.: social media activity during week-ends, Christmas effect in consumption, etc.).

To remove the seasonal pattern, you might want to use a *seasonally-adjusted* time series. Otherwise, you could create a dummy variable for the seasonal period (that is, a variable that follows the seasonal pattern in the data in order to account, in the model, for these fluctuations).

```{r}
# load the ts dataset AirPassenger
data("AirPassengers")

# remove seasonality from a multiplicative model
AirPassengers_decomposed <- decompose(AirPassengers, type="multiplicative")
AirPassengers_seasonal_component <- AirPassengers_decomposed$seasonal
AirPassengers_seasonally_adjusted <- AirPassengers/AirPassengers_seasonal_component

par(mfrow=c(1,2))
plot.ts(AirPassengers, col = "blue", main = "Original series")
plot.ts(AirPassengers_seasonally_adjusted, col = "blue",  
        main = "Seasonally-adjusted series", 
        ylab = "Seasonally-adjusted values")
```

An important reason for nonstationarity is also the presence of a trend in the data. There are **stochastic trends** and **deterministic trends**. Deterministic trends are a fixed function of time, while stochastic trends change in an unpredictable way. 

Series with a deterministic trend are also called *trend stationary* because they can be stationary around a deterministic trend, and it could be possible to achieve stationarity by removing the time trend. In trend stationary processes, the shocks to the process are transitory and the process is *mean reverting*. 

Processes with a *stochastic trend* are also called *difference stationary* because they can become stationary through *differencing*. In series with stochastic trends we could see that shocks have permanent effects.

When dealing with *deterministic trend*, we might want to work with detrended series. 

```{r}
# remove the trend from a multiplicative model
AirPassengers_decomposed <- decompose(AirPassengers, type="multiplicative")
AirPassengers_trend_component <- AirPassengers_decomposed$trend
AirPassengers_detrended <- AirPassengers/AirPassengers_trend_component

par(mfrow=c(1,2))
plot.ts(AirPassengers, col = "blue", main = "Original series")
plot.ts(AirPassengers_detrended, col = "blue",  
        main = "Detrended series", 
        ylab = "Detrended values")
```

Otherwise, in regression analysis, it is more common to add a dummy variable consisting of a value that increases with time, to account for a linear deterministic time trend. This time-count variable will remove the deterministic trend from the dependent variable, allowing the other predictors to explain the remaining variance.

```{r}
# create a simulate series
set.seed(1312)
toy_data <- arima.sim(n = 100, model = list(order = c(0,0,0)))

# add a deterministic trend to the series
toy_data_trend <- toy_data + 0.2*1:length(toy_data)

par(mfrow=c(1,3))
plot.ts(toy_data, main = "Original series")
plot.ts(toy_data_trend, main = "Series with Trend")

dummy_trend <- 1:length(toy_data_trend)
lm_toydata <- lm(toy_data_trend ~ dummy_trend)
plot.ts(lm_toydata$residuals, main = "Residuals (detrended)")

```

When we have a series with a stochastic trend, we can achieve stationarity through differencing.

```{r}
set.seed(111)
Random_Walk <- arima.sim(n = 500, model = list(order = c(0,1,0)))

Random_Walk_diff <- diff(Random_Walk)

par(mfrow=c(1,2))
plot.ts(Random_Walk,
        main = "Random Walk", 
        col = "blue", ylab="")

plot.ts(Random_Walk_diff, 
        main = "Differenced Random Walk", 
        col = "blue", ylab="")
```

#### Tests for Stochastic and Deterministic Trend

The correct detrending method depends on the type of trend. First differencing is appropriate for intergrated *I(1)* time series and time-trend regression is appropriate for trend stationary *I(0)* time series.

In case of deterministic trend, differencing is the incorrect solution, while detrending the series in function of time (regressing the series on a variable such as time and saving the residuals) is the correct solution. Differencing when none is required (*over-differencing*) may induce dynamics into the series that are not part of the data-generating process (for instance, it could create a first-order moving average process).

Specific statistical tests have been developed to distinguish between the two types of trends. In particular, *unit root tests* and *stationary test* can be used to determine if trending data should be first differenced or regressed on deterministic functions of time to render the data stationary.

Considering a simple model like the following, where $Td$ is a deterministic linear trend and $z_t$ is an autoregressive process of order 1 *AR(1)*. The difference between a process with stochastic and deterministic trend can be traced back to the parameter $|\phi|$: When $|\phi| = 1$, then $z_t$ is a *stochastic trend* and $y_t$ is an integrated process *I(1)* with *drift* (the so-called "drift" refers to the presence of a constant term, in this case $\kappa$). When $\phi < 1$, the process is not integrated (*I(0)*) and $y_t$ exhibits a *deterministic trend*^[Reference of this part is Zivot E., Wang J. (2003), Unit Root Tests, in *Modeling Financial Time Series with S-Plus®*. Springer, New York]: 

$$
\begin{aligned} 
& y_t = Td_t + z_t \\
& Td_t = \kappa + \delta_t \\
& z_t = \phi z_{t-1} + \epsilon_t, \ \epsilon_t \sim N(0, \sigma^2)
\end{aligned} 
$$
Let's simulate and visualize the above equation ($y_t = \kappa + \delta_t + \phi z_{t-1} + \epsilon_t$):

```{r}
set.seed(123)
t <- 1:500 
kappa <- 5 # costant term (or "drift")
delta <- 0.1 
epsilon <- function(n){ # function for the error term
    rnorm(n = 500, mean = 0, sd = 0.8)
    } 

y_I1 <- kappa + (delta * t) + arima.sim(n=499, list(order = c(0,1,0)), 
                                        rand.gen = epsilon)
y_I0 <- kappa + (delta * t) + arima.sim(n=500, list(order = c(1,0,0), ar = 0.8),
                                        rand.gen = epsilon)

ts_y <- ts.intersect(y_I1, y_I0)

plot.ts(ts_y, plot.type = "single", 
        lty=c(1,3), col=c("red", "blue"),
        main = "Stochastic w/ drift (red) Deterministic Trend (blue)",
        ylab="")
```

**Unit root tests** are aimed at testing the null hypothesis that $|\phi| = 1$ (*difference stationary*), against the alternative hypothesis that $|\phi| < 1$ (*trend stationary*).

**Stationarity tests** take the null hypothesis that $y_t$ is trend stationary, and are based on testing for a moving average element in $\Delta z_t$ ($\Delta$ represents the operation of differencing).

$$
\begin{aligned} 
& Original \\
& y_t = Td_t + z_t  \\
& \ Td_t = \kappa + \delta_t \\
& z_t = \phi z_{t-1} + \epsilon_t, \ \epsilon_t \sim N(0, \sigma^2) 
\end{aligned} 
$$
$$
\begin{aligned} 
& First \ difference \\
& \Delta y_t = \Delta Td_t + \Delta z_t \\
& \Delta Td_t = \Delta \kappa + \Delta \delta_t = \delta \\
& \Delta z_t = \phi \Delta z_{t-1} + \Delta \epsilon_t = \phi \Delta z_{t-1} + \epsilon_t - \epsilon_{t-1}

\end{aligned} 
$$

$\Delta z_t$ can be also written as:

$$
\Delta \epsilon_t = \phi \Delta z_{t-1} + \epsilon_t + \theta \epsilon_{t-1}
$$

with $\theta = -1$. That is, when the series is trend stationary, taking the first difference results in overdifferencing and in the creation of a moving average (MA) term $\theta \epsilon_{t-1}$. The creation of a moving average element, which is missing in the original series, is also why differencing a trend-stationary process is problematic.

##### KPSS Test

A test to verify if the series is *trend stationary* is the **Kwiatkowski-Phillips-Schmidt-Shin (KPSS)** test. It is one of the most commonly used stationarity test, and is implemented in the library *tseries* (function *kpss.test*). KPSS test the *null hypothesis* that the series is *trend stationary*.

In this case, the *p-value* of the test is higher than 0.05, so the test cannot reject the null hypothesis of trend stationarity. That is to say, there are some evidence of trend-stationary process.

```{r message=FALSE, warning=FALSE}
# install.packages("tseries") # install the library if not yet installed
library(tseries)
kpss.test(y_I0, null = "Trend")
```

By changing the null from "Trend" to "Level", the KPSS test can also test the *null hypothesis* of **level stationarity**. A level stationary time series is a *time series with a non-zero but constant mean*, that is to say, without trend.

```{r message=FALSE, warning=FALSE}
kpss.test(y_I0, null = "Level")
```

In this case, the KPSS test for level stationarity reject the null hypothesis, that is to say, the process seems not to be level stationary. Considered together, the KPSS tests suggest that the series has a deterministic trend.

If we use the KPSS test to test if the *stochastic trend* series we created above is trend or level stationary, the test *rejects* the null hypothesis (i.e.: reject the hypothesis of both a trend and level stationary process).

```{r message=FALSE, warning=FALSE}
kpss.test(y_I1, null = "Trend")
```

```{r message=FALSE, warning=FALSE}
kpss.test(y_I1, null = "Level")
```

When a series has a stochastic trend, we can achieve stationarity through differencing. Indeed, the KPSS test does not reject the null hypothesis of level stationarity when applied to the the stochastic-trend series, once differenced.

```{r message=FALSE, warning=FALSE}
kpss.test(diff(y_I1), null = "Level")
```

In the above cases the KPSS results are correct, since we have simulated and tested a time series with a deterministic and stochastic trend. However, these kind of tests can also be wrong. For instance, it is possible they reject the null hypothesis when it is actually true (["Type I error"](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)). For this reason, it can be useful to use more than one test. For instance, the KPSS can be used along with the Augmented Dickey-Fuller Test (ADF), a popular *unit root test*.

##### Augmented Dickey-Fuller (ADF) Test

The Augmented Dickey-Fuller Test (ADF) is a popular *unit root test*. An R implementation of the test can be found in the library *tseries* (function *adf.test*). The null hypothesis is that the series has a unit root, and the alternative hypothesis is that the series is stationary or trend stationary.

If we use the ADF test on the integrated series (which has a unit root), the test fails to reject the null hypothesis of unit root, which is correct.

```{r message=FALSE, warning=FALSE}
adf.test(y_I1)
```

If we use the ADF test on the trend-stationary series (without unit root), the test reject the null hypothesis of unit root, which is correct.

```{r message=FALSE, warning=FALSE}
adf.test(y_I0)
```

If we use the ADF test on the integrated series, after having transformed through differencing, the test reject the null hypothesis of unit root, which is correct.

```{r message=FALSE, warning=FALSE}
adf.test(diff(y_I1))
```

##### Phillips-Perron Test

Another *unit root test* is the **Phillips-Perron** test. It differs from the ADF test in some aspects (how it deals with serial correlation and heteroskedasticity in the errors). Also this test is implemented in the library *tseries* (funtion *pp.test*). Results of the test are similar to those of the ADF test:

```{r message=FALSE, warning=FALSE}
# series with deterministic trend
pp.test(y_I0)

# series with unit roots
pp.test(y_I1)

# series with unit roots, differenced
pp.test(diff(y_I1))
```

In case of uncertainty, more than one test can be used.


### Non-autocorrelated residuals

We try to fit a linear regression model. First, we create two series $x$ and $y$, with $x$ correlated with $y$ at lags $x_{t-3}$ and $x_{t-4}$.

```{r message=FALSE, warning=FALSE}
# simulated data of x series correlated to y at lag 3 and 4
set.seed(999)
x_series <- arima.sim(n = 200, list(order = c(1,0,0), ar = 0.7, sd=1))
z <- ts.intersect(stats::lag(x_series, -3), stats::lag(x_series, -4)) 
y_series <- 15 + 0.8*z[,1] + 1.5*z[,2] + rnorm(196,0,1)

xy_series <- ts.intersect(y_series, z)
```

The *real* model (in this case we know it because we created it through the above simulation), is as follows:

$$
y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1)
$$
#### lm

To fit a linear regression, we can use the function **lm** (the standard funtion to perform linear regression analysis in base R, no additional packages are necessary).

```{r}
lm1 <- lm(xy_series[,1] ~ xy_series[,2] + xy_series[,3])
```

The function **summary** prints the summary of the model, which includes the estimates (the "coefficients" of the variables), the standard errors, the statistical significance of the variables, and other information.

```{r}
summary(lm1)
```

We said that regression models sometimes work well enough with time series data, if specific conditions are met. Regards the conditions (or **assumptions**), in particular, the **residuals** of the models should have zero mean, they shouldn't show any significant autocorrelation, and they should be normally distributed.

To check whether these assumptions are met, we can visualize the *plot of residuals, its ACF/PACF and histogram*, and also test the residuals for possible autocorrelation using a statistical test like the [Breusch-Godfrey test](https://en.wikipedia.org/wiki/Breusch–Godfrey_test) (this test is the default in the forecast library when a linear regression object *lm* is tested).

To create the plots we can use the base R functions, or we can use the convenient *checkresiduals* function in the *forecast* package. 

In this case everything seems fine.

```{r}
# install.package("forecast") # install the package if necessary
library(forecast)
checkresiduals(lm1)
```

If we look at the model summary printed above, we can see that the estimated model is the following (the standard deviation of residuals is [misnamed as "residual standard error" in the summary of *lm*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sigma.html)):

$$
\begin{equation} 
y_t = 14.96869 + 0.85549x_{t-3} + 1.42126x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1.002^2)
\end{equation} 
$$
The estimated model is also close to the "true" model:

$$
y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \epsilon_t \\
\epsilon \sim N(0, 1)
$$
#### dynml 

Instead of *lm*, the package **dynml** and the function with the same name (*dynml*) can be used to fit a dynamic regression models in R. One of the main advantages of this package is that it allows users to fit time series linear regression models without calculating the lagged values by hand. To add a lagged variable, it can simply be used the *L* (*Lag*) function. The *L* function takes as arguments the name of the variable and the lag length. For instance *L(x, 4)* corresponds to $x_{t-4}$.

```{r message=FALSE, warning=FALSE}
# install.packages("dynml") # install the package if necessary
library(dynlm)

dynlm.fit <- dynlm(y_series ~ L(x_series, 3) + L(x_series, 4))
summary(dynlm.fit)
```

The *dynlm* function also permits to include trend (function *trend*) and seasonal (function *season*) components in the model (it is also possible to change the reference value for the seasonal period, see *?dynlm*). Just to make an example of the code to perform a dynamic regression with *dynlm*:

```{r message=FALSE, warning=FALSE}
set.seed(123)
data("AirPassengers")
ap <- log(AirPassengers)
ap_x <- 2 * stats::lag(ap, -3) + rnorm(length(ap), 0, 0.2)

ap_fm <- dynlm(ap ~ trend(ap) + season(ap) + L(ap_x, 3))
summary(ap_fm)
```

### Regression with ARMA errors

While in the previous case a standard linear model works well, it is often the case that *residuals of times series regressions are autocorrelated*, and a linear regression model can be suboptimal or even wrong. For instance, let's create other two time series that are, as the previous ones, cross-correlated at lag 3 and 4, but with a bit more complicated structure.

```{r}
# another set of simulated data 
# the x series is correlated at lag 3 and 4
set.seed(999)
x2_series <- arima.sim(n = 200, list(order = c(1,0,0), ar = 0.7, sd=1))
z2 <- ts.intersect(x2_series, stats::lag(x2_series, -3), stats::lag(x2_series, -4)) 
y2_series <- 15 + 0.8*z2[,2] + 1.5*z2[,3] 
y2_errors <- arima.sim(n = 196, list(order = c(1,0,1), ar = 0.6, ma = 0.6), sd=1)
y2_series <- y2_series + y2_errors

# check the cross-correlations at lag 3 and 4
library(TSA)
prew <- prewhiten(x2_series, y2_series) 
prewhiten(x2_series, y2_series)
prew
```

Considering the autocorrelated structure of the series, the true model can be written as follows:

$$
\begin{aligned} 
& y_t = 15 + 0.8x_{t-3} + 1.5x_{t-4} + \eta_t \\
& \eta_t = 0.7\eta_{t-1} + \epsilon_t + 0.6\epsilon_{t-1} \\
& \epsilon \sim N(0, 1)
\end{aligned} 
$$
It is possible to calculate the regression using the *lm* function, calculating the lagged variables by hand, or to use the *dynml* library and function.

```{r}
# Calculate the lagged variables by hand and apply the lm function...
x2Lagged <- cbind(
    xLag0 = x2_series,
    xLag3 = stats::lag(x2_series,-3),
    xLag4 = stats::lag(x2_series,-4))

xy2_series <- ts.union(y2_series, x2Lagged)

lm2 <- lm(xy2_series[,1] ~ xy2_series[,3:4])
summary(lm2) # AIC: 821.45

# ... or use the dynml function
dynlm.fit2 <- dynlm(y2_series ~ L(x2_series, 3) + L(x2_series, 4))
summary(dynlm.fit2)
```

The estimated model is the following:

$$
\begin{aligned} 
& y_t = 14.9005 + 1.0407x_{t-3} + 1.5171x_{t-4} + \epsilon_t \\
& \epsilon \sim N(0, 2.028^2)
\end{aligned}
$$
The original series can also be visualized with the fitted values (the values resulting from the model), to visually inspect how well the model represents the original series. The differences between the original and the fitted series are the *residuals*.

```{r}
lm2d <- ts.intersect(na.omit(xy2_series[,1]), lm2$fitted.values)

plot.ts(lm2d, plot.type = "single", col=c("orange","blue"), 
        lty=c(1,4), lwd=c(1,1),
        main = "'Classic' Linear Model - Original (orange) and Fitted series (blue)") 
```

The diagnostic plots of the residuals show the presence of autocorrelation, and the Breusch-Godfrey test is highly significant (its value is far lower than the critical value $\alpha = 0.05$)

```{r}
checkresiduals(lm2)
pacf(lm2$residuals)
```

In this case, it's better to take into account the residuals' autocorrelation by using a regression model capable to handle autocorrelated time series structures.

In the previous chapter we said that ARIMA models are a special type of regression model, in which the dependent variable is the time series itself, and the independent variables are all lags of the time series. This model is capable to take into account the *autocorrelated* structure of time series. 

ARIMA is a modeling technique that can be applied to a single time series, but it can be extended to include additional, **exogenous variables**. The ARIMA model including exogenous regressors (i.e.: other time series besides the lagged dependent variable) is like a multiple regression models for time series. In particular, it can be considered a regression model capable to control for autocorrelation in residuals.

It is possible to use more than one option to fit an ARIMA model with external regressors. A convenient option is provided by the function **auto.arima**, in the package *forecast*. This library has an argument **xreg** which can be use with *a numerical vector or matrix of external regressors, which must have the same number of rows as y* (see ?auto.arima).

```{r}
arima1 <- auto.arima(xy2_series[,1], xreg = xy2_series[,3:4])
arima1
```

The resulting model seems to be more appropriate than the previous one, fitted by using just a "classic" linear regression. This is clear also by comparing the two models through the [**AIC criterion (Akaike information criterion)**](https://en.wikipedia.org/wiki/Akaike_information_criterion). The AIC value is used to compare the *goodness-of-fit* of different models fitted to the same dataset. The lower the AIC value, the better the fit (see also the next paragraph). 

The auto.arima function prints the AIC value by default, while this value is not given with the *lm* function. To get it, we need to use the **AIC** function.

```{r}
AIC(lm2)
```

In this case, the ARIMA regression model results a far better model (*AIC=543.52*) compared with the classic linear model (*AIC=821.45*). 

$$
\begin{aligned} 
& y_t = 14.8532 + 0.9506x_{t-3} + 1.5732x_{t-4} + \eta_t \\
& \eta_t = 0.6863\eta_{t-1} + \epsilon_t + 0.6491\epsilon_{t-1} \\
& \epsilon \sim N(0, 0.9482)
\end{aligned} 
$$
Diagnostic analysis of the residuals, shows that there is no concerning sign of autocorrelation in the residuals, which looks like white noise. Also the test for autocorrelated errors is not significant (the default test for autocorrelation when testing an ARIMA models with external regressors in the *forecast* package is the **Ljung-Box test**)^[There are many tests for detecting autocorrelation. Besides the already mentioned *Breusch-Godfrey test* and *Ljung-Box test*, other popular tests are the *Durbin Watson test*, and the *Box–Pierce test*. Each test has its own characteristics. For instance, the Durbin-Watson test is a popular way to test for autocorrelation, but it [shouldn't be used with lagged dependent variables](https://www.jstor.org/stable/pdf/1909870.pdf?refreqid=excelsior%3A9526730d9debe4fa8f1a4d5fa601d523). In this case it can be used the Breusch-Godfrey test]).

```{r}
checkresiduals(arima1)
```

Also by visually inspect the original series along with the fitted series (the values resulting from the model), it can be seen that the model is better than the previous one. 

```{r}
arima1d <- ts.intersect(na.omit(xy2_series[,1]), arima1$fitted)
plot.ts(arima1d, plot.type = "single", col=c("orange","blue"), 
        lty=c(1,4), lwd=c(1,1),
        main = "ARIMA errors model - Original (orange) and Fitted series (blue)") 
```

We can also compare the fitted versus original values by using a scatterplot. A better model produces a thinner diagonal line.

```{r}
par(mfrow=c(1,2))
plot(na.omit(xy2_series[,1]), lm2$fitted.values, main = "LM", xlab="Original", ylab="Fitted")
plot(na.omit(xy2_series[,1]), arima1$fitted, main = "ARIMA regression model", xlab="Original", ylab="Fitted")
```
The *auto.arima* function does not give the statistical significance of the coefficients (the approach adopted by the *forecast* library is different, based on the choice of the best model to do forecasting), but it is possible to get that by using the function *coeftest* in the library *lmtest*.

```{r}
# install.packages(lmtest) # installa the package
library(lmtest)
coeftest(arima1)
```

### Count regression models

The models described above are mostly used with **continuous** variables (expressed as *numeric* or *double* in the R data format). However, it is often the case that time series are composed of integer values, or **count data** (expressed as *integer*). 

Sometimes, the above mentioned methods work well also with this type of data (for instance, when the counts are large). Other times, time series model developed for count data can be a better choice (for instance, when the series include mostly small integer values).

Two of the most common statistical models to deal with count data are based on the [**Poisson**](https://en.wikipedia.org/wiki/Poisson_distribution) and the [**Negative Binomial**](https://en.wikipedia.org/wiki/Negative_binomial_distribution) distributions. These probability distributions are the ones that are usually employed to model count data.

There are a few libraries to fit count time series regression models in R. We take into consideration **tscount**, and its function *tsglm*.

```{r}
# install.packages("tscount")
library(tscount)
```

We consider, as an example of the *tscount* function to fit count time series regression models, the dataset "Seatbelts" (monthly number of killed drivers of light goods vehicles in Great Britain between January 1969 and December 1984), following the [paper](https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf) that describes the *tscount* library.

The authors of the package write in the [paper (par. 7.2)](https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf) describing the library:

>This time series is part of a dataset which was first considered by Harvey and Durbin (1986) for studying the effect of compulsory wearing of seatbelts introduced on 31 January 1983. The dataset, including additional covariates, is available in R in the object Seatbelts. In their paper Harvey and Durbin (1986) analyze the numbers of casualties for drivers and passengers of cars, which are **so large** that they can be treated with **methods for continuous-valued data**. The monthly number of killed drivers of vans analyzed here is **much smaller** (its *minimum is 2 and its maximum 17*) and **therefore methods for count data are to be preferred**.

```{r}
data("Seatbelts")

timeseries <- Seatbelts[, "VanKilled"]

regressors <- cbind(PetrolPrice = Seatbelts[, c("PetrolPrice")], 
                    linearTrend = seq(along = timeseries))

timeseries_until1981 <- window(timeseries, end = c(1981, 12))
regressors_until1981 <- window(regressors, end = c(1981, 12))

```

We are going to fit a model aimed at capturing a first order autoregressive *AR(1)* term and a yearly *seasonality* by a 12th order autoregressive term. 

```{r}
par(mfrow=c(2,2))
plot.ts(timeseries, main="")
hist(timeseries, main="")
acf(timeseries, main="")
pacf(timeseries, main="")
```
The function *tsglm* allows users to declare the autoregressive and seasonal autoregressive terms in a convenient way (in the following part of the function: *model = list(past_obs = c(1, 12))*).

```{r}
poisson_fit <- tsglm(timeseries_until1981,
                     model = list(past_obs = c(1, 12)), 
                     xreg = regressors_until1981,
                     distr = "poisson", link = "log")
```

It is possible to check the residuals with the usual plots.

```{r}
par(mfrow=c(2,2))

plot(poisson_fit$residuals, main="")
hist(poisson_fit$residuals, main="")
acf(poisson_fit$residuals, main="")
pacf(poisson_fit$residuals, main="")
```
To function *summary* can be used to get the parameter estimates for the model (in this case the function can also emply a parametric bootstrap procedure (*B*) to obtain standard errors and confidence intervals of the regression parameters. The authors use *B=500* in the original paper, since in their experience this value yields stable results. Higher B values can be more precise but require time to be calculated).

```{r}
summary(poisson_fit)
# summary(poisson_fit, B=500) # to use the bootstrap procedure
```

The model is as follows:

$$
log(\lambda_t) = 1.83 + 0.09Y_{t-1} + 0.15Y_{t-12} + 0.83X_t - 0.003t
$$

In the above equation notice that, the Poisson regression, models the logarithm of the *Y* values at times *t* (expressed as $log(\lambda_t)$). 

Another example (using the dataset you can download [here](https://drive.google.com/file/d/1eIOERBLCUCaap3WCoM5QT6I0iTW-Hqwa/view?usp=sharing)):

```{r message=FALSE, warning=FALSE}
library(tidyverse)
gtrend_fakenews_qanon <- read_csv("data/gtrend_fakenews_qanon.csv",
                                  col_types = cols(date = col_date(format = "%Y-%m"),
                                                   fake_news = col_integer(), 
                                                   qanon = col_integer()))

# head(gtrend_fakenews_qanon$date,1) # 2015-01-01
# tail(gtrend_fakenews_qanon$date,1) # 2020-12-01

fake_news <- ts(gtrend_fakenews_qanon$fake_news, 
                start = c(2015,1), end = c(2020,12), frequency = 12)

qanon <- ts(gtrend_fakenews_qanon$qanon, 
            start = c(2015,1), end = c(2020,12), frequency = 12)
```

```{r}
layout(matrix(c(1,1,2,3,4,5), 2,3, byrow=T))
plot.ts(qanon, main="")
hist(qanon, main="")
acf(qanon, 48, main="")
pacf(qanon, 48, main="")
qanon_fake_ccf <- prewhiten(fake_news, qanon, main="")
qanon_fake_ccf$ccf
```
```{r}
reg <- cbind(fake_news_lag4 = stats::lag(fake_news, -4),
             fake_news_lag5 = stats::lag(fake_news, -5),
             fake_news_lag6 = stats::lag(fake_news, -6))

# NA values derives from the application of the "lag" function
# and have to be removed, since the regression function cannot
# work properly with them
reg <- na.omit(reg)

# start(reg) # 2015, 6
# end(reg) # 2021, 4

reg <-  window(reg, start = c(2015, 7), end = c(2020, 12))
qanon <- window(qanon, start = c(2015, 7), end = c(2020, 12))
```

```{r}
poisson_gtrend_fit <- tsglm(qanon,
                            model = list(past_obs = 1), 
                            xreg = reg,
                            distr = "poisson", link = "log")
```


```{r}
layout(matrix(c(1,1,2,3,4,5), 3,2, byrow=T))

plot(poisson_gtrend_fit$residuals, main="")
hist(poisson_gtrend_fit$residuals, main="")
acf(poisson_gtrend_fit$residuals, main="")
pacf(poisson_gtrend_fit$residuals, main="")

plot(as.vector(poisson_gtrend_fit$response), 
     as.vector(poisson_gtrend_fit$fitted.values),
     xlab="response", ylab="fitted")
```
Besides checking the residuals, it is possible to plot the **PIT histogram**, provided by the function **pit** in *tscount*:
>A PIT histogram is a tool for evaluating the statistical consistency between the probabilistic forecast and the observation. The predictive distributions of the observations are compared with the actual observations. If the predictive distribution is ideal the result should be a flat PIT histogram with no bin having an extraordinary high or low level. For more information about PIT histograms see the references listed below.

```{r}
pit(poisson_gtrend_fit, ylim = c(0, 1.5), main = "PIT Poisson") 
```
In the library are included other diagnostic tools and metrics that can help choosing between poisson and negative binomial models (see the [paper](https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf) for further information).

The function *summary* prints the coefficients of the model and their confidence interval.

```{r}
summary(poisson_gtrend_fit)
```


## Model Selection (AIC, AICc, BIC)

Statistical modeling is, usually, a recursive process that requires to fit several different models and, eventually, to select the most appropriate one. For instance, the Box and Jenkins approach employed to find an appropriate ARIMA model for a time series (see the previous chapter), requires the fitting of multiple models to find the most suitable one based on the data. Similarly, the "auto.arima" function in the library *forecast*, that automatizes the search for an appropriate ARIMA model, conducts a search over possible model. 

To compare the models and select the most appropriate one, it is necessary to use some criteria. In the example above we have employed the AIC criterion. Other similar criteria are the AICc, and the BIC. They all can be used to find the most appropriate model, by comparing the *goodness-of-fit* of different models fitted to the same dataset. For instance, the documentation of the "auto.arima" function says that the function *"returns best ARIMA model according to either AIC, AICc or BIC value"*.

The **AIC** criterion is the acronym for [*Akaike information criterion)*](https://en.wikipedia.org/wiki/Akaike_information_criterion). The lower the AIC value, the better the fit (see also the next paragraph). 

The **AICc** criterion, is the same, but with a *correction for small sample size*. When the sample is small it can be used in place of the AIC criterion. As the sample size increases, the AICc converges to the AIC.

The **BIC** criterion is the *Bayesian Information Criterion (or Schwartz's Bayesian Criterion)* and has a stronger penalty than the AIC for overparametrized models (more complex models, with several predictors).

These criteria can also be used when searching for an appropriate regression model, to compare several different models including different lags of the variables.

When comparing models by using these criteria, it is important that the models are fitted to **the same dataset**, otherwise the results are not comparable. This is an important aspect to take into account when using lagged predictors. For instance, you may want to try a model including one lagged predictor $x_{t-1}$ and a model including two lagged predictors $x_{t-1}$ and $x_{t-2}$, and to compare them in order to select the best one according to AIC, AICc or the BIC criterion. However, when you add lagged predictor you loose data points.

```{r}
x_example <- ts(rnorm(40))
y_example <- ts(rnorm(40))

example_data <- cbind(y = y_example,
                      xLag0 = x_example,
                      xLag1 = stats::lag(x_example, -1),
                      xLag2 = stats::lag(x_example, -2))

example_data
```

Thus, when you fit models with different lags, you have to fit them on the same dataset. In this case, for instance, you have to skip the NA rows, and use just the rows from 3 to 40.

```{r}
# Restrict data so models use same fitting period
fit1 <- auto.arima(example_data[3:40,1], xreg=example_data[3:40,2])
fit2 <- auto.arima(example_data[3:40,1], xreg=example_data[3:40,2:3])
fit3 <- auto.arima(example_data[3:40,1], xreg=example_data[3:40,2:4])
```

Then you can compare the model, for instance, using the AIC criterion, and choose the model with the smallest value.

```{r}
fit1$aic
fit2$aic
fit3$aic
```

Finally, you fit the model using all the available data.

```{r}
fit1 <- auto.arima(example_data[,1], xreg=example_data[,2])
fit1
```

Besides these criteria, there are also other strategies for [model selection](https://en.wikipedia.org/wiki/Model_selection).




## Some examples in the literature

There are several examples of the use of time series regression models in the literature in the field of communication science. 

For instance, in [The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates](https://ijoc.org/index.php/ijoc/article/viewFile/14843/3344)^[Wozniak, A., Wessler, H., Chan, C. H., & Lück, J. (2021). The Event-Centered Nature of Global Public Spheres: The UN Climate Change Conferences, Fridays for Future, and the (Limited) Transnationalization of Media Debates. *International Journal of Communication*, 15(27)], the authors *examined whether the UN climate change conferences are conducive to an emergence of a transnational public sphere by triggering issue convergence and increased transnational interconnectedness across national media debates*. They authors detail the method they follows in this way:

>[...] Given the autoregressive nature and other properties of time series, an ordinary least squares regression analysis would violate the normality of error and the independence of observations assumption (Wells et al., 2019). Instead, **we applied the dynamic regression approach** (Gujarati & Porter, 2009; Hyndman & Athanasopoulos, 2018), which assumes that the **error term follows an autoregressive integrated moving average (ARIMA) model** (...). we found the best ARIMA structure of the error term by using the *auto.arima function from the forecast R package* (Hyndman & Khandakar, 2008). It searches for an ARIMA structure that can explain the most variance according to the *Akaike information criterion* (Akaike, 1973).

In this case they use the term "dynamic regression" to refer to a time series regression with ARIMA errors, but they did not include lagged values of their variables, thus analyzing contemporary relationships between variables.

The found, for instance, that *events taking place on a supranational level of governance (...) consistently led to spikes in media attention across countries. In contrast, a bottom-up effort such as Fridays for Future showed an inconsistent relationship with media attention across the four countries.*

```{r  echo=FALSE}
knitr::include_graphics("images/Event-Centered Nature of Global Public Spheres.png")
```

In [Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event](https://ijoc.org/index.php/ijoc/article/viewFile/11666/2819)^[Lee, F. L., Liang, H., & Tang, G. K. (2019). Online incivility, cyberbalkanization, and the dynamics of opinion polarization during and after a mass protest event. International Journal of Communication, 13, 20.], the authors used both standard regression and regression with ARIMA errors to show that *"online incivility — operationalized as the use of foul language — grew as volume of political discussions and levels of cyberbalkanization increased. Incivility led to higher levels of opinion polarization."*. Also in this case the authors analyze a "static process", that is, focus on contemporary relationships between variables.


```{r  echo=FALSE}
knitr::include_graphics("images/Online-Incivility.png")
```

In [Beyond cognitions: A longitudinal study of online search salience and media coverage of the president](https://journals.sagepub.com/doi/abs/10.1177/1077699013493792)^[Ragas, M. W., & Tran, H. (2013). Beyond cognitions: A longitudinal study of online search salience and media coverage of the president. Journalism & Mass Communication Quarterly, 90(3), 478-499.], the authors used regression models with ARIMA errors to examine *shifts in newswire coverage and search interest among Internet users in President Obama during the first two years of his administration (2009-2010)*.

```{r  echo=FALSE}
knitr::include_graphics("images/Beyond-Cognitions.png")
```

In this case, the authors analyze relationships between variables taking into account lagged values, thus adopting a "dynamic process" perspective. For instance, they write:

>RQ2 sought to determine the time span of linkages between coverage volume and search volume. (...) **ARIMA** models were run to gauge the *dynamics* of mutual influence between these two time series. The first model examined the effect of coverage volume on search volume over time (i.e., basic agenda setting) (...) presidential public relations, was included as an additional input series. The first model, with search volume being a single dependent variable, was **identified** through a **close examination of autocorrelation functions (ACFs) and partial autocorrelation functions (PACFs)**. This analysis revealed a classic **autoregressive model for the series (1, 0, 0)**. [...] According to the results, *shifts in aggregate search volume over this two-year period were significantly* **predicted by coverage volume over the prior five weeks** (p < .010)* and by presidential public relations efforts in the preceding two, three (p < .001), and five weeks (p < .005). The ARIMA model with two predictors was correctly specified (**Ljung–Box Q** = 18.132, p = .381) and it explained roughly 35% of the observed variation in the series.

In [AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993–2007](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4126885/)^[Stevens, R., & Hornik, R. C. (2014). AIDS in black and white: The influence of newspaper coverage of HIV/AIDS on HIV/AIDS testing among African Americans and White Americans, 1993–2007. Journal of health communication, 19(8), 893-906], the authors *examined the effect of newspaper coverage of HIV/AIDS on HIV testing behavior in a U.S. population.*, using a *lagged regression* to support *causal order claims by ensuring that newspaper coverage precedes the testing behavior with the inclusion of the 1-month lagged newspaper coverage variable in the model*. Counterintuitively, they found that the news media coverage had a negative effect on testing behavior: *For every additional 100 HIV/AIDS risk related newspaper stories published in this group of U.S. newspapers each month, there was a 1.7% decline in HIV testing levels in the following month*, with a higher negative effects on African Americans.

```{r  echo=FALSE}
knitr::include_graphics("images/AIDS in Black and White.png")
```



<!--chapter:end:08-Regression.Rmd-->

# Intervention Analysis

In this chapter we are going to learn about *intervention analysis* (sometimes also called *interrupted time-series analysis*) and to see how to conduct a intervention analysis.

**Intervention analysis** is typically conducted with the Box & Jenkins ARIMA framework and traditionally uses a method introduced by [Box and Tiao (1975)](https://www.jstor.org/stable/pdf/2285379.pdf)^[Box, G. E., & Tiao, G. C. (1975). Intervention analysis with applications to economic and environmental problems. Journal of the American Statistical association, 70(349), 70-79], who provided a framework for assessing *the effect of an intervention on a time series* under study.

As summarized by Box and Tiao: *Given a known intervention, is there evidence that change in the series of the kind expected actually occurred, and, if so, what can be said of the nature and magnitude of the change?*. In other words *Intervention analysis estimates the effect of an external or exogenous intervention on a time-series*. To conduct such an analysis, it is necessary to know the date of the intervention. 

Intervention analysis is a "quasi-experimental" design and an interesting approach to test whether *exogenous shocks*, such as, for instance, the introduction of a new policy, *impact on a time series process in a significant way*, that is, **by changing the mean function or trend** of a time series. 

Behind intervention analysis there is the *causal hypothesis* that observations *after* a **treatment** (the **"intervention"**) have a different level or slope from those before the intervention/interruption.

Besides *intervention* or *interrupted time-series analysis*, the analysis can be conducted through the *segmented regression* method. However, as in the case of traditional regression models applied to time series data, this approach does not take into account the autocorrelated structure of time series. Other methods include more complex computational approaches.

## Types of intervention

There are different types of interventions. For instance, an intervention can have an abrupt impact determining a permanent or temporary change, a sudden and short-lived change due to an event, or a more gradual yet permanent change. 

```{r  echo=FALSE, caption="Box-Steffensmeier, J. M., Freeman, J. R., Hitt, M. P., & Pevehouse, J. C. (2014). Time series analysis for the social sciences. Cambridge University Press"}
knitr::include_graphics("images/intervention.png")
```

## Intervention analysis with ARIMA

To exemplify an intervention analysis we are going to reproduce the example in the paper [Interrupted time series analysis using autoregressive integrated moving average (ARIMA) models: a guide for evaluating large-scale health interventions](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01235-8#Sec13).

The data to run the analysis can be downloaded [here](https://static-content.springer.com/esm/art%3A10.1186%2Fs12874-021-01235-8/MediaObjects/12874_2021_1235_MOESM1_ESM.csv).

The example evaulates the impact of a health policy intervention (an Australian health policy intervention that restricted the conditions under which a particular medicine (quetiapine) could be subsidised). The same methodological process can be applied to evaluate any intervention in any context.

The case study is described as follows:

>(...) due to growing concerns about inappropriate prescribing, after January 1, 2014 new prescriptions for this tablet strength could not include refills. Our primary outcome was the **number of monthly dispensings** of 25 mg quetiapine, of which we had **48 months of observations** (January 2011 to December 2014). 

Thus, data comprises 48 months of observations, and the date of the intervention is January 1, 2014.

There is also seasonality in the process:

>In Australia, medicine dispensing claims have significant **yearly seasonality**. Medicines are subsidised for citizens and eligible residents through the Pharmaceutical Benefits Scheme (PBS), with people paying an out-of-pocket co-payment towards the cost of their medicines, while the remainder is subsidised. If a person’s (or family’s) total out-of-pocket costs reach the “Safety Net threshold” for the calendar year, they are eligible for a reduced co-payment for the remainder of that year. Thus, there is an incentive for people reaching their Safety Net to refill their medicines more frequently towards the end of the year. Hence, we see an *increase in prescriptions at the end of the year, followed by a decrease in January*.

The researchers hypothesize the nature of the intervention as follows (see the picture below):

>(...) due to the nature of the intervention we postulated there would be an immediate drop in dispensings post-intervention (step change), as well as a change in slope (ramp). Thus, we included variables representing both types of impacts in our model. For both impacts, h = 0 and r = 0.

In the sentence above, *h* describes when the effect happens  while *r* represents the decay pattern (see the picture below).

```{r  echo=FALSE, caption="Schaffer, A. L., Dobbins, T. A., & Pearson, S. A. (2021)"}
knitr::include_graphics("images/intervention-functions.png")
```

First we load the data, converting it to a time series format, and we visualize the time series along with a vertical lines representing the date of the intervention.  

```{r}
# Load data
quet <- read.csv(file = "./data/12874_2021_1235_MOESM1_ESM.csv")

# Convert data to time series object
quet.ts <- ts(quet[,2], frequency=12, start=c(2011, 1))

# Plot data to visualize time series
plot.ts(quet.ts, ylim=c(0, 40000), col = "blue", xlab = "Month", ylab = "Dispensings")
# Add vertical line indicating date of intervention (January 1, 2014)
abline(v=2014, col = "gray", lty = "dashed", lwd=2)

```
Next, we have to create the dummy variables representing our intervention.
This can be tricky in R. In this case, the authors convert the time of the *ts* object in a more human-readable format through the *as.yearmon* function (this is a *zoo* function and you can use it by loading the *xts* library).

```{r}
library(xts)
# Create variable representing step change and view
step <- as.numeric(as.yearmon(time(quet.ts)) >= "Jan 2014")
step
```

The above vectors is a dummy variable for the intervention. It has value equal zero before the date of the intervention, and 1 after that.

Next, in this specific case, we also want to create a variable representing a constant increasing change, capturing an increasing effect of the intervention over time. 
Also in this case the creation of the variable can be a little tricky. We create two vectors by using the *rep* and the *seq* function, and concatenate them by using the *c* function.

The argument of the *rep* function are two integers *x* and *times* (*rep(x, times)*), and the function creates a vectors that repeat ("rep") the *x* values the number of times specified by *times*. We have 36 months before the intervention, and we assign them the value zero.

```{r}
rep(0, 36)
```

Instead, we use the *seq* function to create a vectors with increasing values. This part of the variable represent a gradual increase after the intervention (we have 12 months of data after the intervention). The function *seq* takes the three arguments *from*, *to*, and *by*: *from* and *to* are the starting and end values of the sequence, *by* is the increment of the sequence. In our case we create a sequence of values that increases from 1 to 12 by 1.

```{r}
seq(from = 1, to = 12, by = 1)
```

To create the variable we need, we concatenate both the function with the *c* function, as follows:

```{r}
# Create variable representing ramp (change in slope) and view
ramp <- c(rep(0, 36), seq(1, 12, 1))
ramp 
```
We search for an appropriate ARIMA model for the data by using the *auto.arima* function (*forecast* package). We include the variables we have created as external regressors.

```{r}
library(forecast)

# Use automated algorithm to identify parameters
model1 <- auto.arima(quet.ts, xreg = cbind(step, ramp), stepwise=FALSE)

# Check residuals
checkresiduals(model1)
```

The resulting model is an *ARIMA(2,1,0)(0,1,1)[12]*.

```{r}
model1
```

We use the information retrieved from the auto.arima function to fit the same ARIMA model to the data, without including the intervention (the variables we created), using just the data up to the date of the intervention (up to January 2014). To do that, we use the *window* function in order to restrict the set of data we consider, indicating December 2013 as the end of our series.

```{r}
# To forecast the counterfactual, model data excluding post-intervention time period
model2 <- Arima(window(quet.ts, end = c(2013, 12)), order = c(2, 1, 0), 
                seasonal = list(order = c(0, 1, 1), period = 12))
```

Next, we forecast the 12 months we didn't include (starting from January 2014 until the end of the period of observation, December 2014) by using the *forecast* function (library *forecast*). The logic behind this operation is to see what would have happened to the series in the absence of the intervention. In other words, we use the prediction as a *couterfactual* in order to describe a possible effect of the intervention on the series, by determining how the observed values diverges from this forecast.

```{r}
# Forecast 12 months post-intervention and convert to time series object
fc <- forecast(model2, h = 12)

# covert the average forecast (fc$mean) in a time series object
fc.ts <- ts(as.numeric(fc$mean), start=c(2014, 1), frequency = 12)

# Combine the observed and the forecast data
quet.ts.2 <- ts.union(quet.ts, fc.ts)
quet.ts.2
```

By plotting the data, we can visualize the predicted values in the absence of the intervention (red dashed line) as well as the observed values (blue line). It seems that the health policy considerably impacted the analyzed prescriptions.

```{r}
# Plot
plot.ts(quet.ts.2, plot.type = "single", 
     col=c('blue','red'), xlab="Month", ylab="Dispensings", 
     lty=c("solid", "dashed"), ylim=c(0,40000))

abline(v=2014, lty="dashed", col="gray")
```
Coming back to our initial ARIMA model including the intervention variables, calculating also the confidence intervals and the significance of the coefficients by using the *coeftest* and the *confint* function in the *lmtest* library, we can quantify the impact of the policy. 

```{r}
library(lmtest)

model1
coeftest(model1)
confint(model1)
```

>The estimated step change was − 3285 dispensings (95% CI − 4465 to − 2104) while the estimated change in slope was − 1397 dispensings per month (95% CI − 1606 to − 1188). (The figure, ndr) shows the values predicted by our ARIMA model in absence of the intervention (counterfactual) compared with the observed values. This means that the change in subsidy for 25 mg quetiapine in January 2014 was associated with an immediate, sustained decrease of 3285 dispensings, with a further decrease of 1397 dispensings every month. In other words, there were 4682 (3285 + 1397) fewer dispensings in January 2014 than predicted had the subsidy changes not been implemented. In February 2014, there were 6079 fewer dispensings (3285 + 2*1397). Importantly, our findings should only be considered valid for the duration of the study period (i.e. until December 2014).


# Interrupted time series analysis using segmented regression

**Segmented regression** is another common way for analyzing the impact of an intervention. Two good papers explaining the methods of segmented regression are, for example: 

  - [Bernal, J. L., Cummins, S., & Gasparrini, A. (2017). Interrupted time series regression for the evaluation of public health interventions: a tutorial. International journal of epidemiology, 46(1), 348-355.](https://academic.oup.com/ije/article/46/1/348/2622842)
  - [Wagner, A. K., Soumerai, S. B., Zhang, F., & Ross‐Degnan, D. (2002). Segmented regression analysis of interrupted time series studies in medication use research. Journal of clinical pharmacy and therapeutics, 27(4), 299-309.](https://www.alnap.org/system/files/content/resource/files/main/segmented-regression-wagner-2002.pdf)

As explained by the latter, *"Segmented regression analysis uses statistical models to estimate level and trend in the pre-intervention segment and changes in level and trend after the intervention (or interventions)."*. 

More exactly, a segmented regression model is structured as follow:

$$Y = b_0 + b_1Time + b_2Intervention + b_3TimeSinceIntervention + e$$

It includes at least:

  - an outcome variable (Y); 
  - a variable that indicates the time *1,2,...,t* passed from the start of the series; 
  - a dummy variable (0/1) for observation collected before (0) or after (1) the  intervention; 
  - a variable measuring the time *1,2,...,t* passed since the intervention has occured, and which is equal to zero before the intervention.

The interpretation of coefficients is as follows:

  - $b_0$ is the baseline level at Time 0;
  - The *Time* ($b_1$) coefficient indicates the trend (the *slope*) before the intervention ( change in outcome associated with a time unit increase).
  - The *Intervention* ($b_2$) coefficient indicates the immediate effect (level change) induced by the intervention (from the last observation before the intervention to the first one after).
  - The *Time Since Intervention* ($b_3$) coefficient indicates the "sustained effect", i.e., the change in trend after the intervention (the effect for each time point that passes after the intervention). It measures *the difference* between the slope of the line before and after the intervention. It is also possible to calculate the *slope of the line after* the intervention by summing up the coefficients of Time and Time Since Treatment	($b_1 + b_3$)

A good tutorial on this tecnique can be found at the following link:  [https://ds4ps.org/pe4ps-textbook/docs/p-020-time-series.html](https://ds4ps.org/pe4ps-textbook/docs/p-020-time-series.html).


<!--chapter:end:09-Intervention-Analysis.Rmd-->

# VAR

*VAR* is an acronym that stands for **Vector Autoregressive Model**. It is a common method for the analysis of multivariate time series. 

It can be conceived as a way to model a **system** of time series. In a VAR model, there is no rigid distinction between independent and dependent variables, but each variable is both dependent and independent. Besides these **endogenous** variables that dynamically interact, a VAR model can include **exogenous** variables. Exogenous variables can have an impact on the endogenous variables, while the opposite is not true. To make a simple example, the time series of fans sold by month may be influenced by the quantity of fans produced and distributed by the industry, and by the monthly temperature. While the purchase and production of fans can interact (endogenous variables), the weather is not impacted by these processes, but just impact them as an external force (exogenous variable).

To make another example (from the paper [The Impact of the Politicization of Health on Online Misinformation and Quality Information on Vaccines](http://www.italiansociologicalreview.com/ojs/index.php?journal=ISR&page=article&op=view&path%5B%5D=448&path%5B%5D=346)), the political debate on a certain topic - for instance the political debate that led to the promulgation of the law on mandatory vaccinations in Italy - could be considered an exogenous variable that impacts both the news coverage of the topic and the spread of problematic information on Twitter. While news media coverage and Twitter discussions can be considered part of the same communication system and dependent on each other (news media could set the discussion agenda on Twitter, but also social media can stimulate news media coverage) it could be assumed that the political debate that led to the promulgation of the law on mandatory vaccinations was independent from the Twitter discussions on the topic. 

Stationary tests are usually applied to ascertain that variables are not integrated (the "I" in the ARIMA model). If this is the case, variables are differenced before starting the VAR analysis. Other preliminary analysis (for instance testing for *cointegration*) and pre-processing can be performed before the analysis. Next, the number of lags to be used has to be selected. This number can be automatically identify through automated methods (lag-length selection criteria methods). The model is then evaluated. Results from a VAR model are usually complicated, and the researchers relies on statistical methods such as *Granger causality test*. [*Granger causality test*](https://en.wikipedia.org/wiki/Granger_causality), a test developed by the nobel prize winner Clive Granger, has been applied to study agenda setting processes. A variable $X$ is said to **"Granger cause"** another variable $Y$ if $Y$ can be better predicted from the past values of $X$ and $Y$ together than from the past of $Y$ alone.

>The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969. Ordinarily, regressions reflect "mere" correlations, but Clive Granger argued that causality (...) could be tested for by measuring the ability to predict the future values of a time series using prior values of another time series. Since the question of "true causality" is deeply philosophical, and because of the post hoc ergo propter hoc fallacy of assuming that one thing preceding another can be used as a proof of causation, (...) the Granger test finds only "predictive causality".

An R package to perform VAR modeling is [**vars**](https://cran.r-project.org/web/packages/vars/vars.pdf).

Practical applications of VAR modeling, including Granger causality, can be found, for instance, in the paper [Assembling the Networks and Audiences of Disinformation: How Successful Russian IRA Twitter Accounts Built Their Followings,2015–2017](https://academic.oup.com/joc/article-abstract/71/2/305/6104044) or
[Coordinating a Multi-Platform Disinformation Campaign: Internet Research Agency Activity on Three U.S. Social Media Platforms, 2015 to 2017](https://www-tandfonline-com.uaccess.univie.ac.at/doi/full/10.1080/10584609.2019.1661889) [at this link](https://github.com/jlukito/timeseries-bootcamp/blob/master/3_multivariate/varmodeling.md).

## VAR modeling hands-on tutorial


### Assumption of stationarity

Like most time series techniques, VAR assumes the series to be **stationary**. To recap:

> A stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary — it does not matter when you observe it, it should look much the same at any point in time. [...] In general, a stationary time series will have no predictable patterns in the long-term. Time plots will show the series to be roughly horizontal (although some cyclic behaviour is possible), with constant variance. [Rob J. Hyndman and George Athanasopoulos, "Forecasting: Principles and Practice"](https://otexts.com/fpp2/stationarity.html).

Below are some examples of stationary and non-stationary series.

```{r}
par(mar=c(0,0,1,0))
layout(matrix(c(1,1,2,3,4,5), ncol = 2, nrow = 3, byrow = T))

wn <- rnorm(n=1000)

plot(ts(wn), main = "Stationary Time Series (White Noise)")
abline(h = 0, col = "red", lwd = 2)

nonvariance_stat <- rnorm(n=1000) * 1:1000

plot(ts(nonvariance_stat), 
     main = "Non stationary in variance")

linear_trend <- rnorm(n=1000) + 0.05*1:1000

plot(ts(linear_trend), 
     main = "Non stationary in mean (Linear Trend)")

seasonality <- rnorm(n=24*30) + 0.2*rep(1:24, 30)

plot(ts(seasonality), 
     main = "Non stationary in mean (Seasonality)")

random_walk <- cumsum(sample(c(-100, 100), 1000, TRUE)) + rnorm(1000, sd = 44)

plot(ts(random_walk), 
     main = "Non stationary in mean (Random Walk)")
```

As we already learnt, **some pre-processing steps are usually carried out to make it stationary** when the series is not stationary. For instance, a series with a linear trend can be made stationary by removing the trend. A seasonal series can be seasonally adjusted. A random-walk-like series can be adjusted using differencing. Visual inspection and **statistical tests** can be performed to support findings on the characteristics of a time series.

Some of these pre-processing steps may be unnecessary when using the **vars** library, because its function **VAR** (which is used to fit VAR models) allows you to include trend and seasonality components.

```{r}
par(mar=c(0,0,1,0))
layout(matrix(c(1,1,2,3,4,5,6,7), ncol = 2, nrow = 4, byrow = T))

plot(ts(wn), main = "Stationary Time Series (White Noise)")
abline(h = 0, col = "red", lwd = 2)

plot(ts(random_walk), 
     main = "Non stationary in mean (Random Walk)")

plot(diff(ts(random_walk)), 
     main = "Differenced")

plot(ts(linear_trend), 
     main = "Non stationary in mean (Linear Trend)")

plot(ts(residuals(lm(linear_trend ~ seq(1, length(linear_trend), 1)))), 
     main = "Detrended")

plot(ts(seasonality), 
     main = "Non stationary in mean (Seasonality)")

decomposed <- decompose(ts(seasonality, frequency = 24))
deseasonalized <- seasonality - decomposed$seasonal

plot(ts(deseasonalized), 
     main = "Deseasonalized")

```

### Other assumptions of VAR models: distribution of residuals

Besides **stationarity** and **linearity** between variables and their lagged values, VAR cannot be performed in the presence of **structural breaks**. Structural breaks are abrupt changes in the series' mean or overall process. You can deal with structural breaks in different ways. For example, you might want to split the series into different phases, using the structural breaks as breakpoints.

Other crucial assumptions of VAR are about **residuals**, which are required to be:

  - Non serially correlated
  - Normal distributed
  - Homoskedastic

Statistical tests included in the *vars* package can be used to check these assumptions. 

Another fundamental assumption of VAR models is the **absence of cointegration** between series. Two series may be cointegrated only when they are both **integrated of the same order**. Integrated series achieve stationarity after being differentiated. The number of differentiation needed to achieve stationarity is the **order of integration** (usually, it is one: *I(1)* is the notation). 

Integrated series are non-stationary, but it can happen that **there is a linear combination of integrated series that is stationary. In that case, they are said to be cointegrated**. Although each series follows a seemingly random path, over time, they are characterized by an equilibrium, as if they were driven by a common underlying process. Cointegration has been explained with the help of the story of the **drunk and his dog**:

> In fact, the drunk is not the only creature whose behavior follow a random walk. Puppies, too, wander aimlessly when unleashed. Each new scent that crosses the puppy's nose dictates a direction for the pup's next step. [...] But what if the dog belongs to the drunk? The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But periodically she intones "Oliver, where are you?", and Oliver interrupts his aimless wandering to bark. He hears her; she hears him. He thinks: "Oh I can't let her get too far off; she'll lock me out". She thinks, "Oh, I can't let him get too far off; he'll wake me up in the middle of the night with his barking". Each assessed how far away the other is and moves to partially close that gap. Now neither drunk nor dog follows a random walk; each has added what we formally call an error-correction-mechanism to her or his steps. [...] The paths of the drunk and the dog are still non-stationary. Significantly, despite the nonstationarity of the paths, one might still say, "If you find her, the dog is unlikely to be very far away". If this is right, then the distance between the two paths is stationary, and the walks of the woman and her dog are said to be cointegrated (...). [...] Notice that cointegration is a probabilistic concept. The dog is not on a leash, which would enforce a fixed distance between the drunk and the dog. The distance between the drunk and the dog is instead a random variable. But a stationary one, despite the nonstationarity of the two paths. [Murray, M. P. (1994). A drunk and her dog: an illustration of cointegration and error correction. The American Statistician, 48(1), 37-39.](http://www-stat.wharton.upenn.edu/~steele/Courses/434F2005/Context/Co-integration/Murray93DrunkAndDog.pdf)

Statistical tests are used to determine whether cointegration is present. They have to be performed only when the series are integrated and have the same order of integration (again, cointegration can only exists if at least two series are integrated of the same order). Then, *cointegration tests* are performed. In presence of cointegration, **Vector Error Correction Models (VECM)** are used instead of VAR. VECM is basically a VAR model with an "error correction term". This topic is not (yet) covered in this tutorial.

## VAR fitting

Let's make a simple example.
Upload some data:

```{r echo=FALSE, message=FALSE, warning=FALSE}
ts_tweets <- readRDS("~/Library/CloudStorage/OneDrive-Personale/Teaching/Time Series Analysis/Time-Series-Analysis-With-R/data/ts_tweets.rds")
head(ts_tweets)
```

Let's visually inspect the series:

```{r}
plot.ts(ts_tweets[, c("pro", "anti")], 
        plot.type = "multiple",
        main = "tweets")
```

As the variance does not look constant, so we can tentatively **log-transform the series** and work with log-transformed data.

        
```{r}
ts_tweets$pro <- log(ts_tweets$pro)
ts_tweets$anti <- log(ts_tweets$anti)

plot.ts(ts_tweets[, c("pro", "anti")], 
        plot.type = "multiple",
        main = "tweets")
```
        
To check for stationarity, we can perform a test such as the Augmented Dickey-Fuller Test. Integration is suggested only for the "anti" series. 

```{r}
tseries::adf.test(ts_tweets$pro)
tseries::adf.test(ts_tweets$anti)
```

The series becomes stationary after differencing.

```{r}
anti <- diff(ts_tweets$anti, 1)
tseries::adf.test(anti)
```

We can recreate the dataset including the differentiated series. Using a first difference transformation, we lose the first data point.

```{r}
length(anti)
length(ts_tweets$pro)
```

Indeed, the first point in the differentiated series is obtained by subtracting the first point from the second one. The second point in the differentiated series is obtained by subtracting the second point from the third one, and so on. Hence, the differentiated dataset will start from the second point in time, not the first one as the original series. Thus, we need to drop the first data point from the dataset 


```{r}
# drop the first data point
ts_tweets <- ts_tweets[-1,]

# replance the anti series with the differentiated anti series
ts_tweets$anti <- anti
```

To fit the VAR model we use the library **vars**.

```{r}
# install.packages("vars")
library(vars)
```

VAR models require the researcher to specify the number of lags to include in the model. Roughly speaking, lags are auto-regressive predictors. For example, considering a VAR model with two variables $X$ and $Y$, using 1 lag means that we use the value $X_{t-1}$ and $Y_{t-1}$ to predict $X_t$ and $Y_t$, and 2 lags that we use $X_{t-1}$, $X_{t-2}$, $Y_{t-1}$, and $Y_{t-2}$. The choice of lags is important. The function **VARselect** implements four different statistical tests to find the "best" number of lags.

In this case, all the criteria agree on one lag. 

```{r}
vars::VARselect(ts_tweets[,c("pro", "anti")])
```

The function to fit the model is **VAR**, and the number of lags is specified using the *p* parameter. 

You can also include **trend** and/or **seasonality**, but in this case it doesn't seem useful. It is also possible to include **exogenous variables**. Exogenous variables are usually defined as variables external to the system: they may affect the system, but are not affected by the system.

As in the VARselect function, we only use the columns containing the data (the first column in the dataset contains dates).

```{r}
var_fit <- vars::VAR(ts_tweets[,c("pro", "anti")], p = 1)
```

The model looks like this. As you can see, it is a system of linear regression models that includes lagged predictors. Before analyzing the results we need to confirm that the assumptions of the models are met.

```{r}
summary(var_fit)
```

*serial.test* is a test for serially correlated errors. The test is okay when not significant. The test is not significant.

```{r}
vars::serial.test(var_fit)
```

*normality.test* performs tests for the normality of residuals. The tests are okay when not significant, like in this case.

```{r}
vars::normality.test(var_fit)
```

Another fundamental test is performed by the function *stability*. The test is okay if the series stay within the red bars. It is okay.

```{r}
plot(vars::stability(var_fit))
```

*arch.test* assesses the null hypothesis that a series of residuals exhibits no heteroscedasticity. Also this test is not significant, which is good.

```{r}
vars::arch.test(var_fit)
```

As all the assumptions are met, we can take a look at the overall fit. It doesn't look amazing, but the assumptions are met, so let's assume it is good enough and let's go ahead with the analysis.

```{r fig.height=5, fig.width=5}
par(mar=c(0,0,0,0))
plot(var_fit)
```

The not-so-good fit is probably related to the fact that both variables in the VAR system are not highly correlated. Most coefficients are not statistically significant and the overall R-square is small. 

Generally, the coefficient table is not interpreted. Instead, VAR models are interpreted using other tools, such as the Granger causality test.

```{r}
summary(var_fit)
```

## Granger causality test

We use the **causality** function to perform the Granger causality test. The Granger causality test is the most basic inferential tool available in VAR analysis. A time series X is considered a Granger cause of another time series Y if past values of X and Y predicts Y significantly better than past values of Y alone. This analysis is frequently performed in [communication studies focused on agenda setting phenomena](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=%28%22granger+causality+test%22+OR+%22granger+test%22%29+AND+%22agenda+setting%22&btnG=).

Let's see if there's Granger causality between our series. The researcher needs to specify the "cause" variable (sometimes, the researcher may have a hypothesis about that). In this case, we try both. There is no granger causality effect but some contemporaneous relationship (*instant causation*).

```{r}
vars::causality(var_fit, cause = "pro")
```

```{r}
vars::causality(var_fit, cause = "anti")
```

<!--chapter:end:10-VAR.Rmd-->

# Readings and Bibliographical References


## Mandatory

Shin, Y. (2017). **Time series analysis in the social sciences: the fundamentals.** Univ of California Press.

## Other readings 

Wells, C., Shah, D. V., Pevehouse, J. C., Foley, J., Lukito, J., Pelled, A., & Yang, J. (2019). [The Temporal Turn in Communication Research: Time Series Analyses Using Computational Approaches](https://ijoc.org/index.php/ijoc/article/view/10635). International Journal of Communication (19328036), 13.

Brodersen KH, Gallusser F, Koehler J, Remy N, Scott SL. [Inferring causal impact using Bayesian structural time-series models](http://research.google.com/pubs/pub41854.html). Annals of Applied Statistics, 2015, Vol. 9, No. 1, 247-274. 

Gaubatz, K. T. (2014). [A Survivor's Guide to R: An Introduction for the Uninitiated and the Unnerved](https://methods.sagepub.com/Book/a-survivors-guide-to-r). SAGE Publications.

Liboschik, T., Fokianos, K., & Fried, R. (2017). [tscount: An R package for analysis of count time series following generalized linear models](https://cran.r-project.org/web/packages/tscount/vignettes/tsglm.pdf). Journal of Statistical Software, 82(1), 1-51.

Schaffer, A. L., Dobbins, T. A., & Pearson, S. A. (2021). I[nterrupted time series analysis using autoregressive integrated moving average (ARIMA) models: a guide for evaluating large-scale health interventions](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01235-8). BMC medical research methodology, 21(1), 1-12.

Zivot E., Wang J. (2003) [Unit Root Tests](https://faculty.washington.edu/ezivot/econ584/notes/unitroot.pdf). In: Modeling Financial Time Series with S-Plus®. Springer, New York, NY. https://doi.org/10.1007/978-0-387-21763-5_4

## Useful resources

[Cross Validated](https://stats.stackexchange.com) for statistics-related questions

[Stackoverflow](https://stackoverflow.com/questions) for coding-related questions

<!--chapter:end:Readings-and-Bibliography.Rmd-->

